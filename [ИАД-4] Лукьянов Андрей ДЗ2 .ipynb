{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Домашняя работа №2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Срок сдачи:** 21 октября 2017, 23:59 <br\\>\n",
    "Штраф за опоздание: -1 балл за каждый день\n",
    "\n",
    "Файл с дз надо загрузить по [ссылке](https://www.dropbox.com/request/Sc8LELV7lp5Vl3Ad7wJG)<br/>\n",
    "Постарайтесь назвать файл по следующему формату:** [ИАД-*{Номер}*] *{Фамилия}* *{Имя}* ДЗ*{Номер}* **<br/>\n",
    "\n",
    "Сопровождайте ваш код изображеними, комментариями и выводами. <br/>\n",
    "Иммейте ввиду, что на некоторые задачи нет единственного верного и полного ответа. Чем больше информации вы сможете извлечь, аргументированных выводов сформулировать, тем лучше.\n",
    "\n",
    "Используйте данный Ipython Notebook при оформлении домашнего задания."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    " <p><div class=\"lev1 toc-item\"><a href=\"#Домашняя-работа-2\" data-toc-modified-id=\"Домашняя-работа-2-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Домашняя работа 2</a></div><div class=\"lev2 toc-item\"><a href=\"#Работа-с-данными-(4-балла)\" data-toc-modified-id=\"Работа-с-данными-(4-балла)-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Работа с данными (4 балла)</a></div><div class=\"lev2 toc-item\"><a href=\"#Классификация-(6-баллов)\" data-toc-modified-id=\"Классификация-(6-баллов)-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Классификация (6 баллов)</a></div><div class=\"lev3 toc-item\"><a href=\"#Предписания\" data-toc-modified-id=\"Предписания-1.2.1\"><span class=\"toc-item-num\">1.2.1&nbsp;&nbsp;</span>Предписания</a></div><div class=\"lev3 toc-item\"><a href=\"#Задание\" data-toc-modified-id=\"Задание-1.2.2\"><span class=\"toc-item-num\">1.2.2&nbsp;&nbsp;</span>Задание</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Прочтите статью [\"USING\tDATA\tMINING\tTO\tPREDICT\tSECONDARY SCHOOL\tSTUDENT\tALCOHOL\tCONSUMPTION\"](https://www.dropbox.com/s/ww4h9ivnkbyy9xw/STUDENT%20ALCOHOL%20CONSUMPTION%20%281%29.pdf?dl=0).<br\\>\n",
    "Загрузите [датасет](https://cloud.mail.ru/public/KiKi/Cow8y2yn7), используемый в статье и выполните следующие задания:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Работа с данными (4 балла)\n",
    "1. Приведите описание признаков датасета на русском языке с указанием типа данных (2 балла)\n",
    "2. Описание предобработки данных (2 балла)\n",
    "    - Проверьте наличие пропусков. В случае наличия пропусков заполните их медианными значениями (1 балл)\n",
    "    - Преобразуйте все номинальные признаки в несколько признаков с бинарным значением (1 балл)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 921,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from ipywidgets import interact\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 922,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 638)\n",
    "pd.set_option('display.show_dimensions', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 923,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('student/student-por.csv', sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Количество пропусков:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 924,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 924,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().values.ravel().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выделение целевой переменной:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 925,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = (5 * data['Dalc'] + 2 * data['Walc'])/7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 926,
   "metadata": {},
   "outputs": [],
   "source": [
    "y[y < 3] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 927,
   "metadata": {},
   "outputs": [],
   "source": [
    "y[y >= 3] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 928,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y.name = 'Alc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 929,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.0\n",
       "1    0.0\n",
       "2    0.0\n",
       "3    0.0\n",
       "4    0.0\n",
       "Name: Alc, Length: 5, dtype: float64"
      ]
     },
     "execution_count": 929,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 930,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data\n",
    "X.drop(['Dalc', 'Walc'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Описание признаков"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 1 Школа студента\n",
    "- 2 Пол\n",
    "- 3 Возраст \n",
    "- 4 Адрес \n",
    "- 5 Размер семьи\n",
    "- 6 Семейный статус родителей \n",
    "- 7 Образование матери \n",
    "- 8 Образование отца\n",
    "- 9 Работа матери\n",
    "- 10 Работа отца \n",
    "- 11 Причина выбора школы \n",
    "- 12 Попечитель \n",
    "- 13 Время пути до школы\n",
    "- 14 Продолжительность занятий в неделю\n",
    "- 15 Несдачи школьных предметов\n",
    "- 16 Помощь в учебе\n",
    "- 17 Помощь семье\n",
    "- 18 Дополнительные занятия\n",
    "- 19 Вне-учебные занятия\n",
    "- 20 Посещал ли садик\n",
    "- 21 Желание получить высшее образование\n",
    "- 22 Доступ к интернету\n",
    "- 23 Находится ли в отношениях\n",
    "- 24 Критерий семейного благополучия\n",
    "- 25 Свободное время от занятий\n",
    "- 26 Времяпрепровождение с друзьями\n",
    "- 27 Употребление алкоголя в будние дни\n",
    "- 28 Употребление алкоголя по выходным\n",
    "- 29 Здоровье\n",
    "- 30 Пропуски\n",
    "- 31 Оценка за португальский в 1 период\n",
    "- 31 Оценка за португальский во 2 период\n",
    "- 32 Финальная оценка за португальский"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тип данных по каждому признаку:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 931,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Column</th>\n",
       "      <th>Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>school</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sex</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>age</td>\n",
       "      <td>int64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>address</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>famsize</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Pstatus</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Medu</td>\n",
       "      <td>int64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Fedu</td>\n",
       "      <td>int64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Mjob</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Fjob</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>reason</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>guardian</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>traveltime</td>\n",
       "      <td>int64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>studytime</td>\n",
       "      <td>int64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>failures</td>\n",
       "      <td>int64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>schoolsup</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>famsup</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>paid</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>activities</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>nursery</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>higher</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>internet</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>romantic</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>famrel</td>\n",
       "      <td>int64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>freetime</td>\n",
       "      <td>int64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>goout</td>\n",
       "      <td>int64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>health</td>\n",
       "      <td>int64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>absences</td>\n",
       "      <td>int64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>G1</td>\n",
       "      <td>int64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>G2</td>\n",
       "      <td>int64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>G3</td>\n",
       "      <td>int64</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>31 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Column    Type\n",
       "0       school  object\n",
       "1          sex  object\n",
       "2          age   int64\n",
       "3      address  object\n",
       "4      famsize  object\n",
       "5      Pstatus  object\n",
       "6         Medu   int64\n",
       "7         Fedu   int64\n",
       "8         Mjob  object\n",
       "9         Fjob  object\n",
       "10      reason  object\n",
       "11    guardian  object\n",
       "12  traveltime   int64\n",
       "13   studytime   int64\n",
       "14    failures   int64\n",
       "15   schoolsup  object\n",
       "16      famsup  object\n",
       "17        paid  object\n",
       "18  activities  object\n",
       "19     nursery  object\n",
       "20      higher  object\n",
       "21    internet  object\n",
       "22    romantic  object\n",
       "23      famrel   int64\n",
       "24    freetime   int64\n",
       "25       goout   int64\n",
       "26      health   int64\n",
       "27    absences   int64\n",
       "28          G1   int64\n",
       "29          G2   int64\n",
       "30          G3   int64\n",
       "\n",
       "[31 rows x 2 columns]"
      ]
     },
     "execution_count": 931,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "types = []\n",
    "for column in X.columns:\n",
    "    col_type = X[column].dtype\n",
    "    types.append(col_type)\n",
    "pd.DataFrame({'Column': X.columns, 'Type': types})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Бинарное кодирование absence, как в статье:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 875,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X.loc[X.absences < 10, 'absences'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 876,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.loc[X.absences >= 10, 'absences'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 877,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>school</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>address</th>\n",
       "      <th>famsize</th>\n",
       "      <th>Pstatus</th>\n",
       "      <th>Medu</th>\n",
       "      <th>Fedu</th>\n",
       "      <th>Mjob</th>\n",
       "      <th>Fjob</th>\n",
       "      <th>reason</th>\n",
       "      <th>guardian</th>\n",
       "      <th>traveltime</th>\n",
       "      <th>studytime</th>\n",
       "      <th>failures</th>\n",
       "      <th>schoolsup</th>\n",
       "      <th>famsup</th>\n",
       "      <th>paid</th>\n",
       "      <th>activities</th>\n",
       "      <th>nursery</th>\n",
       "      <th>higher</th>\n",
       "      <th>internet</th>\n",
       "      <th>romantic</th>\n",
       "      <th>famrel</th>\n",
       "      <th>freetime</th>\n",
       "      <th>goout</th>\n",
       "      <th>health</th>\n",
       "      <th>absences</th>\n",
       "      <th>G1</th>\n",
       "      <th>G2</th>\n",
       "      <th>G3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GP</td>\n",
       "      <td>F</td>\n",
       "      <td>18</td>\n",
       "      <td>U</td>\n",
       "      <td>GT3</td>\n",
       "      <td>A</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>at_home</td>\n",
       "      <td>teacher</td>\n",
       "      <td>course</td>\n",
       "      <td>mother</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GP</td>\n",
       "      <td>F</td>\n",
       "      <td>17</td>\n",
       "      <td>U</td>\n",
       "      <td>GT3</td>\n",
       "      <td>T</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>at_home</td>\n",
       "      <td>other</td>\n",
       "      <td>course</td>\n",
       "      <td>father</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GP</td>\n",
       "      <td>F</td>\n",
       "      <td>15</td>\n",
       "      <td>U</td>\n",
       "      <td>LE3</td>\n",
       "      <td>T</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>at_home</td>\n",
       "      <td>other</td>\n",
       "      <td>other</td>\n",
       "      <td>mother</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GP</td>\n",
       "      <td>F</td>\n",
       "      <td>15</td>\n",
       "      <td>U</td>\n",
       "      <td>GT3</td>\n",
       "      <td>T</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>health</td>\n",
       "      <td>services</td>\n",
       "      <td>home</td>\n",
       "      <td>mother</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GP</td>\n",
       "      <td>F</td>\n",
       "      <td>16</td>\n",
       "      <td>U</td>\n",
       "      <td>GT3</td>\n",
       "      <td>T</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>other</td>\n",
       "      <td>other</td>\n",
       "      <td>home</td>\n",
       "      <td>father</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  school sex  age address famsize Pstatus  Medu  Fedu     Mjob      Fjob  \\\n",
       "0     GP   F   18       U     GT3       A     4     4  at_home   teacher   \n",
       "1     GP   F   17       U     GT3       T     1     1  at_home     other   \n",
       "2     GP   F   15       U     LE3       T     1     1  at_home     other   \n",
       "3     GP   F   15       U     GT3       T     4     2   health  services   \n",
       "4     GP   F   16       U     GT3       T     3     3    other     other   \n",
       "\n",
       "   reason guardian  traveltime  studytime  failures schoolsup famsup paid  \\\n",
       "0  course   mother           2          2         0       yes     no   no   \n",
       "1  course   father           1          2         0        no    yes   no   \n",
       "2   other   mother           1          2         0       yes     no   no   \n",
       "3    home   mother           1          3         0        no    yes   no   \n",
       "4    home   father           1          2         0        no    yes   no   \n",
       "\n",
       "  activities nursery higher internet romantic  famrel  freetime  goout  \\\n",
       "0         no     yes    yes       no       no       4         3      4   \n",
       "1         no      no    yes      yes       no       5         3      3   \n",
       "2         no     yes    yes      yes       no       4         3      2   \n",
       "3        yes     yes    yes      yes      yes       3         2      2   \n",
       "4         no     yes    yes       no       no       4         3      2   \n",
       "\n",
       "   health  absences  G1  G2  G3  \n",
       "0       3         0   0  11  11  \n",
       "1       3         0   9  11  11  \n",
       "2       3         0  12  13  12  \n",
       "3       5         0  14  14  14  \n",
       "4       5         0  11  13  13  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 877,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 878,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age           0.139312\n",
       "Medu         -0.019547\n",
       "Fedu          0.004399\n",
       "traveltime    0.105385\n",
       "studytime    -0.126759\n",
       "failures      0.116747\n",
       "famrel       -0.044411\n",
       "freetime      0.080210\n",
       "goout         0.197454\n",
       "health        0.045971\n",
       "absences      0.138445\n",
       "G1           -0.179089\n",
       "G2           -0.166799\n",
       "G3           -0.171620\n",
       "Length: 14, dtype: float64"
      ]
     },
     "execution_count": 878,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[['age', 'Medu', 'Fedu', 'traveltime', 'studytime', 'failures', \n",
    "   'famrel', 'freetime', 'goout', 'health', 'absences', 'G1', 'G2', 'G3']].corrwith(y, axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Хотя в выборке и есть признаки со слабой корреляцией с целевой переменной, далее оказалось, что нет большой разницы удалять их или нет."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для бинарного кодирования признаков напишем функцию:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 932,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data(X):\n",
    "    #номинальные признаки\n",
    "    nom = X[['school', 'sex', 'address', 'famsize', 'Pstatus', 'Mjob', 'Fjob', 'reason', 'guardian',\n",
    "             'schoolsup', 'famsup', 'paid', 'activities', 'nursery', 'higher', 'internet', 'romantic',\n",
    "             'absences']]\n",
    "    #количественные\n",
    "    num = X[['age', 'traveltime', 'failures',\n",
    "             'goout', 'G1', 'G2', 'G3', 'Medu', 'Fedu', 'famrel', 'freetime', 'health']]\n",
    "    \n",
    "    return pd.concat((num, pd.get_dummies(nom)), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 933,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = transform_data(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 934,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>traveltime</th>\n",
       "      <th>failures</th>\n",
       "      <th>goout</th>\n",
       "      <th>G1</th>\n",
       "      <th>G2</th>\n",
       "      <th>G3</th>\n",
       "      <th>Medu</th>\n",
       "      <th>Fedu</th>\n",
       "      <th>famrel</th>\n",
       "      <th>freetime</th>\n",
       "      <th>health</th>\n",
       "      <th>absences</th>\n",
       "      <th>school_GP</th>\n",
       "      <th>school_MS</th>\n",
       "      <th>sex_F</th>\n",
       "      <th>sex_M</th>\n",
       "      <th>address_R</th>\n",
       "      <th>address_U</th>\n",
       "      <th>famsize_GT3</th>\n",
       "      <th>famsize_LE3</th>\n",
       "      <th>Pstatus_A</th>\n",
       "      <th>Pstatus_T</th>\n",
       "      <th>Mjob_at_home</th>\n",
       "      <th>Mjob_health</th>\n",
       "      <th>Mjob_other</th>\n",
       "      <th>Mjob_services</th>\n",
       "      <th>Mjob_teacher</th>\n",
       "      <th>Fjob_at_home</th>\n",
       "      <th>Fjob_health</th>\n",
       "      <th>Fjob_other</th>\n",
       "      <th>Fjob_services</th>\n",
       "      <th>Fjob_teacher</th>\n",
       "      <th>reason_course</th>\n",
       "      <th>reason_home</th>\n",
       "      <th>reason_other</th>\n",
       "      <th>reason_reputation</th>\n",
       "      <th>guardian_father</th>\n",
       "      <th>guardian_mother</th>\n",
       "      <th>guardian_other</th>\n",
       "      <th>schoolsup_no</th>\n",
       "      <th>schoolsup_yes</th>\n",
       "      <th>famsup_no</th>\n",
       "      <th>famsup_yes</th>\n",
       "      <th>paid_no</th>\n",
       "      <th>paid_yes</th>\n",
       "      <th>activities_no</th>\n",
       "      <th>activities_yes</th>\n",
       "      <th>nursery_no</th>\n",
       "      <th>nursery_yes</th>\n",
       "      <th>higher_no</th>\n",
       "      <th>higher_yes</th>\n",
       "      <th>internet_no</th>\n",
       "      <th>internet_yes</th>\n",
       "      <th>romantic_no</th>\n",
       "      <th>romantic_yes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 56 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  traveltime  failures  goout  G1  G2  G3  Medu  Fedu  famrel  freetime  \\\n",
       "0   18           2         0      4   0  11  11     4     4       4         3   \n",
       "1   17           1         0      3   9  11  11     1     1       5         3   \n",
       "2   15           1         0      2  12  13  12     1     1       4         3   \n",
       "3   15           1         0      2  14  14  14     4     2       3         2   \n",
       "4   16           1         0      2  11  13  13     3     3       4         3   \n",
       "\n",
       "   health  absences  school_GP  school_MS  sex_F  sex_M  address_R  address_U  \\\n",
       "0       3         4          1          0      1      0          0          1   \n",
       "1       3         2          1          0      1      0          0          1   \n",
       "2       3         6          1          0      1      0          0          1   \n",
       "3       5         0          1          0      1      0          0          1   \n",
       "4       5         0          1          0      1      0          0          1   \n",
       "\n",
       "   famsize_GT3  famsize_LE3  Pstatus_A  Pstatus_T  Mjob_at_home  Mjob_health  \\\n",
       "0            1            0          1          0             1            0   \n",
       "1            1            0          0          1             1            0   \n",
       "2            0            1          0          1             1            0   \n",
       "3            1            0          0          1             0            1   \n",
       "4            1            0          0          1             0            0   \n",
       "\n",
       "   Mjob_other  Mjob_services  Mjob_teacher  Fjob_at_home  Fjob_health  \\\n",
       "0           0              0             0             0            0   \n",
       "1           0              0             0             0            0   \n",
       "2           0              0             0             0            0   \n",
       "3           0              0             0             0            0   \n",
       "4           1              0             0             0            0   \n",
       "\n",
       "   Fjob_other  Fjob_services  Fjob_teacher  reason_course  reason_home  \\\n",
       "0           0              0             1              1            0   \n",
       "1           1              0             0              1            0   \n",
       "2           1              0             0              0            0   \n",
       "3           0              1             0              0            1   \n",
       "4           1              0             0              0            1   \n",
       "\n",
       "   reason_other  reason_reputation  guardian_father  guardian_mother  \\\n",
       "0             0                  0                0                1   \n",
       "1             0                  0                1                0   \n",
       "2             1                  0                0                1   \n",
       "3             0                  0                0                1   \n",
       "4             0                  0                1                0   \n",
       "\n",
       "   guardian_other  schoolsup_no  schoolsup_yes  famsup_no  famsup_yes  \\\n",
       "0               0             0              1          1           0   \n",
       "1               0             1              0          0           1   \n",
       "2               0             0              1          1           0   \n",
       "3               0             1              0          0           1   \n",
       "4               0             1              0          0           1   \n",
       "\n",
       "   paid_no  paid_yes  activities_no  activities_yes  nursery_no  nursery_yes  \\\n",
       "0        1         0              1               0           0            1   \n",
       "1        1         0              1               0           1            0   \n",
       "2        1         0              1               0           0            1   \n",
       "3        1         0              0               1           0            1   \n",
       "4        1         0              1               0           0            1   \n",
       "\n",
       "   higher_no  higher_yes  internet_no  internet_yes  romantic_no  romantic_yes  \n",
       "0          0           1            1             0            1             0  \n",
       "1          0           1            0             1            1             0  \n",
       "2          0           1            0             1            1             0  \n",
       "3          0           1            0             1            0             1  \n",
       "4          0           1            1             0            1             0  \n",
       "\n",
       "[5 rows x 56 columns]"
      ]
     },
     "execution_count": 934,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Классификация (6 баллов)\n",
    "\n",
    "### Предписания\n",
    "- Используйте accuracy как основную меру качества\n",
    "- Классы в задаче несбалансированные. Для корректной кросс-валидации используйте стратифицированный способ разбиения на фолды [Stratified K-fold](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html#sklearn.model_selection.StratifiedKFold)\n",
    "\n",
    "### Задание\n",
    "1. Задайте RANDOM_SEED и случайным образом разделите выборку на обучающую и контрольную в пропорции 80/20. Этот же RANDOM_SEED используйте при кросс-валидации (0.5 балла)\n",
    "2. Random Forest (2 балла)\n",
    "    * В статье описано использование Random Forest для предсказания важности фактором вляющих на потребление алкоголя.<br\\>\n",
    "    * Повторите эксперимент с использованием [RandomForest](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier) и найдите наилучшие параметры с помощью кросс-валидации. (1 балл)<br\\>\n",
    "    * Изобразите аналогичную таблицу важности признаков для наилучшей настройки метода (значения могут отличаться). (0.5 балла)<br\\>\n",
    "    * Укажите значение accuracy для пяти наилучших настроек метода на контрольной выборке. (0.5 балла)<br\\>\n",
    "\n",
    "3. GradientBoosting (2 балла)\n",
    "    * Обучите [GradientBoostingClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html#sklearn.ensemble.GradientBoostingClassifier)  и найдите для него наилучшие параметры с помощью кросс-валидации. (0.5 балла)<br\\>\n",
    "    * Изобразите таблицу важности признаков для наилучшей настройки метода. Отличается ли она от таблицы метода RandomForest. Почему? (1 балл)<br\\>\n",
    "    * Укажите значение accuracy для пяти наилучших настроек метода. (0.5 балла)<br\\>\n",
    "\n",
    "4. AdaBoost (1 балл)\n",
    "    * Обучите [AdaBoostClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html) и найдите для него наилучшие параметры с помощью кросс-валидации. (0.5 балла)<br\\>\n",
    "    * Укажите значение accuracy для пяти наилучших настроек метода. (0.5 балла)<br\\>\n",
    "\n",
    "5. Какой из классификаторов оказался лучше? (0.5 балла)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 882,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 883,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "RANDOM_SEED = 64\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size = 0.8, random_state = RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 884,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(519, 56)"
      ]
     },
     "execution_count": 884,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 935,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 936,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd_forest = RandomForestClassifier()\n",
    "params = {'n_estimators': np.arange(1,11),\n",
    "          'max_depth': np.arange(1,11)} \n",
    "splits = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 937,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd_forest_CV = GridSearchCV(estimator=rnd_forest, param_grid=params, \n",
    "                             scoring='accuracy', cv=splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 938,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=StratifiedKFold(n_splits=5, random_state=64, shuffle=True),\n",
       "       error_score='raise',\n",
       "       estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            n_estimators=10, n_jobs=1, oob_score=False, random_state=None,\n",
       "            verbose=0, warm_start=False),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid={'n_estimators': array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10]), 'max_depth': array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10])},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring='accuracy', verbose=0)"
      ]
     },
     "execution_count": 938,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnd_forest_CV.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 939,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd_forest_grid = pd.DataFrame(rnd_forest_CV.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 940,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>param_n_estimators</th>\n",
       "      <th>params</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split3_train_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>split4_train_score</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>0.008282</td>\n",
       "      <td>0.001002</td>\n",
       "      <td>0.885978</td>\n",
       "      <td>0.944143</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>{'max_depth': 7, 'n_estimators': 6}</td>\n",
       "      <td>1</td>\n",
       "      <td>0.892308</td>\n",
       "      <td>0.951830</td>\n",
       "      <td>0.869231</td>\n",
       "      <td>0.936416</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.940270</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.944123</td>\n",
       "      <td>0.883721</td>\n",
       "      <td>0.948077</td>\n",
       "      <td>0.000507</td>\n",
       "      <td>0.000128</td>\n",
       "      <td>0.010246</td>\n",
       "      <td>0.005464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>0.010447</td>\n",
       "      <td>0.001093</td>\n",
       "      <td>0.885978</td>\n",
       "      <td>0.925272</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>{'max_depth': 6, 'n_estimators': 9}</td>\n",
       "      <td>1</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.924855</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.926782</td>\n",
       "      <td>0.892308</td>\n",
       "      <td>0.934489</td>\n",
       "      <td>0.876923</td>\n",
       "      <td>0.921002</td>\n",
       "      <td>0.891473</td>\n",
       "      <td>0.919231</td>\n",
       "      <td>0.000171</td>\n",
       "      <td>0.000094</td>\n",
       "      <td>0.005585</td>\n",
       "      <td>0.005332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.009004</td>\n",
       "      <td>0.000951</td>\n",
       "      <td>0.885978</td>\n",
       "      <td>0.889445</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>{'max_depth': 4, 'n_estimators': 8}</td>\n",
       "      <td>1</td>\n",
       "      <td>0.892308</td>\n",
       "      <td>0.892100</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.886320</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.888247</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.890173</td>\n",
       "      <td>0.883721</td>\n",
       "      <td>0.890385</td>\n",
       "      <td>0.000213</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.003186</td>\n",
       "      <td>0.001983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.008064</td>\n",
       "      <td>0.000896</td>\n",
       "      <td>0.885978</td>\n",
       "      <td>0.894839</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>{'max_depth': 4, 'n_estimators': 7}</td>\n",
       "      <td>1</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.903661</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.897881</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.884393</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.895954</td>\n",
       "      <td>0.891473</td>\n",
       "      <td>0.892308</td>\n",
       "      <td>0.000153</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.002737</td>\n",
       "      <td>0.006385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.002630</td>\n",
       "      <td>0.000637</td>\n",
       "      <td>0.884438</td>\n",
       "      <td>0.884438</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>{'max_depth': 1, 'n_estimators': 1}</td>\n",
       "      <td>5</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.884393</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.884393</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.884393</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.884393</td>\n",
       "      <td>0.883721</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.000272</td>\n",
       "      <td>0.000055</td>\n",
       "      <td>0.000357</td>\n",
       "      <td>0.000089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.008063</td>\n",
       "      <td>0.000969</td>\n",
       "      <td>0.884438</td>\n",
       "      <td>0.884438</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>{'max_depth': 2, 'n_estimators': 7}</td>\n",
       "      <td>5</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.884393</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.884393</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.884393</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.884393</td>\n",
       "      <td>0.883721</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.000481</td>\n",
       "      <td>0.000087</td>\n",
       "      <td>0.000357</td>\n",
       "      <td>0.000089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.003618</td>\n",
       "      <td>0.000761</td>\n",
       "      <td>0.884438</td>\n",
       "      <td>0.884438</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>{'max_depth': 1, 'n_estimators': 2}</td>\n",
       "      <td>5</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.884393</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.884393</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.884393</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.884393</td>\n",
       "      <td>0.883721</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.000201</td>\n",
       "      <td>0.000132</td>\n",
       "      <td>0.000357</td>\n",
       "      <td>0.000089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.005941</td>\n",
       "      <td>0.000734</td>\n",
       "      <td>0.884438</td>\n",
       "      <td>0.893298</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>{'max_depth': 4, 'n_estimators': 5}</td>\n",
       "      <td>5</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.901734</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.897881</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.888247</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.886320</td>\n",
       "      <td>0.883721</td>\n",
       "      <td>0.892308</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000357</td>\n",
       "      <td>0.005785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.009528</td>\n",
       "      <td>0.001017</td>\n",
       "      <td>0.884438</td>\n",
       "      <td>0.884823</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>{'max_depth': 3, 'n_estimators': 9}</td>\n",
       "      <td>5</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.884393</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.884393</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.886320</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.884393</td>\n",
       "      <td>0.883721</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.000357</td>\n",
       "      <td>0.000753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.007673</td>\n",
       "      <td>0.000873</td>\n",
       "      <td>0.884438</td>\n",
       "      <td>0.885594</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>{'max_depth': 3, 'n_estimators': 7}</td>\n",
       "      <td>5</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.886320</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.886320</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.886320</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.884393</td>\n",
       "      <td>0.883721</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.000063</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.000357</td>\n",
       "      <td>0.000892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.006834</td>\n",
       "      <td>0.000798</td>\n",
       "      <td>0.884438</td>\n",
       "      <td>0.887135</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>{'max_depth': 3, 'n_estimators': 6}</td>\n",
       "      <td>5</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.894027</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.884393</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.888247</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.884393</td>\n",
       "      <td>0.883721</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.000253</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.000357</td>\n",
       "      <td>0.003745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.006535</td>\n",
       "      <td>0.000880</td>\n",
       "      <td>0.884438</td>\n",
       "      <td>0.887134</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>{'max_depth': 3, 'n_estimators': 5}</td>\n",
       "      <td>5</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.886320</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.894027</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.884393</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.884393</td>\n",
       "      <td>0.883721</td>\n",
       "      <td>0.886538</td>\n",
       "      <td>0.000188</td>\n",
       "      <td>0.000125</td>\n",
       "      <td>0.000357</td>\n",
       "      <td>0.003565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.011172</td>\n",
       "      <td>0.001208</td>\n",
       "      <td>0.884438</td>\n",
       "      <td>0.884823</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>{'max_depth': 2, 'n_estimators': 10}</td>\n",
       "      <td>5</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.884393</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.884393</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.884393</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.886320</td>\n",
       "      <td>0.883721</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.000378</td>\n",
       "      <td>0.000141</td>\n",
       "      <td>0.000357</td>\n",
       "      <td>0.000753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.011276</td>\n",
       "      <td>0.001287</td>\n",
       "      <td>0.884438</td>\n",
       "      <td>0.884438</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>{'max_depth': 2, 'n_estimators': 9}</td>\n",
       "      <td>5</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.884393</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.884393</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.884393</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.884393</td>\n",
       "      <td>0.883721</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.001117</td>\n",
       "      <td>0.000445</td>\n",
       "      <td>0.000357</td>\n",
       "      <td>0.000089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.009683</td>\n",
       "      <td>0.001058</td>\n",
       "      <td>0.884438</td>\n",
       "      <td>0.884823</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>{'max_depth': 2, 'n_estimators': 8}</td>\n",
       "      <td>5</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.884393</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.886320</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.884393</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.884393</td>\n",
       "      <td>0.883721</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.001247</td>\n",
       "      <td>0.000066</td>\n",
       "      <td>0.000357</td>\n",
       "      <td>0.000753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>0.011808</td>\n",
       "      <td>0.001095</td>\n",
       "      <td>0.884438</td>\n",
       "      <td>0.977274</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>{'max_depth': 10, 'n_estimators': 10}</td>\n",
       "      <td>5</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.974952</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.973025</td>\n",
       "      <td>0.876923</td>\n",
       "      <td>0.982659</td>\n",
       "      <td>0.876923</td>\n",
       "      <td>0.982659</td>\n",
       "      <td>0.899225</td>\n",
       "      <td>0.973077</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.008130</td>\n",
       "      <td>0.004451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.006959</td>\n",
       "      <td>0.000811</td>\n",
       "      <td>0.884438</td>\n",
       "      <td>0.884438</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>{'max_depth': 2, 'n_estimators': 6}</td>\n",
       "      <td>5</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.884393</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.884393</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.884393</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.884393</td>\n",
       "      <td>0.883721</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.000543</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.000357</td>\n",
       "      <td>0.000089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.010171</td>\n",
       "      <td>0.001087</td>\n",
       "      <td>0.884438</td>\n",
       "      <td>0.884438</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>{'max_depth': 1, 'n_estimators': 10}</td>\n",
       "      <td>5</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.884393</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.884393</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.884393</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.884393</td>\n",
       "      <td>0.883721</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.000367</td>\n",
       "      <td>0.000057</td>\n",
       "      <td>0.000357</td>\n",
       "      <td>0.000089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.005758</td>\n",
       "      <td>0.000894</td>\n",
       "      <td>0.884438</td>\n",
       "      <td>0.884438</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>{'max_depth': 1, 'n_estimators': 3}</td>\n",
       "      <td>5</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.884393</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.884393</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.884393</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.884393</td>\n",
       "      <td>0.883721</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.001558</td>\n",
       "      <td>0.000312</td>\n",
       "      <td>0.000357</td>\n",
       "      <td>0.000089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.006622</td>\n",
       "      <td>0.000796</td>\n",
       "      <td>0.884438</td>\n",
       "      <td>0.884052</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>{'max_depth': 2, 'n_estimators': 5}</td>\n",
       "      <td>5</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.882466</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.884393</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.884393</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.884393</td>\n",
       "      <td>0.883721</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.001055</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000357</td>\n",
       "      <td>0.000798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.005533</td>\n",
       "      <td>0.000727</td>\n",
       "      <td>0.884438</td>\n",
       "      <td>0.884438</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>{'max_depth': 1, 'n_estimators': 5}</td>\n",
       "      <td>5</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.884393</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.884393</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.884393</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.884393</td>\n",
       "      <td>0.883721</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000357</td>\n",
       "      <td>0.000089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.006721</td>\n",
       "      <td>0.000824</td>\n",
       "      <td>0.884438</td>\n",
       "      <td>0.884438</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>{'max_depth': 1, 'n_estimators': 6}</td>\n",
       "      <td>5</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.884393</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.884393</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.884393</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.884393</td>\n",
       "      <td>0.883721</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.000384</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.000357</td>\n",
       "      <td>0.000089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.008004</td>\n",
       "      <td>0.001014</td>\n",
       "      <td>0.884438</td>\n",
       "      <td>0.884438</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>{'max_depth': 1, 'n_estimators': 7}</td>\n",
       "      <td>5</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.884393</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.884393</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.884393</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.884393</td>\n",
       "      <td>0.883721</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.000349</td>\n",
       "      <td>0.000088</td>\n",
       "      <td>0.000357</td>\n",
       "      <td>0.000089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.010371</td>\n",
       "      <td>0.001289</td>\n",
       "      <td>0.884438</td>\n",
       "      <td>0.884438</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>{'max_depth': 1, 'n_estimators': 8}</td>\n",
       "      <td>5</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.884393</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.884393</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.884393</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.884393</td>\n",
       "      <td>0.883721</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.002301</td>\n",
       "      <td>0.000603</td>\n",
       "      <td>0.000357</td>\n",
       "      <td>0.000089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.008963</td>\n",
       "      <td>0.000952</td>\n",
       "      <td>0.884438</td>\n",
       "      <td>0.884438</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>{'max_depth': 1, 'n_estimators': 9}</td>\n",
       "      <td>5</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.884393</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.884393</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.884393</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.884393</td>\n",
       "      <td>0.883721</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000357</td>\n",
       "      <td>0.000089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.004903</td>\n",
       "      <td>0.000670</td>\n",
       "      <td>0.884438</td>\n",
       "      <td>0.884438</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>{'max_depth': 1, 'n_estimators': 4}</td>\n",
       "      <td>5</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.884393</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.884393</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.884393</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.884393</td>\n",
       "      <td>0.883721</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.000452</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000357</td>\n",
       "      <td>0.000089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.003037</td>\n",
       "      <td>0.000568</td>\n",
       "      <td>0.884438</td>\n",
       "      <td>0.884823</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>{'max_depth': 2, 'n_estimators': 2}</td>\n",
       "      <td>5</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.884393</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.884393</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.886320</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.884393</td>\n",
       "      <td>0.883721</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.000057</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000357</td>\n",
       "      <td>0.000753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.005206</td>\n",
       "      <td>0.000740</td>\n",
       "      <td>0.884438</td>\n",
       "      <td>0.884438</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>{'max_depth': 2, 'n_estimators': 4}</td>\n",
       "      <td>5</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.884393</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.884393</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.884393</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.884393</td>\n",
       "      <td>0.883721</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.000394</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.000357</td>\n",
       "      <td>0.000089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.008694</td>\n",
       "      <td>0.001006</td>\n",
       "      <td>0.882897</td>\n",
       "      <td>0.898304</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>{'max_depth': 5, 'n_estimators': 6}</td>\n",
       "      <td>29</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.899807</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.905588</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.892100</td>\n",
       "      <td>0.876923</td>\n",
       "      <td>0.894027</td>\n",
       "      <td>0.883721</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.000959</td>\n",
       "      <td>0.000225</td>\n",
       "      <td>0.003010</td>\n",
       "      <td>0.004795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.005583</td>\n",
       "      <td>0.000851</td>\n",
       "      <td>0.882897</td>\n",
       "      <td>0.885208</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>{'max_depth': 3, 'n_estimators': 4}</td>\n",
       "      <td>29</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.884393</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.886320</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.886320</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.884393</td>\n",
       "      <td>0.875969</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.000246</td>\n",
       "      <td>0.000063</td>\n",
       "      <td>0.003451</td>\n",
       "      <td>0.000911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>0.007004</td>\n",
       "      <td>0.000871</td>\n",
       "      <td>0.870570</td>\n",
       "      <td>0.926043</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>{'max_depth': 6, 'n_estimators': 5}</td>\n",
       "      <td>70</td>\n",
       "      <td>0.861538</td>\n",
       "      <td>0.932563</td>\n",
       "      <td>0.876923</td>\n",
       "      <td>0.917148</td>\n",
       "      <td>0.876923</td>\n",
       "      <td>0.930636</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.930636</td>\n",
       "      <td>0.891473</td>\n",
       "      <td>0.919231</td>\n",
       "      <td>0.000770</td>\n",
       "      <td>0.000123</td>\n",
       "      <td>0.015452</td>\n",
       "      <td>0.006484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>0.011630</td>\n",
       "      <td>0.001087</td>\n",
       "      <td>0.870570</td>\n",
       "      <td>0.966481</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>{'max_depth': 8, 'n_estimators': 10}</td>\n",
       "      <td>70</td>\n",
       "      <td>0.869231</td>\n",
       "      <td>0.973025</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.959538</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.953757</td>\n",
       "      <td>0.861538</td>\n",
       "      <td>0.963391</td>\n",
       "      <td>0.852713</td>\n",
       "      <td>0.982692</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.012614</td>\n",
       "      <td>0.010250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.004655</td>\n",
       "      <td>0.000727</td>\n",
       "      <td>0.870570</td>\n",
       "      <td>0.892141</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>{'max_depth': 4, 'n_estimators': 2}</td>\n",
       "      <td>70</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.895954</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.888247</td>\n",
       "      <td>0.861538</td>\n",
       "      <td>0.884393</td>\n",
       "      <td>0.861538</td>\n",
       "      <td>0.897881</td>\n",
       "      <td>0.860465</td>\n",
       "      <td>0.894231</td>\n",
       "      <td>0.002135</td>\n",
       "      <td>0.000125</td>\n",
       "      <td>0.011489</td>\n",
       "      <td>0.005041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>0.008465</td>\n",
       "      <td>0.000885</td>\n",
       "      <td>0.869029</td>\n",
       "      <td>0.927193</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>{'max_depth': 6, 'n_estimators': 6}</td>\n",
       "      <td>74</td>\n",
       "      <td>0.869231</td>\n",
       "      <td>0.924855</td>\n",
       "      <td>0.861538</td>\n",
       "      <td>0.928709</td>\n",
       "      <td>0.869231</td>\n",
       "      <td>0.917148</td>\n",
       "      <td>0.869231</td>\n",
       "      <td>0.930636</td>\n",
       "      <td>0.875969</td>\n",
       "      <td>0.934615</td>\n",
       "      <td>0.001761</td>\n",
       "      <td>0.000098</td>\n",
       "      <td>0.004565</td>\n",
       "      <td>0.005926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>0.006572</td>\n",
       "      <td>0.000778</td>\n",
       "      <td>0.869029</td>\n",
       "      <td>0.968410</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>{'max_depth': 9, 'n_estimators': 5}</td>\n",
       "      <td>74</td>\n",
       "      <td>0.892308</td>\n",
       "      <td>0.980732</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.967245</td>\n",
       "      <td>0.869231</td>\n",
       "      <td>0.951830</td>\n",
       "      <td>0.876923</td>\n",
       "      <td>0.967245</td>\n",
       "      <td>0.860465</td>\n",
       "      <td>0.975000</td>\n",
       "      <td>0.000161</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.015506</td>\n",
       "      <td>0.009725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.008881</td>\n",
       "      <td>0.001012</td>\n",
       "      <td>0.869029</td>\n",
       "      <td>0.975731</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>{'max_depth': 10, 'n_estimators': 7}</td>\n",
       "      <td>74</td>\n",
       "      <td>0.876923</td>\n",
       "      <td>0.984586</td>\n",
       "      <td>0.876923</td>\n",
       "      <td>0.971098</td>\n",
       "      <td>0.861538</td>\n",
       "      <td>0.978805</td>\n",
       "      <td>0.853846</td>\n",
       "      <td>0.967245</td>\n",
       "      <td>0.875969</td>\n",
       "      <td>0.976923</td>\n",
       "      <td>0.000077</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.009589</td>\n",
       "      <td>0.006046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>0.008529</td>\n",
       "      <td>0.000903</td>\n",
       "      <td>0.869029</td>\n",
       "      <td>0.960709</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>{'max_depth': 8, 'n_estimators': 7}</td>\n",
       "      <td>74</td>\n",
       "      <td>0.861538</td>\n",
       "      <td>0.963391</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.959538</td>\n",
       "      <td>0.876923</td>\n",
       "      <td>0.955684</td>\n",
       "      <td>0.853846</td>\n",
       "      <td>0.965318</td>\n",
       "      <td>0.868217</td>\n",
       "      <td>0.959615</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.010894</td>\n",
       "      <td>0.003354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>0.003263</td>\n",
       "      <td>0.000548</td>\n",
       "      <td>0.867488</td>\n",
       "      <td>0.937597</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>{'max_depth': 10, 'n_estimators': 2}</td>\n",
       "      <td>78</td>\n",
       "      <td>0.876923</td>\n",
       "      <td>0.930636</td>\n",
       "      <td>0.861538</td>\n",
       "      <td>0.924855</td>\n",
       "      <td>0.861538</td>\n",
       "      <td>0.949904</td>\n",
       "      <td>0.869231</td>\n",
       "      <td>0.946050</td>\n",
       "      <td>0.868217</td>\n",
       "      <td>0.936538</td>\n",
       "      <td>0.000088</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.005721</td>\n",
       "      <td>0.009326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>0.006749</td>\n",
       "      <td>0.000840</td>\n",
       "      <td>0.867488</td>\n",
       "      <td>0.957630</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>{'max_depth': 9, 'n_estimators': 4}</td>\n",
       "      <td>78</td>\n",
       "      <td>0.861538</td>\n",
       "      <td>0.957611</td>\n",
       "      <td>0.876923</td>\n",
       "      <td>0.947977</td>\n",
       "      <td>0.876923</td>\n",
       "      <td>0.969171</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.963391</td>\n",
       "      <td>0.875969</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.000438</td>\n",
       "      <td>0.000111</td>\n",
       "      <td>0.012175</td>\n",
       "      <td>0.007972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>0.005256</td>\n",
       "      <td>0.000680</td>\n",
       "      <td>0.867488</td>\n",
       "      <td>0.931427</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>{'max_depth': 7, 'n_estimators': 4}</td>\n",
       "      <td>78</td>\n",
       "      <td>0.869231</td>\n",
       "      <td>0.924855</td>\n",
       "      <td>0.869231</td>\n",
       "      <td>0.924855</td>\n",
       "      <td>0.861538</td>\n",
       "      <td>0.922929</td>\n",
       "      <td>0.838462</td>\n",
       "      <td>0.936416</td>\n",
       "      <td>0.899225</td>\n",
       "      <td>0.948077</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.019424</td>\n",
       "      <td>0.009599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.003099</td>\n",
       "      <td>0.000547</td>\n",
       "      <td>0.867488</td>\n",
       "      <td>0.909477</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>{'max_depth': 5, 'n_estimators': 2}</td>\n",
       "      <td>78</td>\n",
       "      <td>0.892308</td>\n",
       "      <td>0.905588</td>\n",
       "      <td>0.853846</td>\n",
       "      <td>0.907514</td>\n",
       "      <td>0.869231</td>\n",
       "      <td>0.911368</td>\n",
       "      <td>0.838462</td>\n",
       "      <td>0.915222</td>\n",
       "      <td>0.883721</td>\n",
       "      <td>0.907692</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.019556</td>\n",
       "      <td>0.003428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>0.006378</td>\n",
       "      <td>0.000756</td>\n",
       "      <td>0.865948</td>\n",
       "      <td>0.959171</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>{'max_depth': 8, 'n_estimators': 5}</td>\n",
       "      <td>82</td>\n",
       "      <td>0.861538</td>\n",
       "      <td>0.957611</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.971098</td>\n",
       "      <td>0.876923</td>\n",
       "      <td>0.951830</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.963391</td>\n",
       "      <td>0.860465</td>\n",
       "      <td>0.951923</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.013501</td>\n",
       "      <td>0.007334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>0.008058</td>\n",
       "      <td>0.000850</td>\n",
       "      <td>0.865948</td>\n",
       "      <td>0.969568</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>{'max_depth': 9, 'n_estimators': 6}</td>\n",
       "      <td>82</td>\n",
       "      <td>0.853846</td>\n",
       "      <td>0.974952</td>\n",
       "      <td>0.876923</td>\n",
       "      <td>0.963391</td>\n",
       "      <td>0.861538</td>\n",
       "      <td>0.965318</td>\n",
       "      <td>0.861538</td>\n",
       "      <td>0.973025</td>\n",
       "      <td>0.875969</td>\n",
       "      <td>0.971154</td>\n",
       "      <td>0.001034</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.009012</td>\n",
       "      <td>0.004465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>0.004568</td>\n",
       "      <td>0.000798</td>\n",
       "      <td>0.865948</td>\n",
       "      <td>0.918339</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>{'max_depth': 6, 'n_estimators': 3}</td>\n",
       "      <td>82</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.909441</td>\n",
       "      <td>0.869231</td>\n",
       "      <td>0.934489</td>\n",
       "      <td>0.869231</td>\n",
       "      <td>0.919075</td>\n",
       "      <td>0.861538</td>\n",
       "      <td>0.917148</td>\n",
       "      <td>0.883721</td>\n",
       "      <td>0.911538</td>\n",
       "      <td>0.000398</td>\n",
       "      <td>0.000324</td>\n",
       "      <td>0.012226</td>\n",
       "      <td>0.008811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>0.004166</td>\n",
       "      <td>0.000649</td>\n",
       "      <td>0.864407</td>\n",
       "      <td>0.919103</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>{'max_depth': 6, 'n_estimators': 2}</td>\n",
       "      <td>85</td>\n",
       "      <td>0.869231</td>\n",
       "      <td>0.921002</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.917148</td>\n",
       "      <td>0.830769</td>\n",
       "      <td>0.915222</td>\n",
       "      <td>0.869231</td>\n",
       "      <td>0.915222</td>\n",
       "      <td>0.868217</td>\n",
       "      <td>0.926923</td>\n",
       "      <td>0.001469</td>\n",
       "      <td>0.000154</td>\n",
       "      <td>0.017907</td>\n",
       "      <td>0.004443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>0.004269</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.862866</td>\n",
       "      <td>0.941060</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>{'max_depth': 7, 'n_estimators': 3}</td>\n",
       "      <td>86</td>\n",
       "      <td>0.853846</td>\n",
       "      <td>0.949904</td>\n",
       "      <td>0.876923</td>\n",
       "      <td>0.932563</td>\n",
       "      <td>0.869231</td>\n",
       "      <td>0.940270</td>\n",
       "      <td>0.869231</td>\n",
       "      <td>0.932563</td>\n",
       "      <td>0.844961</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.000095</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.011655</td>\n",
       "      <td>0.007787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>0.005378</td>\n",
       "      <td>0.000689</td>\n",
       "      <td>0.862866</td>\n",
       "      <td>0.956856</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>{'max_depth': 8, 'n_estimators': 4}</td>\n",
       "      <td>86</td>\n",
       "      <td>0.861538</td>\n",
       "      <td>0.967245</td>\n",
       "      <td>0.876923</td>\n",
       "      <td>0.957611</td>\n",
       "      <td>0.869231</td>\n",
       "      <td>0.957611</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.942197</td>\n",
       "      <td>0.860465</td>\n",
       "      <td>0.959615</td>\n",
       "      <td>0.000091</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.010254</td>\n",
       "      <td>0.008143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>0.006254</td>\n",
       "      <td>0.000739</td>\n",
       "      <td>0.862866</td>\n",
       "      <td>0.959169</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>{'max_depth': 10, 'n_estimators': 4}</td>\n",
       "      <td>86</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.963391</td>\n",
       "      <td>0.861538</td>\n",
       "      <td>0.953757</td>\n",
       "      <td>0.869231</td>\n",
       "      <td>0.959538</td>\n",
       "      <td>0.876923</td>\n",
       "      <td>0.963391</td>\n",
       "      <td>0.860465</td>\n",
       "      <td>0.955769</td>\n",
       "      <td>0.001663</td>\n",
       "      <td>0.000083</td>\n",
       "      <td>0.010254</td>\n",
       "      <td>0.003915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>0.003522</td>\n",
       "      <td>0.000646</td>\n",
       "      <td>0.859784</td>\n",
       "      <td>0.937981</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>{'max_depth': 9, 'n_estimators': 2}</td>\n",
       "      <td>89</td>\n",
       "      <td>0.838462</td>\n",
       "      <td>0.942197</td>\n",
       "      <td>0.892308</td>\n",
       "      <td>0.922929</td>\n",
       "      <td>0.853846</td>\n",
       "      <td>0.946050</td>\n",
       "      <td>0.853846</td>\n",
       "      <td>0.938343</td>\n",
       "      <td>0.860465</td>\n",
       "      <td>0.940385</td>\n",
       "      <td>0.000415</td>\n",
       "      <td>0.000082</td>\n",
       "      <td>0.017809</td>\n",
       "      <td>0.007942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.002405</td>\n",
       "      <td>0.000605</td>\n",
       "      <td>0.859784</td>\n",
       "      <td>0.886748</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>{'max_depth': 4, 'n_estimators': 1}</td>\n",
       "      <td>89</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.872832</td>\n",
       "      <td>0.853846</td>\n",
       "      <td>0.895954</td>\n",
       "      <td>0.853846</td>\n",
       "      <td>0.892100</td>\n",
       "      <td>0.861538</td>\n",
       "      <td>0.884393</td>\n",
       "      <td>0.883721</td>\n",
       "      <td>0.888462</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.012878</td>\n",
       "      <td>0.007944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>0.003247</td>\n",
       "      <td>0.000554</td>\n",
       "      <td>0.859784</td>\n",
       "      <td>0.936058</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>{'max_depth': 8, 'n_estimators': 2}</td>\n",
       "      <td>89</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.932563</td>\n",
       "      <td>0.861538</td>\n",
       "      <td>0.932563</td>\n",
       "      <td>0.853846</td>\n",
       "      <td>0.934489</td>\n",
       "      <td>0.853846</td>\n",
       "      <td>0.951830</td>\n",
       "      <td>0.883721</td>\n",
       "      <td>0.928846</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.012878</td>\n",
       "      <td>0.008095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>0.004417</td>\n",
       "      <td>0.000643</td>\n",
       "      <td>0.859784</td>\n",
       "      <td>0.942604</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>{'max_depth': 8, 'n_estimators': 3}</td>\n",
       "      <td>89</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.938343</td>\n",
       "      <td>0.876923</td>\n",
       "      <td>0.951830</td>\n",
       "      <td>0.869231</td>\n",
       "      <td>0.932563</td>\n",
       "      <td>0.830769</td>\n",
       "      <td>0.947977</td>\n",
       "      <td>0.875969</td>\n",
       "      <td>0.942308</td>\n",
       "      <td>0.000299</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>0.018293</td>\n",
       "      <td>0.006827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>0.002110</td>\n",
       "      <td>0.000464</td>\n",
       "      <td>0.856703</td>\n",
       "      <td>0.901387</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>{'max_depth': 6, 'n_estimators': 1}</td>\n",
       "      <td>93</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.907514</td>\n",
       "      <td>0.861538</td>\n",
       "      <td>0.903661</td>\n",
       "      <td>0.876923</td>\n",
       "      <td>0.903661</td>\n",
       "      <td>0.823077</td>\n",
       "      <td>0.890173</td>\n",
       "      <td>0.875969</td>\n",
       "      <td>0.901923</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.020227</td>\n",
       "      <td>0.005898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>0.004355</td>\n",
       "      <td>0.000620</td>\n",
       "      <td>0.853621</td>\n",
       "      <td>0.957632</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>{'max_depth': 10, 'n_estimators': 3}</td>\n",
       "      <td>94</td>\n",
       "      <td>0.861538</td>\n",
       "      <td>0.955684</td>\n",
       "      <td>0.869231</td>\n",
       "      <td>0.965318</td>\n",
       "      <td>0.830769</td>\n",
       "      <td>0.961464</td>\n",
       "      <td>0.861538</td>\n",
       "      <td>0.961464</td>\n",
       "      <td>0.844961</td>\n",
       "      <td>0.944231</td>\n",
       "      <td>0.000113</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.013908</td>\n",
       "      <td>0.007373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>0.004828</td>\n",
       "      <td>0.000762</td>\n",
       "      <td>0.847458</td>\n",
       "      <td>0.954541</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>{'max_depth': 9, 'n_estimators': 3}</td>\n",
       "      <td>95</td>\n",
       "      <td>0.830769</td>\n",
       "      <td>0.959538</td>\n",
       "      <td>0.861538</td>\n",
       "      <td>0.944123</td>\n",
       "      <td>0.838462</td>\n",
       "      <td>0.951830</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.951830</td>\n",
       "      <td>0.860465</td>\n",
       "      <td>0.965385</td>\n",
       "      <td>0.000053</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>0.012070</td>\n",
       "      <td>0.007291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.002064</td>\n",
       "      <td>0.000459</td>\n",
       "      <td>0.841294</td>\n",
       "      <td>0.884056</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>{'max_depth': 5, 'n_estimators': 1}</td>\n",
       "      <td>96</td>\n",
       "      <td>0.823077</td>\n",
       "      <td>0.890173</td>\n",
       "      <td>0.815385</td>\n",
       "      <td>0.888247</td>\n",
       "      <td>0.869231</td>\n",
       "      <td>0.895954</td>\n",
       "      <td>0.861538</td>\n",
       "      <td>0.870906</td>\n",
       "      <td>0.837209</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.021040</td>\n",
       "      <td>0.009502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>0.002111</td>\n",
       "      <td>0.000461</td>\n",
       "      <td>0.841294</td>\n",
       "      <td>0.911406</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>{'max_depth': 7, 'n_estimators': 1}</td>\n",
       "      <td>96</td>\n",
       "      <td>0.861538</td>\n",
       "      <td>0.901734</td>\n",
       "      <td>0.830769</td>\n",
       "      <td>0.921002</td>\n",
       "      <td>0.761538</td>\n",
       "      <td>0.903661</td>\n",
       "      <td>0.907692</td>\n",
       "      <td>0.928709</td>\n",
       "      <td>0.844961</td>\n",
       "      <td>0.901923</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.047584</td>\n",
       "      <td>0.011269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>0.002142</td>\n",
       "      <td>0.000464</td>\n",
       "      <td>0.835131</td>\n",
       "      <td>0.922958</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>{'max_depth': 9, 'n_estimators': 1}</td>\n",
       "      <td>98</td>\n",
       "      <td>0.838462</td>\n",
       "      <td>0.915222</td>\n",
       "      <td>0.838462</td>\n",
       "      <td>0.930636</td>\n",
       "      <td>0.823077</td>\n",
       "      <td>0.913295</td>\n",
       "      <td>0.830769</td>\n",
       "      <td>0.930636</td>\n",
       "      <td>0.844961</td>\n",
       "      <td>0.925000</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.007521</td>\n",
       "      <td>0.007420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>0.002870</td>\n",
       "      <td>0.000689</td>\n",
       "      <td>0.825886</td>\n",
       "      <td>0.907937</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>{'max_depth': 8, 'n_estimators': 1}</td>\n",
       "      <td>99</td>\n",
       "      <td>0.830769</td>\n",
       "      <td>0.905588</td>\n",
       "      <td>0.815385</td>\n",
       "      <td>0.924855</td>\n",
       "      <td>0.823077</td>\n",
       "      <td>0.903661</td>\n",
       "      <td>0.815385</td>\n",
       "      <td>0.901734</td>\n",
       "      <td>0.844961</td>\n",
       "      <td>0.903846</td>\n",
       "      <td>0.000808</td>\n",
       "      <td>0.000318</td>\n",
       "      <td>0.011084</td>\n",
       "      <td>0.008547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>0.002140</td>\n",
       "      <td>0.000461</td>\n",
       "      <td>0.819723</td>\n",
       "      <td>0.934897</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>{'max_depth': 10, 'n_estimators': 1}</td>\n",
       "      <td>100</td>\n",
       "      <td>0.861538</td>\n",
       "      <td>0.934489</td>\n",
       "      <td>0.823077</td>\n",
       "      <td>0.922929</td>\n",
       "      <td>0.784615</td>\n",
       "      <td>0.940270</td>\n",
       "      <td>0.807692</td>\n",
       "      <td>0.934489</td>\n",
       "      <td>0.821705</td>\n",
       "      <td>0.942308</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.025083</td>\n",
       "      <td>0.006743</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  mean_score_time  mean_test_score  mean_train_score  \\\n",
       "65       0.008282         0.001002         0.885978          0.944143   \n",
       "58       0.010447         0.001093         0.885978          0.925272   \n",
       "37       0.009004         0.000951         0.885978          0.889445   \n",
       "36       0.008064         0.000896         0.885978          0.894839   \n",
       "0        0.002630         0.000637         0.884438          0.884438   \n",
       "16       0.008063         0.000969         0.884438          0.884438   \n",
       "1        0.003618         0.000761         0.884438          0.884438   \n",
       "34       0.005941         0.000734         0.884438          0.893298   \n",
       "28       0.009528         0.001017         0.884438          0.884823   \n",
       "26       0.007673         0.000873         0.884438          0.885594   \n",
       "25       0.006834         0.000798         0.884438          0.887135   \n",
       "24       0.006535         0.000880         0.884438          0.887134   \n",
       "19       0.011172         0.001208         0.884438          0.884823   \n",
       "18       0.011276         0.001287         0.884438          0.884438   \n",
       "17       0.009683         0.001058         0.884438          0.884823   \n",
       "99       0.011808         0.001095         0.884438          0.977274   \n",
       "15       0.006959         0.000811         0.884438          0.884438   \n",
       "9        0.010171         0.001087         0.884438          0.884438   \n",
       "2        0.005758         0.000894         0.884438          0.884438   \n",
       "14       0.006622         0.000796         0.884438          0.884052   \n",
       "4        0.005533         0.000727         0.884438          0.884438   \n",
       "5        0.006721         0.000824         0.884438          0.884438   \n",
       "6        0.008004         0.001014         0.884438          0.884438   \n",
       "7        0.010371         0.001289         0.884438          0.884438   \n",
       "8        0.008963         0.000952         0.884438          0.884438   \n",
       "3        0.004903         0.000670         0.884438          0.884438   \n",
       "11       0.003037         0.000568         0.884438          0.884823   \n",
       "13       0.005206         0.000740         0.884438          0.884438   \n",
       "45       0.008694         0.001006         0.882897          0.898304   \n",
       "23       0.005583         0.000851         0.882897          0.885208   \n",
       "..            ...              ...              ...               ...   \n",
       "54       0.007004         0.000871         0.870570          0.926043   \n",
       "79       0.011630         0.001087         0.870570          0.966481   \n",
       "31       0.004655         0.000727         0.870570          0.892141   \n",
       "55       0.008465         0.000885         0.869029          0.927193   \n",
       "84       0.006572         0.000778         0.869029          0.968410   \n",
       "96       0.008881         0.001012         0.869029          0.975731   \n",
       "76       0.008529         0.000903         0.869029          0.960709   \n",
       "91       0.003263         0.000548         0.867488          0.937597   \n",
       "83       0.006749         0.000840         0.867488          0.957630   \n",
       "63       0.005256         0.000680         0.867488          0.931427   \n",
       "41       0.003099         0.000547         0.867488          0.909477   \n",
       "74       0.006378         0.000756         0.865948          0.959171   \n",
       "85       0.008058         0.000850         0.865948          0.969568   \n",
       "52       0.004568         0.000798         0.865948          0.918339   \n",
       "51       0.004166         0.000649         0.864407          0.919103   \n",
       "62       0.004269         0.000617         0.862866          0.941060   \n",
       "73       0.005378         0.000689         0.862866          0.956856   \n",
       "93       0.006254         0.000739         0.862866          0.959169   \n",
       "81       0.003522         0.000646         0.859784          0.937981   \n",
       "30       0.002405         0.000605         0.859784          0.886748   \n",
       "71       0.003247         0.000554         0.859784          0.936058   \n",
       "72       0.004417         0.000643         0.859784          0.942604   \n",
       "50       0.002110         0.000464         0.856703          0.901387   \n",
       "92       0.004355         0.000620         0.853621          0.957632   \n",
       "82       0.004828         0.000762         0.847458          0.954541   \n",
       "40       0.002064         0.000459         0.841294          0.884056   \n",
       "60       0.002111         0.000461         0.841294          0.911406   \n",
       "80       0.002142         0.000464         0.835131          0.922958   \n",
       "70       0.002870         0.000689         0.825886          0.907937   \n",
       "90       0.002140         0.000461         0.819723          0.934897   \n",
       "\n",
       "   param_max_depth param_n_estimators                                 params  \\\n",
       "65               7                  6    {'max_depth': 7, 'n_estimators': 6}   \n",
       "58               6                  9    {'max_depth': 6, 'n_estimators': 9}   \n",
       "37               4                  8    {'max_depth': 4, 'n_estimators': 8}   \n",
       "36               4                  7    {'max_depth': 4, 'n_estimators': 7}   \n",
       "0                1                  1    {'max_depth': 1, 'n_estimators': 1}   \n",
       "16               2                  7    {'max_depth': 2, 'n_estimators': 7}   \n",
       "1                1                  2    {'max_depth': 1, 'n_estimators': 2}   \n",
       "34               4                  5    {'max_depth': 4, 'n_estimators': 5}   \n",
       "28               3                  9    {'max_depth': 3, 'n_estimators': 9}   \n",
       "26               3                  7    {'max_depth': 3, 'n_estimators': 7}   \n",
       "25               3                  6    {'max_depth': 3, 'n_estimators': 6}   \n",
       "24               3                  5    {'max_depth': 3, 'n_estimators': 5}   \n",
       "19               2                 10   {'max_depth': 2, 'n_estimators': 10}   \n",
       "18               2                  9    {'max_depth': 2, 'n_estimators': 9}   \n",
       "17               2                  8    {'max_depth': 2, 'n_estimators': 8}   \n",
       "99              10                 10  {'max_depth': 10, 'n_estimators': 10}   \n",
       "15               2                  6    {'max_depth': 2, 'n_estimators': 6}   \n",
       "9                1                 10   {'max_depth': 1, 'n_estimators': 10}   \n",
       "2                1                  3    {'max_depth': 1, 'n_estimators': 3}   \n",
       "14               2                  5    {'max_depth': 2, 'n_estimators': 5}   \n",
       "4                1                  5    {'max_depth': 1, 'n_estimators': 5}   \n",
       "5                1                  6    {'max_depth': 1, 'n_estimators': 6}   \n",
       "6                1                  7    {'max_depth': 1, 'n_estimators': 7}   \n",
       "7                1                  8    {'max_depth': 1, 'n_estimators': 8}   \n",
       "8                1                  9    {'max_depth': 1, 'n_estimators': 9}   \n",
       "3                1                  4    {'max_depth': 1, 'n_estimators': 4}   \n",
       "11               2                  2    {'max_depth': 2, 'n_estimators': 2}   \n",
       "13               2                  4    {'max_depth': 2, 'n_estimators': 4}   \n",
       "45               5                  6    {'max_depth': 5, 'n_estimators': 6}   \n",
       "23               3                  4    {'max_depth': 3, 'n_estimators': 4}   \n",
       "..             ...                ...                                    ...   \n",
       "54               6                  5    {'max_depth': 6, 'n_estimators': 5}   \n",
       "79               8                 10   {'max_depth': 8, 'n_estimators': 10}   \n",
       "31               4                  2    {'max_depth': 4, 'n_estimators': 2}   \n",
       "55               6                  6    {'max_depth': 6, 'n_estimators': 6}   \n",
       "84               9                  5    {'max_depth': 9, 'n_estimators': 5}   \n",
       "96              10                  7   {'max_depth': 10, 'n_estimators': 7}   \n",
       "76               8                  7    {'max_depth': 8, 'n_estimators': 7}   \n",
       "91              10                  2   {'max_depth': 10, 'n_estimators': 2}   \n",
       "83               9                  4    {'max_depth': 9, 'n_estimators': 4}   \n",
       "63               7                  4    {'max_depth': 7, 'n_estimators': 4}   \n",
       "41               5                  2    {'max_depth': 5, 'n_estimators': 2}   \n",
       "74               8                  5    {'max_depth': 8, 'n_estimators': 5}   \n",
       "85               9                  6    {'max_depth': 9, 'n_estimators': 6}   \n",
       "52               6                  3    {'max_depth': 6, 'n_estimators': 3}   \n",
       "51               6                  2    {'max_depth': 6, 'n_estimators': 2}   \n",
       "62               7                  3    {'max_depth': 7, 'n_estimators': 3}   \n",
       "73               8                  4    {'max_depth': 8, 'n_estimators': 4}   \n",
       "93              10                  4   {'max_depth': 10, 'n_estimators': 4}   \n",
       "81               9                  2    {'max_depth': 9, 'n_estimators': 2}   \n",
       "30               4                  1    {'max_depth': 4, 'n_estimators': 1}   \n",
       "71               8                  2    {'max_depth': 8, 'n_estimators': 2}   \n",
       "72               8                  3    {'max_depth': 8, 'n_estimators': 3}   \n",
       "50               6                  1    {'max_depth': 6, 'n_estimators': 1}   \n",
       "92              10                  3   {'max_depth': 10, 'n_estimators': 3}   \n",
       "82               9                  3    {'max_depth': 9, 'n_estimators': 3}   \n",
       "40               5                  1    {'max_depth': 5, 'n_estimators': 1}   \n",
       "60               7                  1    {'max_depth': 7, 'n_estimators': 1}   \n",
       "80               9                  1    {'max_depth': 9, 'n_estimators': 1}   \n",
       "70               8                  1    {'max_depth': 8, 'n_estimators': 1}   \n",
       "90              10                  1   {'max_depth': 10, 'n_estimators': 1}   \n",
       "\n",
       "    rank_test_score  split0_test_score  split0_train_score  split1_test_score  \\\n",
       "65                1           0.892308            0.951830           0.869231   \n",
       "58                1           0.884615            0.924855           0.884615   \n",
       "37                1           0.892308            0.892100           0.884615   \n",
       "36                1           0.884615            0.903661           0.884615   \n",
       "0                 5           0.884615            0.884393           0.884615   \n",
       "16                5           0.884615            0.884393           0.884615   \n",
       "1                 5           0.884615            0.884393           0.884615   \n",
       "34                5           0.884615            0.901734           0.884615   \n",
       "28                5           0.884615            0.884393           0.884615   \n",
       "26                5           0.884615            0.886320           0.884615   \n",
       "25                5           0.884615            0.894027           0.884615   \n",
       "24                5           0.884615            0.886320           0.884615   \n",
       "19                5           0.884615            0.884393           0.884615   \n",
       "18                5           0.884615            0.884393           0.884615   \n",
       "17                5           0.884615            0.884393           0.884615   \n",
       "99                5           0.884615            0.974952           0.884615   \n",
       "15                5           0.884615            0.884393           0.884615   \n",
       "9                 5           0.884615            0.884393           0.884615   \n",
       "2                 5           0.884615            0.884393           0.884615   \n",
       "14                5           0.884615            0.882466           0.884615   \n",
       "4                 5           0.884615            0.884393           0.884615   \n",
       "5                 5           0.884615            0.884393           0.884615   \n",
       "6                 5           0.884615            0.884393           0.884615   \n",
       "7                 5           0.884615            0.884393           0.884615   \n",
       "8                 5           0.884615            0.884393           0.884615   \n",
       "3                 5           0.884615            0.884393           0.884615   \n",
       "11                5           0.884615            0.884393           0.884615   \n",
       "13                5           0.884615            0.884393           0.884615   \n",
       "45               29           0.884615            0.899807           0.884615   \n",
       "23               29           0.884615            0.884393           0.884615   \n",
       "..              ...                ...                 ...                ...   \n",
       "54               70           0.861538            0.932563           0.876923   \n",
       "79               70           0.869231            0.973025           0.884615   \n",
       "31               70           0.884615            0.895954           0.884615   \n",
       "55               74           0.869231            0.924855           0.861538   \n",
       "84               74           0.892308            0.980732           0.846154   \n",
       "96               74           0.876923            0.984586           0.876923   \n",
       "76               74           0.861538            0.963391           0.884615   \n",
       "91               78           0.876923            0.930636           0.861538   \n",
       "83               78           0.861538            0.957611           0.876923   \n",
       "63               78           0.869231            0.924855           0.869231   \n",
       "41               78           0.892308            0.905588           0.853846   \n",
       "74               82           0.861538            0.957611           0.846154   \n",
       "85               82           0.853846            0.974952           0.876923   \n",
       "52               82           0.846154            0.909441           0.869231   \n",
       "51               85           0.869231            0.921002           0.884615   \n",
       "62               86           0.853846            0.949904           0.876923   \n",
       "73               86           0.861538            0.967245           0.876923   \n",
       "93               86           0.846154            0.963391           0.861538   \n",
       "81               89           0.838462            0.942197           0.892308   \n",
       "30               89           0.846154            0.872832           0.853846   \n",
       "71               89           0.846154            0.932563           0.861538   \n",
       "72               89           0.846154            0.938343           0.876923   \n",
       "50               93           0.846154            0.907514           0.861538   \n",
       "92               94           0.861538            0.955684           0.869231   \n",
       "82               95           0.830769            0.959538           0.861538   \n",
       "40               96           0.823077            0.890173           0.815385   \n",
       "60               96           0.861538            0.901734           0.830769   \n",
       "80               98           0.838462            0.915222           0.838462   \n",
       "70               99           0.830769            0.905588           0.815385   \n",
       "90              100           0.861538            0.934489           0.823077   \n",
       "\n",
       "    split1_train_score  split2_test_score  split2_train_score  \\\n",
       "65            0.936416           0.884615            0.940270   \n",
       "58            0.926782           0.892308            0.934489   \n",
       "37            0.886320           0.884615            0.888247   \n",
       "36            0.897881           0.884615            0.884393   \n",
       "0             0.884393           0.884615            0.884393   \n",
       "16            0.884393           0.884615            0.884393   \n",
       "1             0.884393           0.884615            0.884393   \n",
       "34            0.897881           0.884615            0.888247   \n",
       "28            0.884393           0.884615            0.886320   \n",
       "26            0.886320           0.884615            0.886320   \n",
       "25            0.884393           0.884615            0.888247   \n",
       "24            0.894027           0.884615            0.884393   \n",
       "19            0.884393           0.884615            0.884393   \n",
       "18            0.884393           0.884615            0.884393   \n",
       "17            0.886320           0.884615            0.884393   \n",
       "99            0.973025           0.876923            0.982659   \n",
       "15            0.884393           0.884615            0.884393   \n",
       "9             0.884393           0.884615            0.884393   \n",
       "2             0.884393           0.884615            0.884393   \n",
       "14            0.884393           0.884615            0.884393   \n",
       "4             0.884393           0.884615            0.884393   \n",
       "5             0.884393           0.884615            0.884393   \n",
       "6             0.884393           0.884615            0.884393   \n",
       "7             0.884393           0.884615            0.884393   \n",
       "8             0.884393           0.884615            0.884393   \n",
       "3             0.884393           0.884615            0.884393   \n",
       "11            0.884393           0.884615            0.886320   \n",
       "13            0.884393           0.884615            0.884393   \n",
       "45            0.905588           0.884615            0.892100   \n",
       "23            0.886320           0.884615            0.886320   \n",
       "..                 ...                ...                 ...   \n",
       "54            0.917148           0.876923            0.930636   \n",
       "79            0.959538           0.884615            0.953757   \n",
       "31            0.888247           0.861538            0.884393   \n",
       "55            0.928709           0.869231            0.917148   \n",
       "84            0.967245           0.869231            0.951830   \n",
       "96            0.971098           0.861538            0.978805   \n",
       "76            0.959538           0.876923            0.955684   \n",
       "91            0.924855           0.861538            0.949904   \n",
       "83            0.947977           0.876923            0.969171   \n",
       "63            0.924855           0.861538            0.922929   \n",
       "41            0.907514           0.869231            0.911368   \n",
       "74            0.971098           0.876923            0.951830   \n",
       "85            0.963391           0.861538            0.965318   \n",
       "52            0.934489           0.869231            0.919075   \n",
       "51            0.917148           0.830769            0.915222   \n",
       "62            0.932563           0.869231            0.940270   \n",
       "73            0.957611           0.869231            0.957611   \n",
       "93            0.953757           0.869231            0.959538   \n",
       "81            0.922929           0.853846            0.946050   \n",
       "30            0.895954           0.853846            0.892100   \n",
       "71            0.932563           0.853846            0.934489   \n",
       "72            0.951830           0.869231            0.932563   \n",
       "50            0.903661           0.876923            0.903661   \n",
       "92            0.965318           0.830769            0.961464   \n",
       "82            0.944123           0.838462            0.951830   \n",
       "40            0.888247           0.869231            0.895954   \n",
       "60            0.921002           0.761538            0.903661   \n",
       "80            0.930636           0.823077            0.913295   \n",
       "70            0.924855           0.823077            0.903661   \n",
       "90            0.922929           0.784615            0.940270   \n",
       "\n",
       "    split3_test_score  split3_train_score  split4_test_score  \\\n",
       "65           0.900000            0.944123           0.883721   \n",
       "58           0.876923            0.921002           0.891473   \n",
       "37           0.884615            0.890173           0.883721   \n",
       "36           0.884615            0.895954           0.891473   \n",
       "0            0.884615            0.884393           0.883721   \n",
       "16           0.884615            0.884393           0.883721   \n",
       "1            0.884615            0.884393           0.883721   \n",
       "34           0.884615            0.886320           0.883721   \n",
       "28           0.884615            0.884393           0.883721   \n",
       "26           0.884615            0.884393           0.883721   \n",
       "25           0.884615            0.884393           0.883721   \n",
       "24           0.884615            0.884393           0.883721   \n",
       "19           0.884615            0.886320           0.883721   \n",
       "18           0.884615            0.884393           0.883721   \n",
       "17           0.884615            0.884393           0.883721   \n",
       "99           0.876923            0.982659           0.899225   \n",
       "15           0.884615            0.884393           0.883721   \n",
       "9            0.884615            0.884393           0.883721   \n",
       "2            0.884615            0.884393           0.883721   \n",
       "14           0.884615            0.884393           0.883721   \n",
       "4            0.884615            0.884393           0.883721   \n",
       "5            0.884615            0.884393           0.883721   \n",
       "6            0.884615            0.884393           0.883721   \n",
       "7            0.884615            0.884393           0.883721   \n",
       "8            0.884615            0.884393           0.883721   \n",
       "3            0.884615            0.884393           0.883721   \n",
       "11           0.884615            0.884393           0.883721   \n",
       "13           0.884615            0.884393           0.883721   \n",
       "45           0.876923            0.894027           0.883721   \n",
       "23           0.884615            0.884393           0.875969   \n",
       "..                ...                 ...                ...   \n",
       "54           0.846154            0.930636           0.891473   \n",
       "79           0.861538            0.963391           0.852713   \n",
       "31           0.861538            0.897881           0.860465   \n",
       "55           0.869231            0.930636           0.875969   \n",
       "84           0.876923            0.967245           0.860465   \n",
       "96           0.853846            0.967245           0.875969   \n",
       "76           0.853846            0.965318           0.868217   \n",
       "91           0.869231            0.946050           0.868217   \n",
       "83           0.846154            0.963391           0.875969   \n",
       "63           0.838462            0.936416           0.899225   \n",
       "41           0.838462            0.915222           0.883721   \n",
       "74           0.884615            0.963391           0.860465   \n",
       "85           0.861538            0.973025           0.875969   \n",
       "52           0.861538            0.917148           0.883721   \n",
       "51           0.869231            0.915222           0.868217   \n",
       "62           0.869231            0.932563           0.844961   \n",
       "73           0.846154            0.942197           0.860465   \n",
       "93           0.876923            0.963391           0.860465   \n",
       "81           0.853846            0.938343           0.860465   \n",
       "30           0.861538            0.884393           0.883721   \n",
       "71           0.853846            0.951830           0.883721   \n",
       "72           0.830769            0.947977           0.875969   \n",
       "50           0.823077            0.890173           0.875969   \n",
       "92           0.861538            0.961464           0.844961   \n",
       "82           0.846154            0.951830           0.860465   \n",
       "40           0.861538            0.870906           0.837209   \n",
       "60           0.907692            0.928709           0.844961   \n",
       "80           0.830769            0.930636           0.844961   \n",
       "70           0.815385            0.901734           0.844961   \n",
       "90           0.807692            0.934489           0.821705   \n",
       "\n",
       "    split4_train_score  std_fit_time  std_score_time  std_test_score  \\\n",
       "65            0.948077      0.000507        0.000128        0.010246   \n",
       "58            0.919231      0.000171        0.000094        0.005585   \n",
       "37            0.890385      0.000213        0.000029        0.003186   \n",
       "36            0.892308      0.000153        0.000029        0.002737   \n",
       "0             0.884615      0.000272        0.000055        0.000357   \n",
       "16            0.884615      0.000481        0.000087        0.000357   \n",
       "1             0.884615      0.000201        0.000132        0.000357   \n",
       "34            0.892308      0.000089        0.000006        0.000357   \n",
       "28            0.884615      0.000020        0.000075        0.000357   \n",
       "26            0.884615      0.000063        0.000029        0.000357   \n",
       "25            0.884615      0.000253        0.000021        0.000357   \n",
       "24            0.886538      0.000188        0.000125        0.000357   \n",
       "19            0.884615      0.000378        0.000141        0.000357   \n",
       "18            0.884615      0.001117        0.000445        0.000357   \n",
       "17            0.884615      0.001247        0.000066        0.000357   \n",
       "99            0.973077      0.000044        0.000009        0.008130   \n",
       "15            0.884615      0.000543        0.000029        0.000357   \n",
       "9             0.884615      0.000367        0.000057        0.000357   \n",
       "2             0.884615      0.001558        0.000312        0.000357   \n",
       "14            0.884615      0.001055        0.000049        0.000357   \n",
       "4             0.884615      0.000031        0.000007        0.000357   \n",
       "5             0.884615      0.000384        0.000052        0.000357   \n",
       "6             0.884615      0.000349        0.000088        0.000357   \n",
       "7             0.884615      0.002301        0.000603        0.000357   \n",
       "8             0.884615      0.000042        0.000007        0.000357   \n",
       "3             0.884615      0.000452        0.000025        0.000357   \n",
       "11            0.884615      0.000057        0.000020        0.000357   \n",
       "13            0.884615      0.000394        0.000039        0.000357   \n",
       "45            0.900000      0.000959        0.000225        0.003010   \n",
       "23            0.884615      0.000246        0.000063        0.003451   \n",
       "..                 ...           ...             ...             ...   \n",
       "54            0.919231      0.000770        0.000123        0.015452   \n",
       "79            0.982692      0.000033        0.000005        0.012614   \n",
       "31            0.894231      0.002135        0.000125        0.011489   \n",
       "55            0.934615      0.001761        0.000098        0.004565   \n",
       "84            0.975000      0.000161        0.000023        0.015506   \n",
       "96            0.976923      0.000077        0.000043        0.009589   \n",
       "76            0.959615      0.000065        0.000031        0.010894   \n",
       "91            0.936538      0.000088        0.000011        0.005721   \n",
       "83            0.950000      0.000438        0.000111        0.012175   \n",
       "63            0.948077      0.000036        0.000003        0.019424   \n",
       "41            0.907692      0.000051        0.000022        0.019556   \n",
       "74            0.951923      0.000011        0.000004        0.013501   \n",
       "85            0.971154      0.001034        0.000044        0.009012   \n",
       "52            0.911538      0.000398        0.000324        0.012226   \n",
       "51            0.926923      0.001469        0.000154        0.017907   \n",
       "62            0.950000      0.000095        0.000005        0.011655   \n",
       "73            0.959615      0.000091        0.000006        0.010254   \n",
       "93            0.955769      0.001663        0.000083        0.010254   \n",
       "81            0.940385      0.000415        0.000082        0.017809   \n",
       "30            0.888462      0.000039        0.000037        0.012878   \n",
       "71            0.928846      0.000043        0.000013        0.012878   \n",
       "72            0.942308      0.000299        0.000027        0.018293   \n",
       "50            0.901923      0.000020        0.000005        0.020227   \n",
       "92            0.944231      0.000113        0.000005        0.013908   \n",
       "82            0.965385      0.000053        0.000032        0.012070   \n",
       "40            0.875000      0.000019        0.000003        0.021040   \n",
       "60            0.901923      0.000018        0.000003        0.047584   \n",
       "80            0.925000      0.000023        0.000003        0.007521   \n",
       "70            0.903846      0.000808        0.000318        0.011084   \n",
       "90            0.942308      0.000004        0.000002        0.025083   \n",
       "\n",
       "    std_train_score  \n",
       "65         0.005464  \n",
       "58         0.005332  \n",
       "37         0.001983  \n",
       "36         0.006385  \n",
       "0          0.000089  \n",
       "16         0.000089  \n",
       "1          0.000089  \n",
       "34         0.005785  \n",
       "28         0.000753  \n",
       "26         0.000892  \n",
       "25         0.003745  \n",
       "24         0.003565  \n",
       "19         0.000753  \n",
       "18         0.000089  \n",
       "17         0.000753  \n",
       "99         0.004451  \n",
       "15         0.000089  \n",
       "9          0.000089  \n",
       "2          0.000089  \n",
       "14         0.000798  \n",
       "4          0.000089  \n",
       "5          0.000089  \n",
       "6          0.000089  \n",
       "7          0.000089  \n",
       "8          0.000089  \n",
       "3          0.000089  \n",
       "11         0.000753  \n",
       "13         0.000089  \n",
       "45         0.004795  \n",
       "23         0.000911  \n",
       "..              ...  \n",
       "54         0.006484  \n",
       "79         0.010250  \n",
       "31         0.005041  \n",
       "55         0.005926  \n",
       "84         0.009725  \n",
       "96         0.006046  \n",
       "76         0.003354  \n",
       "91         0.009326  \n",
       "83         0.007972  \n",
       "63         0.009599  \n",
       "41         0.003428  \n",
       "74         0.007334  \n",
       "85         0.004465  \n",
       "52         0.008811  \n",
       "51         0.004443  \n",
       "62         0.007787  \n",
       "73         0.008143  \n",
       "93         0.003915  \n",
       "81         0.007942  \n",
       "30         0.007944  \n",
       "71         0.008095  \n",
       "72         0.006827  \n",
       "50         0.005898  \n",
       "92         0.007373  \n",
       "82         0.007291  \n",
       "40         0.009502  \n",
       "60         0.011269  \n",
       "80         0.007420  \n",
       "70         0.008547  \n",
       "90         0.006743  \n",
       "\n",
       "[100 rows x 22 columns]"
      ]
     },
     "execution_count": 940,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnd_forest_grid.sort_values('mean_test_score', ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим, как меняется значение метрики с увеличением глубины деревьев:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 941,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_interact(n_est):\n",
    "    plt.plot(rnd_forest_grid[rnd_forest_grid['param_n_estimators'] == n_est]['param_max_depth'],\n",
    "             rnd_forest_grid[rnd_forest_grid['param_n_estimators'] == n_est]['mean_test_score'])\n",
    "    plt.xlabel('depth')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.ylim(0.5,1)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 942,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from ipywidgets import IntSlider\n",
    "from ipywidgets import interactive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 943,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bbdd9cb5be64b3d9dfeae386e727802"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "interactive(plot_interact, n_est=IntSlider(min=1, max=10, value=1, step=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Найдем лучшую модель и веса признаков:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 944,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': 4, 'n_estimators': 7}"
      ]
     },
     "execution_count": 944,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnd_forest_CV.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 945,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_model = RandomForestClassifier(max_depth = 5, n_estimators = 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 946,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>columns</th>\n",
       "      <th>weights</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>absences</td>\n",
       "      <td>0.110693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>sex_M</td>\n",
       "      <td>0.067143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>G3</td>\n",
       "      <td>0.065503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>goout</td>\n",
       "      <td>0.063576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>G2</td>\n",
       "      <td>0.061263</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     columns   weights\n",
       "12  absences  0.110693\n",
       "16     sex_M  0.067143\n",
       "6         G3  0.065503\n",
       "3      goout  0.063576\n",
       "5         G2  0.061263\n",
       "\n",
       "[5 rows x 2 columns]"
      ]
     },
     "execution_count": 946,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame({'columns': X.columns, \n",
    "              'weights': best_model.fit(X, y).feature_importances_}).sort_values('weights', \n",
    "                                                                                 ascending = False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Значения метрики для 5 лучших моделей:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 947,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.885978428351\n",
      "0.885978428351\n",
      "0.885978428351\n",
      "0.885978428351\n",
      "0.884437596302\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(rnd_forest_grid.sort_values('mean_test_score', ascending = False).iloc[i].mean_test_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Построим ROC кривую для лучшей модели, чтобы понять переобучается она или нет:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 948,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 949,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x11bd152e8>"
      ]
     },
     "execution_count": 949,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\njuBTZsea/b8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11bd40128>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fpr_train, tpr_train, _ = roc_curve(y_train, best_model.predict(X_train))\n",
    "plt.plot(fpr_train, tpr_train,  label = 'train')\n",
    "\n",
    "fpr_test, tpr_test, _ = roc_curve(y_test, best_model.predict(X_test))\n",
    "plt.plot(fpr_test, tpr_test, label = 'test')\n",
    "plt.xlabel('fpr')\n",
    "plt.ylabel('tpr')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Судя по тому, что кривые для тестовой и контрольной выборки не сильно отличаются, модель наиболее вероятно не переобучается."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 950,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 951,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "GBC = GradientBoostingClassifier()\n",
    "params = {'n_estimators': np.arange(1,11),\n",
    "          'max_depth': np.arange(1,11),\n",
    "          'learning_rate': np.arange(1,11)} \n",
    "splits = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 952,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "GBC_CV = GridSearchCV(estimator=GBC, param_grid=params, \n",
    "                             scoring='accuracy', cv=splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 953,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andrey_lukyanov/anaconda/lib/python3.6/site-packages/sklearn/ensemble/gradient_boosting.py:256: RuntimeWarning: overflow encountered in multiply\n",
      "  * tree.value[:, 0, 0].take(terminal_regions, axis=0))\n",
      "/Users/andrey_lukyanov/anaconda/lib/python3.6/site-packages/sklearn/ensemble/gradient_boosting.py:490: RuntimeWarning: invalid value encountered in multiply\n",
      "  np.sum(sample_weight * ((y * pred) - np.logaddexp(0.0, pred))))\n",
      "/Users/andrey_lukyanov/anaconda/lib/python3.6/site-packages/sklearn/ensemble/gradient_boosting.py:490: RuntimeWarning: invalid value encountered in subtract\n",
      "  np.sum(sample_weight * ((y * pred) - np.logaddexp(0.0, pred))))\n",
      "/Users/andrey_lukyanov/anaconda/lib/python3.6/site-packages/sklearn/ensemble/gradient_boosting.py:517: RuntimeWarning: overflow encountered in double_scalars\n",
      "  tree.value[leaf, 0, 0] = numerator / denominator\n",
      "/Users/andrey_lukyanov/anaconda/lib/python3.6/site-packages/sklearn/ensemble/gradient_boosting.py:490: RuntimeWarning: invalid value encountered in multiply\n",
      "  np.sum(sample_weight * ((y * pred) - np.logaddexp(0.0, pred))))\n",
      "/Users/andrey_lukyanov/anaconda/lib/python3.6/site-packages/sklearn/ensemble/gradient_boosting.py:517: RuntimeWarning: overflow encountered in double_scalars\n",
      "  tree.value[leaf, 0, 0] = numerator / denominator\n",
      "/Users/andrey_lukyanov/anaconda/lib/python3.6/site-packages/sklearn/ensemble/gradient_boosting.py:490: RuntimeWarning: invalid value encountered in multiply\n",
      "  np.sum(sample_weight * ((y * pred) - np.logaddexp(0.0, pred))))\n",
      "/Users/andrey_lukyanov/anaconda/lib/python3.6/site-packages/sklearn/ensemble/gradient_boosting.py:517: RuntimeWarning: overflow encountered in double_scalars\n",
      "  tree.value[leaf, 0, 0] = numerator / denominator\n",
      "/Users/andrey_lukyanov/anaconda/lib/python3.6/site-packages/sklearn/ensemble/gradient_boosting.py:490: RuntimeWarning: invalid value encountered in multiply\n",
      "  np.sum(sample_weight * ((y * pred) - np.logaddexp(0.0, pred))))\n",
      "/Users/andrey_lukyanov/anaconda/lib/python3.6/site-packages/sklearn/ensemble/gradient_boosting.py:256: RuntimeWarning: overflow encountered in multiply\n",
      "  * tree.value[:, 0, 0].take(terminal_regions, axis=0))\n",
      "/Users/andrey_lukyanov/anaconda/lib/python3.6/site-packages/sklearn/ensemble/gradient_boosting.py:490: RuntimeWarning: invalid value encountered in multiply\n",
      "  np.sum(sample_weight * ((y * pred) - np.logaddexp(0.0, pred))))\n",
      "/Users/andrey_lukyanov/anaconda/lib/python3.6/site-packages/sklearn/ensemble/gradient_boosting.py:256: RuntimeWarning: overflow encountered in multiply\n",
      "  * tree.value[:, 0, 0].take(terminal_regions, axis=0))\n",
      "/Users/andrey_lukyanov/anaconda/lib/python3.6/site-packages/sklearn/ensemble/gradient_boosting.py:490: RuntimeWarning: invalid value encountered in multiply\n",
      "  np.sum(sample_weight * ((y * pred) - np.logaddexp(0.0, pred))))\n",
      "/Users/andrey_lukyanov/anaconda/lib/python3.6/site-packages/sklearn/ensemble/gradient_boosting.py:256: RuntimeWarning: overflow encountered in multiply\n",
      "  * tree.value[:, 0, 0].take(terminal_regions, axis=0))\n",
      "/Users/andrey_lukyanov/anaconda/lib/python3.6/site-packages/sklearn/ensemble/gradient_boosting.py:490: RuntimeWarning: invalid value encountered in multiply\n",
      "  np.sum(sample_weight * ((y * pred) - np.logaddexp(0.0, pred))))\n",
      "/Users/andrey_lukyanov/anaconda/lib/python3.6/site-packages/sklearn/ensemble/gradient_boosting.py:256: RuntimeWarning: overflow encountered in multiply\n",
      "  * tree.value[:, 0, 0].take(terminal_regions, axis=0))\n",
      "/Users/andrey_lukyanov/anaconda/lib/python3.6/site-packages/sklearn/ensemble/gradient_boosting.py:490: RuntimeWarning: invalid value encountered in multiply\n",
      "  np.sum(sample_weight * ((y * pred) - np.logaddexp(0.0, pred))))\n",
      "/Users/andrey_lukyanov/anaconda/lib/python3.6/site-packages/sklearn/ensemble/gradient_boosting.py:256: RuntimeWarning: overflow encountered in multiply\n",
      "  * tree.value[:, 0, 0].take(terminal_regions, axis=0))\n",
      "/Users/andrey_lukyanov/anaconda/lib/python3.6/site-packages/sklearn/ensemble/gradient_boosting.py:490: RuntimeWarning: invalid value encountered in multiply\n",
      "  np.sum(sample_weight * ((y * pred) - np.logaddexp(0.0, pred))))\n",
      "/Users/andrey_lukyanov/anaconda/lib/python3.6/site-packages/sklearn/ensemble/gradient_boosting.py:256: RuntimeWarning: overflow encountered in multiply\n",
      "  * tree.value[:, 0, 0].take(terminal_regions, axis=0))\n",
      "/Users/andrey_lukyanov/anaconda/lib/python3.6/site-packages/sklearn/ensemble/gradient_boosting.py:490: RuntimeWarning: invalid value encountered in multiply\n",
      "  np.sum(sample_weight * ((y * pred) - np.logaddexp(0.0, pred))))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=StratifiedKFold(n_splits=5, random_state=64, shuffle=True),\n",
       "       error_score='raise',\n",
       "       estimator=GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "              learning_rate=0.1, loss='deviance', max_depth=3,\n",
       "              max_features=None, max_leaf_nodes=None,\n",
       "              min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "              min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "              n_estimators=100, presort='auto', random_state=None,\n",
       "              subsample=1.0, verbose=0, warm_start=False),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid={'n_estimators': array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10]), 'max_depth': array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10]), 'learning_rate': array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10])},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring='accuracy', verbose=0)"
      ]
     },
     "execution_count": 953,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GBC_CV.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 954,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>param_learning_rate</th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>param_n_estimators</th>\n",
       "      <th>params</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split3_train_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>split4_train_score</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.002438</td>\n",
       "      <td>0.000428</td>\n",
       "      <td>0.889060</td>\n",
       "      <td>0.887133</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>{'learning_rate': 1, 'max_depth': 1, 'n_estima...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.884393</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.884393</td>\n",
       "      <td>0.892308</td>\n",
       "      <td>0.892100</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.884393</td>\n",
       "      <td>0.899225</td>\n",
       "      <td>0.890385</td>\n",
       "      <td>0.000128</td>\n",
       "      <td>2.726350e-05</td>\n",
       "      <td>0.005875</td>\n",
       "      <td>0.003399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.004315</td>\n",
       "      <td>0.000522</td>\n",
       "      <td>0.887519</td>\n",
       "      <td>0.893296</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>{'learning_rate': 1, 'max_depth': 1, 'n_estima...</td>\n",
       "      <td>2</td>\n",
       "      <td>0.892308</td>\n",
       "      <td>0.892100</td>\n",
       "      <td>0.876923</td>\n",
       "      <td>0.894027</td>\n",
       "      <td>0.876923</td>\n",
       "      <td>0.895954</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.888247</td>\n",
       "      <td>0.891473</td>\n",
       "      <td>0.896154</td>\n",
       "      <td>0.001313</td>\n",
       "      <td>1.192671e-04</td>\n",
       "      <td>0.009159</td>\n",
       "      <td>0.002922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.002938</td>\n",
       "      <td>0.000528</td>\n",
       "      <td>0.885978</td>\n",
       "      <td>0.889443</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>{'learning_rate': 1, 'max_depth': 1, 'n_estima...</td>\n",
       "      <td>3</td>\n",
       "      <td>0.892308</td>\n",
       "      <td>0.886320</td>\n",
       "      <td>0.876923</td>\n",
       "      <td>0.895954</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.884393</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.884393</td>\n",
       "      <td>0.891473</td>\n",
       "      <td>0.896154</td>\n",
       "      <td>0.000115</td>\n",
       "      <td>9.174538e-05</td>\n",
       "      <td>0.005585</td>\n",
       "      <td>0.005444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.004478</td>\n",
       "      <td>0.000649</td>\n",
       "      <td>0.885978</td>\n",
       "      <td>0.893682</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>{'learning_rate': 1, 'max_depth': 1, 'n_estima...</td>\n",
       "      <td>3</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.892100</td>\n",
       "      <td>0.876923</td>\n",
       "      <td>0.894027</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.892100</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.894027</td>\n",
       "      <td>0.883721</td>\n",
       "      <td>0.896154</td>\n",
       "      <td>0.000490</td>\n",
       "      <td>2.083224e-04</td>\n",
       "      <td>0.007587</td>\n",
       "      <td>0.001507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>0.002143</td>\n",
       "      <td>0.000349</td>\n",
       "      <td>0.884438</td>\n",
       "      <td>0.884438</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>{'learning_rate': 2, 'max_depth': 1, 'n_estima...</td>\n",
       "      <td>5</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.884393</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.884393</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.884393</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.884393</td>\n",
       "      <td>0.883721</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>4.712251e-06</td>\n",
       "      <td>0.000357</td>\n",
       "      <td>0.000089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.003663</td>\n",
       "      <td>0.000426</td>\n",
       "      <td>0.882897</td>\n",
       "      <td>0.897149</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>{'learning_rate': 1, 'max_depth': 2, 'n_estima...</td>\n",
       "      <td>6</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.892100</td>\n",
       "      <td>0.876923</td>\n",
       "      <td>0.899807</td>\n",
       "      <td>0.876923</td>\n",
       "      <td>0.901734</td>\n",
       "      <td>0.861538</td>\n",
       "      <td>0.894027</td>\n",
       "      <td>0.899225</td>\n",
       "      <td>0.898077</td>\n",
       "      <td>0.001266</td>\n",
       "      <td>1.082034e-04</td>\n",
       "      <td>0.014740</td>\n",
       "      <td>0.003583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.004097</td>\n",
       "      <td>0.000525</td>\n",
       "      <td>0.879815</td>\n",
       "      <td>0.899845</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>{'learning_rate': 1, 'max_depth': 1, 'n_estima...</td>\n",
       "      <td>7</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.899807</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.897881</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.897881</td>\n",
       "      <td>0.869231</td>\n",
       "      <td>0.901734</td>\n",
       "      <td>0.875969</td>\n",
       "      <td>0.901923</td>\n",
       "      <td>0.000552</td>\n",
       "      <td>1.130305e-04</td>\n",
       "      <td>0.006263</td>\n",
       "      <td>0.001767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.004067</td>\n",
       "      <td>0.000451</td>\n",
       "      <td>0.879815</td>\n",
       "      <td>0.903313</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>{'learning_rate': 1, 'max_depth': 1, 'n_estima...</td>\n",
       "      <td>7</td>\n",
       "      <td>0.876923</td>\n",
       "      <td>0.903661</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.899807</td>\n",
       "      <td>0.853846</td>\n",
       "      <td>0.901734</td>\n",
       "      <td>0.892308</td>\n",
       "      <td>0.907514</td>\n",
       "      <td>0.875969</td>\n",
       "      <td>0.903846</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>7.240031e-05</td>\n",
       "      <td>0.015893</td>\n",
       "      <td>0.002563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.004409</td>\n",
       "      <td>0.000433</td>\n",
       "      <td>0.878274</td>\n",
       "      <td>0.904083</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>{'learning_rate': 1, 'max_depth': 1, 'n_estima...</td>\n",
       "      <td>9</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.901734</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.899807</td>\n",
       "      <td>0.853846</td>\n",
       "      <td>0.905588</td>\n",
       "      <td>0.876923</td>\n",
       "      <td>0.909441</td>\n",
       "      <td>0.875969</td>\n",
       "      <td>0.903846</td>\n",
       "      <td>0.000082</td>\n",
       "      <td>1.108541e-05</td>\n",
       "      <td>0.014952</td>\n",
       "      <td>0.003311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.003775</td>\n",
       "      <td>0.000410</td>\n",
       "      <td>0.876733</td>\n",
       "      <td>0.901771</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>{'learning_rate': 1, 'max_depth': 1, 'n_estima...</td>\n",
       "      <td>10</td>\n",
       "      <td>0.876923</td>\n",
       "      <td>0.899807</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.897881</td>\n",
       "      <td>0.853846</td>\n",
       "      <td>0.903661</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.903661</td>\n",
       "      <td>0.883721</td>\n",
       "      <td>0.903846</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>7.563545e-06</td>\n",
       "      <td>0.011812</td>\n",
       "      <td>0.002467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>0.002789</td>\n",
       "      <td>0.000350</td>\n",
       "      <td>0.876733</td>\n",
       "      <td>0.889062</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>{'learning_rate': 2, 'max_depth': 2, 'n_estima...</td>\n",
       "      <td>10</td>\n",
       "      <td>0.892308</td>\n",
       "      <td>0.888247</td>\n",
       "      <td>0.876923</td>\n",
       "      <td>0.903661</td>\n",
       "      <td>0.853846</td>\n",
       "      <td>0.886320</td>\n",
       "      <td>0.876923</td>\n",
       "      <td>0.882466</td>\n",
       "      <td>0.883721</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.000130</td>\n",
       "      <td>6.106495e-07</td>\n",
       "      <td>0.012776</td>\n",
       "      <td>0.007544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.002385</td>\n",
       "      <td>0.000461</td>\n",
       "      <td>0.875193</td>\n",
       "      <td>0.882125</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>{'learning_rate': 1, 'max_depth': 1, 'n_estima...</td>\n",
       "      <td>12</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.884393</td>\n",
       "      <td>0.838462</td>\n",
       "      <td>0.872832</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.884393</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.884393</td>\n",
       "      <td>0.883721</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.000256</td>\n",
       "      <td>2.436866e-05</td>\n",
       "      <td>0.018386</td>\n",
       "      <td>0.004647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>0.001840</td>\n",
       "      <td>0.000346</td>\n",
       "      <td>0.875193</td>\n",
       "      <td>0.882125</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>{'learning_rate': 2, 'max_depth': 1, 'n_estima...</td>\n",
       "      <td>12</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.884393</td>\n",
       "      <td>0.838462</td>\n",
       "      <td>0.872832</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.884393</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.884393</td>\n",
       "      <td>0.883721</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>4.128430e-06</td>\n",
       "      <td>0.018386</td>\n",
       "      <td>0.004647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>0.002818</td>\n",
       "      <td>0.000357</td>\n",
       "      <td>0.875193</td>\n",
       "      <td>0.882125</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>{'learning_rate': 2, 'max_depth': 1, 'n_estima...</td>\n",
       "      <td>12</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.884393</td>\n",
       "      <td>0.838462</td>\n",
       "      <td>0.872832</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.884393</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.884393</td>\n",
       "      <td>0.883721</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>1.059459e-05</td>\n",
       "      <td>0.018386</td>\n",
       "      <td>0.004647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.004800</td>\n",
       "      <td>0.000459</td>\n",
       "      <td>0.873652</td>\n",
       "      <td>0.905625</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>{'learning_rate': 1, 'max_depth': 1, 'n_estima...</td>\n",
       "      <td>15</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.903661</td>\n",
       "      <td>0.892308</td>\n",
       "      <td>0.903661</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.905588</td>\n",
       "      <td>0.876923</td>\n",
       "      <td>0.913295</td>\n",
       "      <td>0.868217</td>\n",
       "      <td>0.901923</td>\n",
       "      <td>0.000156</td>\n",
       "      <td>1.628244e-05</td>\n",
       "      <td>0.015914</td>\n",
       "      <td>0.004006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.003616</td>\n",
       "      <td>0.000392</td>\n",
       "      <td>0.873652</td>\n",
       "      <td>0.902156</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>{'learning_rate': 1, 'max_depth': 2, 'n_estima...</td>\n",
       "      <td>15</td>\n",
       "      <td>0.892308</td>\n",
       "      <td>0.897881</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.897881</td>\n",
       "      <td>0.861538</td>\n",
       "      <td>0.911368</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.897881</td>\n",
       "      <td>0.883721</td>\n",
       "      <td>0.905769</td>\n",
       "      <td>0.000374</td>\n",
       "      <td>3.545745e-05</td>\n",
       "      <td>0.017170</td>\n",
       "      <td>0.005527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.004288</td>\n",
       "      <td>0.000488</td>\n",
       "      <td>0.870570</td>\n",
       "      <td>0.906396</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>{'learning_rate': 1, 'max_depth': 2, 'n_estima...</td>\n",
       "      <td>17</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.905588</td>\n",
       "      <td>0.869231</td>\n",
       "      <td>0.907514</td>\n",
       "      <td>0.853846</td>\n",
       "      <td>0.913295</td>\n",
       "      <td>0.853846</td>\n",
       "      <td>0.903661</td>\n",
       "      <td>0.875969</td>\n",
       "      <td>0.901923</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>3.197711e-05</td>\n",
       "      <td>0.017079</td>\n",
       "      <td>0.003924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.005097</td>\n",
       "      <td>0.000411</td>\n",
       "      <td>0.870570</td>\n",
       "      <td>0.916411</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>{'learning_rate': 1, 'max_depth': 2, 'n_estima...</td>\n",
       "      <td>17</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.924855</td>\n",
       "      <td>0.876923</td>\n",
       "      <td>0.911368</td>\n",
       "      <td>0.838462</td>\n",
       "      <td>0.919075</td>\n",
       "      <td>0.869231</td>\n",
       "      <td>0.913295</td>\n",
       "      <td>0.883721</td>\n",
       "      <td>0.913462</td>\n",
       "      <td>0.000573</td>\n",
       "      <td>7.712051e-05</td>\n",
       "      <td>0.016995</td>\n",
       "      <td>0.004945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.005150</td>\n",
       "      <td>0.000547</td>\n",
       "      <td>0.867488</td>\n",
       "      <td>0.910248</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>{'learning_rate': 1, 'max_depth': 2, 'n_estima...</td>\n",
       "      <td>19</td>\n",
       "      <td>0.876923</td>\n",
       "      <td>0.909441</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.907514</td>\n",
       "      <td>0.830769</td>\n",
       "      <td>0.919075</td>\n",
       "      <td>0.861538</td>\n",
       "      <td>0.907514</td>\n",
       "      <td>0.883721</td>\n",
       "      <td>0.907692</td>\n",
       "      <td>0.000704</td>\n",
       "      <td>5.429008e-05</td>\n",
       "      <td>0.020153</td>\n",
       "      <td>0.004473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.005502</td>\n",
       "      <td>0.000380</td>\n",
       "      <td>0.861325</td>\n",
       "      <td>0.919878</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>{'learning_rate': 1, 'max_depth': 2, 'n_estima...</td>\n",
       "      <td>20</td>\n",
       "      <td>0.853846</td>\n",
       "      <td>0.924855</td>\n",
       "      <td>0.869231</td>\n",
       "      <td>0.917148</td>\n",
       "      <td>0.838462</td>\n",
       "      <td>0.921002</td>\n",
       "      <td>0.876923</td>\n",
       "      <td>0.921002</td>\n",
       "      <td>0.868217</td>\n",
       "      <td>0.915385</td>\n",
       "      <td>0.000150</td>\n",
       "      <td>2.422239e-05</td>\n",
       "      <td>0.013660</td>\n",
       "      <td>0.003315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.004217</td>\n",
       "      <td>0.000528</td>\n",
       "      <td>0.861325</td>\n",
       "      <td>0.908709</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>{'learning_rate': 1, 'max_depth': 3, 'n_estima...</td>\n",
       "      <td>20</td>\n",
       "      <td>0.876923</td>\n",
       "      <td>0.917148</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.913295</td>\n",
       "      <td>0.861538</td>\n",
       "      <td>0.913295</td>\n",
       "      <td>0.853846</td>\n",
       "      <td>0.899807</td>\n",
       "      <td>0.829457</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.000417</td>\n",
       "      <td>1.428125e-04</td>\n",
       "      <td>0.019247</td>\n",
       "      <td>0.007326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.004988</td>\n",
       "      <td>0.000483</td>\n",
       "      <td>0.856703</td>\n",
       "      <td>0.917561</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>{'learning_rate': 1, 'max_depth': 3, 'n_estima...</td>\n",
       "      <td>22</td>\n",
       "      <td>0.876923</td>\n",
       "      <td>0.922929</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.915222</td>\n",
       "      <td>0.853846</td>\n",
       "      <td>0.913295</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.907514</td>\n",
       "      <td>0.821705</td>\n",
       "      <td>0.928846</td>\n",
       "      <td>0.000369</td>\n",
       "      <td>1.153071e-04</td>\n",
       "      <td>0.022480</td>\n",
       "      <td>0.007494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.006611</td>\n",
       "      <td>0.000374</td>\n",
       "      <td>0.855162</td>\n",
       "      <td>0.923728</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>{'learning_rate': 1, 'max_depth': 2, 'n_estima...</td>\n",
       "      <td>23</td>\n",
       "      <td>0.853846</td>\n",
       "      <td>0.936416</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.915222</td>\n",
       "      <td>0.838462</td>\n",
       "      <td>0.922929</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.917148</td>\n",
       "      <td>0.852713</td>\n",
       "      <td>0.926923</td>\n",
       "      <td>0.000775</td>\n",
       "      <td>1.699214e-05</td>\n",
       "      <td>0.015730</td>\n",
       "      <td>0.007583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.005838</td>\n",
       "      <td>0.000416</td>\n",
       "      <td>0.855162</td>\n",
       "      <td>0.931431</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>{'learning_rate': 1, 'max_depth': 3, 'n_estima...</td>\n",
       "      <td>23</td>\n",
       "      <td>0.876923</td>\n",
       "      <td>0.934489</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.922929</td>\n",
       "      <td>0.853846</td>\n",
       "      <td>0.938343</td>\n",
       "      <td>0.830769</td>\n",
       "      <td>0.924855</td>\n",
       "      <td>0.829457</td>\n",
       "      <td>0.936538</td>\n",
       "      <td>0.000295</td>\n",
       "      <td>3.678866e-05</td>\n",
       "      <td>0.022792</td>\n",
       "      <td>0.006305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.009158</td>\n",
       "      <td>0.000389</td>\n",
       "      <td>0.855162</td>\n",
       "      <td>0.964946</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>{'learning_rate': 1, 'max_depth': 4, 'n_estima...</td>\n",
       "      <td>23</td>\n",
       "      <td>0.861538</td>\n",
       "      <td>0.971098</td>\n",
       "      <td>0.869231</td>\n",
       "      <td>0.967245</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.959538</td>\n",
       "      <td>0.838462</td>\n",
       "      <td>0.961464</td>\n",
       "      <td>0.860465</td>\n",
       "      <td>0.965385</td>\n",
       "      <td>0.000372</td>\n",
       "      <td>1.956637e-05</td>\n",
       "      <td>0.011203</td>\n",
       "      <td>0.004116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.003662</td>\n",
       "      <td>0.000432</td>\n",
       "      <td>0.853621</td>\n",
       "      <td>0.901381</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>{'learning_rate': 1, 'max_depth': 4, 'n_estima...</td>\n",
       "      <td>26</td>\n",
       "      <td>0.861538</td>\n",
       "      <td>0.895954</td>\n",
       "      <td>0.853846</td>\n",
       "      <td>0.890173</td>\n",
       "      <td>0.830769</td>\n",
       "      <td>0.911368</td>\n",
       "      <td>0.838462</td>\n",
       "      <td>0.892100</td>\n",
       "      <td>0.883721</td>\n",
       "      <td>0.917308</td>\n",
       "      <td>0.000479</td>\n",
       "      <td>2.061051e-05</td>\n",
       "      <td>0.018528</td>\n",
       "      <td>0.010905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.005073</td>\n",
       "      <td>0.000446</td>\n",
       "      <td>0.852080</td>\n",
       "      <td>0.927965</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>{'learning_rate': 1, 'max_depth': 4, 'n_estima...</td>\n",
       "      <td>27</td>\n",
       "      <td>0.869231</td>\n",
       "      <td>0.928709</td>\n",
       "      <td>0.869231</td>\n",
       "      <td>0.940270</td>\n",
       "      <td>0.830769</td>\n",
       "      <td>0.919075</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.921002</td>\n",
       "      <td>0.844961</td>\n",
       "      <td>0.930769</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>2.656629e-05</td>\n",
       "      <td>0.015031</td>\n",
       "      <td>0.007581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.010575</td>\n",
       "      <td>0.000419</td>\n",
       "      <td>0.852080</td>\n",
       "      <td>0.954543</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>{'learning_rate': 1, 'max_depth': 3, 'n_estima...</td>\n",
       "      <td>27</td>\n",
       "      <td>0.823077</td>\n",
       "      <td>0.963391</td>\n",
       "      <td>0.892308</td>\n",
       "      <td>0.946050</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.961464</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.940270</td>\n",
       "      <td>0.852713</td>\n",
       "      <td>0.961538</td>\n",
       "      <td>0.001518</td>\n",
       "      <td>2.768967e-05</td>\n",
       "      <td>0.022512</td>\n",
       "      <td>0.009497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.007513</td>\n",
       "      <td>0.000475</td>\n",
       "      <td>0.850539</td>\n",
       "      <td>0.939521</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>{'learning_rate': 1, 'max_depth': 3, 'n_estima...</td>\n",
       "      <td>29</td>\n",
       "      <td>0.830769</td>\n",
       "      <td>0.946050</td>\n",
       "      <td>0.892308</td>\n",
       "      <td>0.936416</td>\n",
       "      <td>0.830769</td>\n",
       "      <td>0.942197</td>\n",
       "      <td>0.838462</td>\n",
       "      <td>0.930636</td>\n",
       "      <td>0.860465</td>\n",
       "      <td>0.942308</td>\n",
       "      <td>0.000743</td>\n",
       "      <td>4.063380e-05</td>\n",
       "      <td>0.023555</td>\n",
       "      <td>0.005406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.007791</td>\n",
       "      <td>0.000493</td>\n",
       "      <td>0.850539</td>\n",
       "      <td>0.930278</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>{'learning_rate': 1, 'max_depth': 2, 'n_estima...</td>\n",
       "      <td>29</td>\n",
       "      <td>0.853846</td>\n",
       "      <td>0.938343</td>\n",
       "      <td>0.869231</td>\n",
       "      <td>0.930636</td>\n",
       "      <td>0.830769</td>\n",
       "      <td>0.926782</td>\n",
       "      <td>0.853846</td>\n",
       "      <td>0.926782</td>\n",
       "      <td>0.844961</td>\n",
       "      <td>0.928846</td>\n",
       "      <td>0.000231</td>\n",
       "      <td>2.344536e-05</td>\n",
       "      <td>0.012603</td>\n",
       "      <td>0.004282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>829</th>\n",
       "      <td>0.013829</td>\n",
       "      <td>0.000413</td>\n",
       "      <td>0.261941</td>\n",
       "      <td>0.243391</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>{'learning_rate': 9, 'max_depth': 3, 'n_estima...</td>\n",
       "      <td>970</td>\n",
       "      <td>0.346154</td>\n",
       "      <td>0.342967</td>\n",
       "      <td>0.130769</td>\n",
       "      <td>0.102119</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.163776</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.208092</td>\n",
       "      <td>0.403101</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.002139</td>\n",
       "      <td>2.482138e-05</td>\n",
       "      <td>0.098949</td>\n",
       "      <td>0.111341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>746</th>\n",
       "      <td>0.017846</td>\n",
       "      <td>0.000396</td>\n",
       "      <td>0.261941</td>\n",
       "      <td>0.277348</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>{'learning_rate': 8, 'max_depth': 5, 'n_estima...</td>\n",
       "      <td>970</td>\n",
       "      <td>0.169231</td>\n",
       "      <td>0.183044</td>\n",
       "      <td>0.284615</td>\n",
       "      <td>0.312139</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.350674</td>\n",
       "      <td>0.276923</td>\n",
       "      <td>0.260116</td>\n",
       "      <td>0.279070</td>\n",
       "      <td>0.280769</td>\n",
       "      <td>0.001416</td>\n",
       "      <td>7.329035e-06</td>\n",
       "      <td>0.047097</td>\n",
       "      <td>0.056194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>945</th>\n",
       "      <td>0.014981</td>\n",
       "      <td>0.000412</td>\n",
       "      <td>0.257319</td>\n",
       "      <td>0.280866</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>{'learning_rate': 10, 'max_depth': 5, 'n_estim...</td>\n",
       "      <td>973</td>\n",
       "      <td>0.161538</td>\n",
       "      <td>0.188825</td>\n",
       "      <td>0.253846</td>\n",
       "      <td>0.344894</td>\n",
       "      <td>0.338462</td>\n",
       "      <td>0.375723</td>\n",
       "      <td>0.369231</td>\n",
       "      <td>0.342967</td>\n",
       "      <td>0.162791</td>\n",
       "      <td>0.151923</td>\n",
       "      <td>0.001392</td>\n",
       "      <td>3.638239e-05</td>\n",
       "      <td>0.086277</td>\n",
       "      <td>0.091708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>615</th>\n",
       "      <td>0.005353</td>\n",
       "      <td>0.000368</td>\n",
       "      <td>0.254237</td>\n",
       "      <td>0.224486</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>{'learning_rate': 7, 'max_depth': 2, 'n_estima...</td>\n",
       "      <td>974</td>\n",
       "      <td>0.207692</td>\n",
       "      <td>0.210019</td>\n",
       "      <td>0.161538</td>\n",
       "      <td>0.127168</td>\n",
       "      <td>0.207692</td>\n",
       "      <td>0.156069</td>\n",
       "      <td>0.238462</td>\n",
       "      <td>0.169557</td>\n",
       "      <td>0.457364</td>\n",
       "      <td>0.459615</td>\n",
       "      <td>0.000268</td>\n",
       "      <td>5.866454e-06</td>\n",
       "      <td>0.104117</td>\n",
       "      <td>0.120552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>618</th>\n",
       "      <td>0.007522</td>\n",
       "      <td>0.000439</td>\n",
       "      <td>0.254237</td>\n",
       "      <td>0.224486</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>{'learning_rate': 7, 'max_depth': 2, 'n_estima...</td>\n",
       "      <td>974</td>\n",
       "      <td>0.207692</td>\n",
       "      <td>0.210019</td>\n",
       "      <td>0.161538</td>\n",
       "      <td>0.127168</td>\n",
       "      <td>0.207692</td>\n",
       "      <td>0.156069</td>\n",
       "      <td>0.238462</td>\n",
       "      <td>0.169557</td>\n",
       "      <td>0.457364</td>\n",
       "      <td>0.459615</td>\n",
       "      <td>0.000514</td>\n",
       "      <td>4.084761e-05</td>\n",
       "      <td>0.104117</td>\n",
       "      <td>0.120552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>617</th>\n",
       "      <td>0.006580</td>\n",
       "      <td>0.000371</td>\n",
       "      <td>0.254237</td>\n",
       "      <td>0.224486</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>{'learning_rate': 7, 'max_depth': 2, 'n_estima...</td>\n",
       "      <td>974</td>\n",
       "      <td>0.207692</td>\n",
       "      <td>0.210019</td>\n",
       "      <td>0.161538</td>\n",
       "      <td>0.127168</td>\n",
       "      <td>0.207692</td>\n",
       "      <td>0.156069</td>\n",
       "      <td>0.238462</td>\n",
       "      <td>0.169557</td>\n",
       "      <td>0.457364</td>\n",
       "      <td>0.459615</td>\n",
       "      <td>0.000384</td>\n",
       "      <td>5.840039e-06</td>\n",
       "      <td>0.104117</td>\n",
       "      <td>0.120552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>616</th>\n",
       "      <td>0.006437</td>\n",
       "      <td>0.000397</td>\n",
       "      <td>0.254237</td>\n",
       "      <td>0.224486</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>{'learning_rate': 7, 'max_depth': 2, 'n_estima...</td>\n",
       "      <td>974</td>\n",
       "      <td>0.207692</td>\n",
       "      <td>0.210019</td>\n",
       "      <td>0.161538</td>\n",
       "      <td>0.127168</td>\n",
       "      <td>0.207692</td>\n",
       "      <td>0.156069</td>\n",
       "      <td>0.238462</td>\n",
       "      <td>0.169557</td>\n",
       "      <td>0.457364</td>\n",
       "      <td>0.459615</td>\n",
       "      <td>0.000374</td>\n",
       "      <td>5.914849e-05</td>\n",
       "      <td>0.104117</td>\n",
       "      <td>0.120552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>619</th>\n",
       "      <td>0.009525</td>\n",
       "      <td>0.000486</td>\n",
       "      <td>0.254237</td>\n",
       "      <td>0.224486</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>{'learning_rate': 7, 'max_depth': 2, 'n_estima...</td>\n",
       "      <td>974</td>\n",
       "      <td>0.207692</td>\n",
       "      <td>0.210019</td>\n",
       "      <td>0.161538</td>\n",
       "      <td>0.127168</td>\n",
       "      <td>0.207692</td>\n",
       "      <td>0.156069</td>\n",
       "      <td>0.238462</td>\n",
       "      <td>0.169557</td>\n",
       "      <td>0.457364</td>\n",
       "      <td>0.459615</td>\n",
       "      <td>0.000344</td>\n",
       "      <td>1.810136e-05</td>\n",
       "      <td>0.104117</td>\n",
       "      <td>0.120552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>614</th>\n",
       "      <td>0.004829</td>\n",
       "      <td>0.000381</td>\n",
       "      <td>0.254237</td>\n",
       "      <td>0.224486</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>{'learning_rate': 7, 'max_depth': 2, 'n_estima...</td>\n",
       "      <td>974</td>\n",
       "      <td>0.207692</td>\n",
       "      <td>0.210019</td>\n",
       "      <td>0.161538</td>\n",
       "      <td>0.127168</td>\n",
       "      <td>0.207692</td>\n",
       "      <td>0.156069</td>\n",
       "      <td>0.238462</td>\n",
       "      <td>0.169557</td>\n",
       "      <td>0.457364</td>\n",
       "      <td>0.459615</td>\n",
       "      <td>0.000288</td>\n",
       "      <td>2.362149e-05</td>\n",
       "      <td>0.104117</td>\n",
       "      <td>0.120552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>613</th>\n",
       "      <td>0.004684</td>\n",
       "      <td>0.000499</td>\n",
       "      <td>0.254237</td>\n",
       "      <td>0.224486</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>{'learning_rate': 7, 'max_depth': 2, 'n_estima...</td>\n",
       "      <td>974</td>\n",
       "      <td>0.207692</td>\n",
       "      <td>0.210019</td>\n",
       "      <td>0.161538</td>\n",
       "      <td>0.127168</td>\n",
       "      <td>0.207692</td>\n",
       "      <td>0.156069</td>\n",
       "      <td>0.238462</td>\n",
       "      <td>0.169557</td>\n",
       "      <td>0.457364</td>\n",
       "      <td>0.459615</td>\n",
       "      <td>0.000291</td>\n",
       "      <td>1.223498e-04</td>\n",
       "      <td>0.104117</td>\n",
       "      <td>0.120552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417</th>\n",
       "      <td>0.006855</td>\n",
       "      <td>0.000398</td>\n",
       "      <td>0.229584</td>\n",
       "      <td>0.198396</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>{'learning_rate': 5, 'max_depth': 2, 'n_estima...</td>\n",
       "      <td>981</td>\n",
       "      <td>0.207692</td>\n",
       "      <td>0.210019</td>\n",
       "      <td>0.123077</td>\n",
       "      <td>0.104046</td>\n",
       "      <td>0.407692</td>\n",
       "      <td>0.344894</td>\n",
       "      <td>0.238462</td>\n",
       "      <td>0.169557</td>\n",
       "      <td>0.170543</td>\n",
       "      <td>0.163462</td>\n",
       "      <td>0.000425</td>\n",
       "      <td>9.755487e-06</td>\n",
       "      <td>0.097114</td>\n",
       "      <td>0.080686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416</th>\n",
       "      <td>0.006659</td>\n",
       "      <td>0.000422</td>\n",
       "      <td>0.229584</td>\n",
       "      <td>0.198396</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>{'learning_rate': 5, 'max_depth': 2, 'n_estima...</td>\n",
       "      <td>981</td>\n",
       "      <td>0.207692</td>\n",
       "      <td>0.210019</td>\n",
       "      <td>0.123077</td>\n",
       "      <td>0.104046</td>\n",
       "      <td>0.407692</td>\n",
       "      <td>0.344894</td>\n",
       "      <td>0.238462</td>\n",
       "      <td>0.169557</td>\n",
       "      <td>0.170543</td>\n",
       "      <td>0.163462</td>\n",
       "      <td>0.000878</td>\n",
       "      <td>1.652500e-05</td>\n",
       "      <td>0.097114</td>\n",
       "      <td>0.080686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>414</th>\n",
       "      <td>0.004706</td>\n",
       "      <td>0.000364</td>\n",
       "      <td>0.229584</td>\n",
       "      <td>0.198396</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>{'learning_rate': 5, 'max_depth': 2, 'n_estima...</td>\n",
       "      <td>981</td>\n",
       "      <td>0.207692</td>\n",
       "      <td>0.210019</td>\n",
       "      <td>0.123077</td>\n",
       "      <td>0.104046</td>\n",
       "      <td>0.407692</td>\n",
       "      <td>0.344894</td>\n",
       "      <td>0.238462</td>\n",
       "      <td>0.169557</td>\n",
       "      <td>0.170543</td>\n",
       "      <td>0.163462</td>\n",
       "      <td>0.000210</td>\n",
       "      <td>1.846782e-06</td>\n",
       "      <td>0.097114</td>\n",
       "      <td>0.080686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>415</th>\n",
       "      <td>0.005373</td>\n",
       "      <td>0.000375</td>\n",
       "      <td>0.229584</td>\n",
       "      <td>0.198396</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>{'learning_rate': 5, 'max_depth': 2, 'n_estima...</td>\n",
       "      <td>981</td>\n",
       "      <td>0.207692</td>\n",
       "      <td>0.210019</td>\n",
       "      <td>0.123077</td>\n",
       "      <td>0.104046</td>\n",
       "      <td>0.407692</td>\n",
       "      <td>0.344894</td>\n",
       "      <td>0.238462</td>\n",
       "      <td>0.169557</td>\n",
       "      <td>0.170543</td>\n",
       "      <td>0.163462</td>\n",
       "      <td>0.000306</td>\n",
       "      <td>2.197852e-05</td>\n",
       "      <td>0.097114</td>\n",
       "      <td>0.080686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>418</th>\n",
       "      <td>0.007935</td>\n",
       "      <td>0.000453</td>\n",
       "      <td>0.229584</td>\n",
       "      <td>0.198396</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>{'learning_rate': 5, 'max_depth': 2, 'n_estima...</td>\n",
       "      <td>981</td>\n",
       "      <td>0.207692</td>\n",
       "      <td>0.210019</td>\n",
       "      <td>0.123077</td>\n",
       "      <td>0.104046</td>\n",
       "      <td>0.407692</td>\n",
       "      <td>0.344894</td>\n",
       "      <td>0.238462</td>\n",
       "      <td>0.169557</td>\n",
       "      <td>0.170543</td>\n",
       "      <td>0.163462</td>\n",
       "      <td>0.000735</td>\n",
       "      <td>1.774667e-05</td>\n",
       "      <td>0.097114</td>\n",
       "      <td>0.080686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>419</th>\n",
       "      <td>0.008984</td>\n",
       "      <td>0.000492</td>\n",
       "      <td>0.229584</td>\n",
       "      <td>0.198396</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>{'learning_rate': 5, 'max_depth': 2, 'n_estima...</td>\n",
       "      <td>981</td>\n",
       "      <td>0.207692</td>\n",
       "      <td>0.210019</td>\n",
       "      <td>0.123077</td>\n",
       "      <td>0.104046</td>\n",
       "      <td>0.407692</td>\n",
       "      <td>0.344894</td>\n",
       "      <td>0.238462</td>\n",
       "      <td>0.169557</td>\n",
       "      <td>0.170543</td>\n",
       "      <td>0.163462</td>\n",
       "      <td>0.000288</td>\n",
       "      <td>3.751815e-05</td>\n",
       "      <td>0.097114</td>\n",
       "      <td>0.080686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413</th>\n",
       "      <td>0.004102</td>\n",
       "      <td>0.000364</td>\n",
       "      <td>0.226502</td>\n",
       "      <td>0.196083</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>{'learning_rate': 5, 'max_depth': 2, 'n_estima...</td>\n",
       "      <td>987</td>\n",
       "      <td>0.207692</td>\n",
       "      <td>0.210019</td>\n",
       "      <td>0.123077</td>\n",
       "      <td>0.104046</td>\n",
       "      <td>0.238462</td>\n",
       "      <td>0.167630</td>\n",
       "      <td>0.392308</td>\n",
       "      <td>0.335260</td>\n",
       "      <td>0.170543</td>\n",
       "      <td>0.163462</td>\n",
       "      <td>0.000142</td>\n",
       "      <td>9.658277e-06</td>\n",
       "      <td>0.091495</td>\n",
       "      <td>0.077342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>407</th>\n",
       "      <td>0.004605</td>\n",
       "      <td>0.000396</td>\n",
       "      <td>0.224961</td>\n",
       "      <td>0.225774</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>{'learning_rate': 5, 'max_depth': 1, 'n_estima...</td>\n",
       "      <td>988</td>\n",
       "      <td>0.115385</td>\n",
       "      <td>0.115607</td>\n",
       "      <td>0.161538</td>\n",
       "      <td>0.127168</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.655106</td>\n",
       "      <td>0.115385</td>\n",
       "      <td>0.115607</td>\n",
       "      <td>0.116279</td>\n",
       "      <td>0.115385</td>\n",
       "      <td>0.001149</td>\n",
       "      <td>5.023284e-05</td>\n",
       "      <td>0.196206</td>\n",
       "      <td>0.214713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>403</th>\n",
       "      <td>0.002736</td>\n",
       "      <td>0.000351</td>\n",
       "      <td>0.224961</td>\n",
       "      <td>0.225774</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>{'learning_rate': 5, 'max_depth': 1, 'n_estima...</td>\n",
       "      <td>988</td>\n",
       "      <td>0.115385</td>\n",
       "      <td>0.115607</td>\n",
       "      <td>0.161538</td>\n",
       "      <td>0.127168</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.655106</td>\n",
       "      <td>0.115385</td>\n",
       "      <td>0.115607</td>\n",
       "      <td>0.116279</td>\n",
       "      <td>0.115385</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>1.435272e-06</td>\n",
       "      <td>0.196206</td>\n",
       "      <td>0.214713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404</th>\n",
       "      <td>0.003092</td>\n",
       "      <td>0.000372</td>\n",
       "      <td>0.224961</td>\n",
       "      <td>0.225774</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>{'learning_rate': 5, 'max_depth': 1, 'n_estima...</td>\n",
       "      <td>988</td>\n",
       "      <td>0.115385</td>\n",
       "      <td>0.115607</td>\n",
       "      <td>0.161538</td>\n",
       "      <td>0.127168</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.655106</td>\n",
       "      <td>0.115385</td>\n",
       "      <td>0.115607</td>\n",
       "      <td>0.116279</td>\n",
       "      <td>0.115385</td>\n",
       "      <td>0.000085</td>\n",
       "      <td>2.610630e-05</td>\n",
       "      <td>0.196206</td>\n",
       "      <td>0.214713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>405</th>\n",
       "      <td>0.003787</td>\n",
       "      <td>0.000391</td>\n",
       "      <td>0.224961</td>\n",
       "      <td>0.225774</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>{'learning_rate': 5, 'max_depth': 1, 'n_estima...</td>\n",
       "      <td>988</td>\n",
       "      <td>0.115385</td>\n",
       "      <td>0.115607</td>\n",
       "      <td>0.161538</td>\n",
       "      <td>0.127168</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.655106</td>\n",
       "      <td>0.115385</td>\n",
       "      <td>0.115607</td>\n",
       "      <td>0.116279</td>\n",
       "      <td>0.115385</td>\n",
       "      <td>0.000506</td>\n",
       "      <td>4.432072e-05</td>\n",
       "      <td>0.196206</td>\n",
       "      <td>0.214713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>409</th>\n",
       "      <td>0.005509</td>\n",
       "      <td>0.000512</td>\n",
       "      <td>0.224961</td>\n",
       "      <td>0.225774</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>{'learning_rate': 5, 'max_depth': 1, 'n_estima...</td>\n",
       "      <td>988</td>\n",
       "      <td>0.115385</td>\n",
       "      <td>0.115607</td>\n",
       "      <td>0.161538</td>\n",
       "      <td>0.127168</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.655106</td>\n",
       "      <td>0.115385</td>\n",
       "      <td>0.115607</td>\n",
       "      <td>0.116279</td>\n",
       "      <td>0.115385</td>\n",
       "      <td>0.000556</td>\n",
       "      <td>1.447084e-04</td>\n",
       "      <td>0.196206</td>\n",
       "      <td>0.214713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>408</th>\n",
       "      <td>0.004930</td>\n",
       "      <td>0.000452</td>\n",
       "      <td>0.224961</td>\n",
       "      <td>0.225774</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>{'learning_rate': 5, 'max_depth': 1, 'n_estima...</td>\n",
       "      <td>988</td>\n",
       "      <td>0.115385</td>\n",
       "      <td>0.115607</td>\n",
       "      <td>0.161538</td>\n",
       "      <td>0.127168</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.655106</td>\n",
       "      <td>0.115385</td>\n",
       "      <td>0.115607</td>\n",
       "      <td>0.116279</td>\n",
       "      <td>0.115385</td>\n",
       "      <td>0.000778</td>\n",
       "      <td>1.534926e-04</td>\n",
       "      <td>0.196206</td>\n",
       "      <td>0.214713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>406</th>\n",
       "      <td>0.003594</td>\n",
       "      <td>0.000351</td>\n",
       "      <td>0.224961</td>\n",
       "      <td>0.225774</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>{'learning_rate': 5, 'max_depth': 1, 'n_estima...</td>\n",
       "      <td>988</td>\n",
       "      <td>0.115385</td>\n",
       "      <td>0.115607</td>\n",
       "      <td>0.161538</td>\n",
       "      <td>0.127168</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.655106</td>\n",
       "      <td>0.115385</td>\n",
       "      <td>0.115607</td>\n",
       "      <td>0.116279</td>\n",
       "      <td>0.115385</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>1.081065e-06</td>\n",
       "      <td>0.196206</td>\n",
       "      <td>0.214713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>0.004780</td>\n",
       "      <td>0.000460</td>\n",
       "      <td>0.124807</td>\n",
       "      <td>0.117875</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>{'learning_rate': 3, 'max_depth': 1, 'n_estima...</td>\n",
       "      <td>995</td>\n",
       "      <td>0.115385</td>\n",
       "      <td>0.115607</td>\n",
       "      <td>0.161538</td>\n",
       "      <td>0.127168</td>\n",
       "      <td>0.115385</td>\n",
       "      <td>0.115607</td>\n",
       "      <td>0.115385</td>\n",
       "      <td>0.115607</td>\n",
       "      <td>0.116279</td>\n",
       "      <td>0.115385</td>\n",
       "      <td>0.000197</td>\n",
       "      <td>2.905437e-05</td>\n",
       "      <td>0.018386</td>\n",
       "      <td>0.004647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>0.004338</td>\n",
       "      <td>0.000518</td>\n",
       "      <td>0.124807</td>\n",
       "      <td>0.117875</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>{'learning_rate': 3, 'max_depth': 1, 'n_estima...</td>\n",
       "      <td>995</td>\n",
       "      <td>0.115385</td>\n",
       "      <td>0.115607</td>\n",
       "      <td>0.161538</td>\n",
       "      <td>0.127168</td>\n",
       "      <td>0.115385</td>\n",
       "      <td>0.115607</td>\n",
       "      <td>0.115385</td>\n",
       "      <td>0.115607</td>\n",
       "      <td>0.116279</td>\n",
       "      <td>0.115385</td>\n",
       "      <td>0.000201</td>\n",
       "      <td>7.573408e-05</td>\n",
       "      <td>0.018386</td>\n",
       "      <td>0.004647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>0.004223</td>\n",
       "      <td>0.000474</td>\n",
       "      <td>0.124807</td>\n",
       "      <td>0.117875</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>{'learning_rate': 3, 'max_depth': 1, 'n_estima...</td>\n",
       "      <td>995</td>\n",
       "      <td>0.115385</td>\n",
       "      <td>0.115607</td>\n",
       "      <td>0.161538</td>\n",
       "      <td>0.127168</td>\n",
       "      <td>0.115385</td>\n",
       "      <td>0.115607</td>\n",
       "      <td>0.115385</td>\n",
       "      <td>0.115607</td>\n",
       "      <td>0.116279</td>\n",
       "      <td>0.115385</td>\n",
       "      <td>0.000388</td>\n",
       "      <td>7.798701e-05</td>\n",
       "      <td>0.018386</td>\n",
       "      <td>0.004647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>0.005191</td>\n",
       "      <td>0.000568</td>\n",
       "      <td>0.124807</td>\n",
       "      <td>0.117875</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>{'learning_rate': 3, 'max_depth': 1, 'n_estima...</td>\n",
       "      <td>995</td>\n",
       "      <td>0.115385</td>\n",
       "      <td>0.115607</td>\n",
       "      <td>0.161538</td>\n",
       "      <td>0.127168</td>\n",
       "      <td>0.115385</td>\n",
       "      <td>0.115607</td>\n",
       "      <td>0.115385</td>\n",
       "      <td>0.115607</td>\n",
       "      <td>0.116279</td>\n",
       "      <td>0.115385</td>\n",
       "      <td>0.000243</td>\n",
       "      <td>9.925496e-05</td>\n",
       "      <td>0.018386</td>\n",
       "      <td>0.004647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>0.005420</td>\n",
       "      <td>0.000532</td>\n",
       "      <td>0.124807</td>\n",
       "      <td>0.117875</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>{'learning_rate': 3, 'max_depth': 1, 'n_estima...</td>\n",
       "      <td>995</td>\n",
       "      <td>0.115385</td>\n",
       "      <td>0.115607</td>\n",
       "      <td>0.161538</td>\n",
       "      <td>0.127168</td>\n",
       "      <td>0.115385</td>\n",
       "      <td>0.115607</td>\n",
       "      <td>0.115385</td>\n",
       "      <td>0.115607</td>\n",
       "      <td>0.116279</td>\n",
       "      <td>0.115385</td>\n",
       "      <td>0.000612</td>\n",
       "      <td>1.101124e-04</td>\n",
       "      <td>0.018386</td>\n",
       "      <td>0.004647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>0.005147</td>\n",
       "      <td>0.000499</td>\n",
       "      <td>0.124807</td>\n",
       "      <td>0.117875</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>{'learning_rate': 3, 'max_depth': 1, 'n_estima...</td>\n",
       "      <td>995</td>\n",
       "      <td>0.115385</td>\n",
       "      <td>0.115607</td>\n",
       "      <td>0.161538</td>\n",
       "      <td>0.127168</td>\n",
       "      <td>0.115385</td>\n",
       "      <td>0.115607</td>\n",
       "      <td>0.115385</td>\n",
       "      <td>0.115607</td>\n",
       "      <td>0.116279</td>\n",
       "      <td>0.115385</td>\n",
       "      <td>0.000464</td>\n",
       "      <td>1.330406e-04</td>\n",
       "      <td>0.018386</td>\n",
       "      <td>0.004647</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     mean_fit_time  mean_score_time  mean_test_score  mean_train_score  \\\n",
       "1         0.002438         0.000428         0.889060          0.887133   \n",
       "3         0.004315         0.000522         0.887519          0.893296   \n",
       "2         0.002938         0.000528         0.885978          0.889443   \n",
       "4         0.004478         0.000649         0.885978          0.893682   \n",
       "101       0.002143         0.000349         0.884438          0.884438   \n",
       "11        0.003663         0.000426         0.882897          0.897149   \n",
       "5         0.004097         0.000525         0.879815          0.899845   \n",
       "7         0.004067         0.000451         0.879815          0.903313   \n",
       "8         0.004409         0.000433         0.878274          0.904083   \n",
       "6         0.003775         0.000410         0.876733          0.901771   \n",
       "111       0.002789         0.000350         0.876733          0.889062   \n",
       "0         0.002385         0.000461         0.875193          0.882125   \n",
       "100       0.001840         0.000346         0.875193          0.882125   \n",
       "103       0.002818         0.000357         0.875193          0.882125   \n",
       "9         0.004800         0.000459         0.873652          0.905625   \n",
       "12        0.003616         0.000392         0.873652          0.902156   \n",
       "13        0.004288         0.000488         0.870570          0.906396   \n",
       "15        0.005097         0.000411         0.870570          0.916411   \n",
       "14        0.005150         0.000547         0.867488          0.910248   \n",
       "16        0.005502         0.000380         0.861325          0.919878   \n",
       "21        0.004217         0.000528         0.861325          0.908709   \n",
       "22        0.004988         0.000483         0.856703          0.917561   \n",
       "17        0.006611         0.000374         0.855162          0.923728   \n",
       "23        0.005838         0.000416         0.855162          0.931431   \n",
       "34        0.009158         0.000389         0.855162          0.964946   \n",
       "30        0.003662         0.000432         0.853621          0.901381   \n",
       "31        0.005073         0.000446         0.852080          0.927965   \n",
       "27        0.010575         0.000419         0.852080          0.954543   \n",
       "24        0.007513         0.000475         0.850539          0.939521   \n",
       "19        0.007791         0.000493         0.850539          0.930278   \n",
       "..             ...              ...              ...               ...   \n",
       "829       0.013829         0.000413         0.261941          0.243391   \n",
       "746       0.017846         0.000396         0.261941          0.277348   \n",
       "945       0.014981         0.000412         0.257319          0.280866   \n",
       "615       0.005353         0.000368         0.254237          0.224486   \n",
       "618       0.007522         0.000439         0.254237          0.224486   \n",
       "617       0.006580         0.000371         0.254237          0.224486   \n",
       "616       0.006437         0.000397         0.254237          0.224486   \n",
       "619       0.009525         0.000486         0.254237          0.224486   \n",
       "614       0.004829         0.000381         0.254237          0.224486   \n",
       "613       0.004684         0.000499         0.254237          0.224486   \n",
       "417       0.006855         0.000398         0.229584          0.198396   \n",
       "416       0.006659         0.000422         0.229584          0.198396   \n",
       "414       0.004706         0.000364         0.229584          0.198396   \n",
       "415       0.005373         0.000375         0.229584          0.198396   \n",
       "418       0.007935         0.000453         0.229584          0.198396   \n",
       "419       0.008984         0.000492         0.229584          0.198396   \n",
       "413       0.004102         0.000364         0.226502          0.196083   \n",
       "407       0.004605         0.000396         0.224961          0.225774   \n",
       "403       0.002736         0.000351         0.224961          0.225774   \n",
       "404       0.003092         0.000372         0.224961          0.225774   \n",
       "405       0.003787         0.000391         0.224961          0.225774   \n",
       "409       0.005509         0.000512         0.224961          0.225774   \n",
       "408       0.004930         0.000452         0.224961          0.225774   \n",
       "406       0.003594         0.000351         0.224961          0.225774   \n",
       "206       0.004780         0.000460         0.124807          0.117875   \n",
       "203       0.004338         0.000518         0.124807          0.117875   \n",
       "205       0.004223         0.000474         0.124807          0.117875   \n",
       "207       0.005191         0.000568         0.124807          0.117875   \n",
       "209       0.005420         0.000532         0.124807          0.117875   \n",
       "208       0.005147         0.000499         0.124807          0.117875   \n",
       "\n",
       "    param_learning_rate param_max_depth param_n_estimators  \\\n",
       "1                     1               1                  2   \n",
       "3                     1               1                  4   \n",
       "2                     1               1                  3   \n",
       "4                     1               1                  5   \n",
       "101                   2               1                  2   \n",
       "11                    1               2                  2   \n",
       "5                     1               1                  6   \n",
       "7                     1               1                  8   \n",
       "8                     1               1                  9   \n",
       "6                     1               1                  7   \n",
       "111                   2               2                  2   \n",
       "0                     1               1                  1   \n",
       "100                   2               1                  1   \n",
       "103                   2               1                  4   \n",
       "9                     1               1                 10   \n",
       "12                    1               2                  3   \n",
       "13                    1               2                  4   \n",
       "15                    1               2                  6   \n",
       "14                    1               2                  5   \n",
       "16                    1               2                  7   \n",
       "21                    1               3                  2   \n",
       "22                    1               3                  3   \n",
       "17                    1               2                  8   \n",
       "23                    1               3                  4   \n",
       "34                    1               4                  5   \n",
       "30                    1               4                  1   \n",
       "31                    1               4                  2   \n",
       "27                    1               3                  8   \n",
       "24                    1               3                  5   \n",
       "19                    1               2                 10   \n",
       "..                  ...             ...                ...   \n",
       "829                   9               3                 10   \n",
       "746                   8               5                  7   \n",
       "945                  10               5                  6   \n",
       "615                   7               2                  6   \n",
       "618                   7               2                  9   \n",
       "617                   7               2                  8   \n",
       "616                   7               2                  7   \n",
       "619                   7               2                 10   \n",
       "614                   7               2                  5   \n",
       "613                   7               2                  4   \n",
       "417                   5               2                  8   \n",
       "416                   5               2                  7   \n",
       "414                   5               2                  5   \n",
       "415                   5               2                  6   \n",
       "418                   5               2                  9   \n",
       "419                   5               2                 10   \n",
       "413                   5               2                  4   \n",
       "407                   5               1                  8   \n",
       "403                   5               1                  4   \n",
       "404                   5               1                  5   \n",
       "405                   5               1                  6   \n",
       "409                   5               1                 10   \n",
       "408                   5               1                  9   \n",
       "406                   5               1                  7   \n",
       "206                   3               1                  7   \n",
       "203                   3               1                  4   \n",
       "205                   3               1                  6   \n",
       "207                   3               1                  8   \n",
       "209                   3               1                 10   \n",
       "208                   3               1                  9   \n",
       "\n",
       "                                                params  rank_test_score  \\\n",
       "1    {'learning_rate': 1, 'max_depth': 1, 'n_estima...                1   \n",
       "3    {'learning_rate': 1, 'max_depth': 1, 'n_estima...                2   \n",
       "2    {'learning_rate': 1, 'max_depth': 1, 'n_estima...                3   \n",
       "4    {'learning_rate': 1, 'max_depth': 1, 'n_estima...                3   \n",
       "101  {'learning_rate': 2, 'max_depth': 1, 'n_estima...                5   \n",
       "11   {'learning_rate': 1, 'max_depth': 2, 'n_estima...                6   \n",
       "5    {'learning_rate': 1, 'max_depth': 1, 'n_estima...                7   \n",
       "7    {'learning_rate': 1, 'max_depth': 1, 'n_estima...                7   \n",
       "8    {'learning_rate': 1, 'max_depth': 1, 'n_estima...                9   \n",
       "6    {'learning_rate': 1, 'max_depth': 1, 'n_estima...               10   \n",
       "111  {'learning_rate': 2, 'max_depth': 2, 'n_estima...               10   \n",
       "0    {'learning_rate': 1, 'max_depth': 1, 'n_estima...               12   \n",
       "100  {'learning_rate': 2, 'max_depth': 1, 'n_estima...               12   \n",
       "103  {'learning_rate': 2, 'max_depth': 1, 'n_estima...               12   \n",
       "9    {'learning_rate': 1, 'max_depth': 1, 'n_estima...               15   \n",
       "12   {'learning_rate': 1, 'max_depth': 2, 'n_estima...               15   \n",
       "13   {'learning_rate': 1, 'max_depth': 2, 'n_estima...               17   \n",
       "15   {'learning_rate': 1, 'max_depth': 2, 'n_estima...               17   \n",
       "14   {'learning_rate': 1, 'max_depth': 2, 'n_estima...               19   \n",
       "16   {'learning_rate': 1, 'max_depth': 2, 'n_estima...               20   \n",
       "21   {'learning_rate': 1, 'max_depth': 3, 'n_estima...               20   \n",
       "22   {'learning_rate': 1, 'max_depth': 3, 'n_estima...               22   \n",
       "17   {'learning_rate': 1, 'max_depth': 2, 'n_estima...               23   \n",
       "23   {'learning_rate': 1, 'max_depth': 3, 'n_estima...               23   \n",
       "34   {'learning_rate': 1, 'max_depth': 4, 'n_estima...               23   \n",
       "30   {'learning_rate': 1, 'max_depth': 4, 'n_estima...               26   \n",
       "31   {'learning_rate': 1, 'max_depth': 4, 'n_estima...               27   \n",
       "27   {'learning_rate': 1, 'max_depth': 3, 'n_estima...               27   \n",
       "24   {'learning_rate': 1, 'max_depth': 3, 'n_estima...               29   \n",
       "19   {'learning_rate': 1, 'max_depth': 2, 'n_estima...               29   \n",
       "..                                                 ...              ...   \n",
       "829  {'learning_rate': 9, 'max_depth': 3, 'n_estima...              970   \n",
       "746  {'learning_rate': 8, 'max_depth': 5, 'n_estima...              970   \n",
       "945  {'learning_rate': 10, 'max_depth': 5, 'n_estim...              973   \n",
       "615  {'learning_rate': 7, 'max_depth': 2, 'n_estima...              974   \n",
       "618  {'learning_rate': 7, 'max_depth': 2, 'n_estima...              974   \n",
       "617  {'learning_rate': 7, 'max_depth': 2, 'n_estima...              974   \n",
       "616  {'learning_rate': 7, 'max_depth': 2, 'n_estima...              974   \n",
       "619  {'learning_rate': 7, 'max_depth': 2, 'n_estima...              974   \n",
       "614  {'learning_rate': 7, 'max_depth': 2, 'n_estima...              974   \n",
       "613  {'learning_rate': 7, 'max_depth': 2, 'n_estima...              974   \n",
       "417  {'learning_rate': 5, 'max_depth': 2, 'n_estima...              981   \n",
       "416  {'learning_rate': 5, 'max_depth': 2, 'n_estima...              981   \n",
       "414  {'learning_rate': 5, 'max_depth': 2, 'n_estima...              981   \n",
       "415  {'learning_rate': 5, 'max_depth': 2, 'n_estima...              981   \n",
       "418  {'learning_rate': 5, 'max_depth': 2, 'n_estima...              981   \n",
       "419  {'learning_rate': 5, 'max_depth': 2, 'n_estima...              981   \n",
       "413  {'learning_rate': 5, 'max_depth': 2, 'n_estima...              987   \n",
       "407  {'learning_rate': 5, 'max_depth': 1, 'n_estima...              988   \n",
       "403  {'learning_rate': 5, 'max_depth': 1, 'n_estima...              988   \n",
       "404  {'learning_rate': 5, 'max_depth': 1, 'n_estima...              988   \n",
       "405  {'learning_rate': 5, 'max_depth': 1, 'n_estima...              988   \n",
       "409  {'learning_rate': 5, 'max_depth': 1, 'n_estima...              988   \n",
       "408  {'learning_rate': 5, 'max_depth': 1, 'n_estima...              988   \n",
       "406  {'learning_rate': 5, 'max_depth': 1, 'n_estima...              988   \n",
       "206  {'learning_rate': 3, 'max_depth': 1, 'n_estima...              995   \n",
       "203  {'learning_rate': 3, 'max_depth': 1, 'n_estima...              995   \n",
       "205  {'learning_rate': 3, 'max_depth': 1, 'n_estima...              995   \n",
       "207  {'learning_rate': 3, 'max_depth': 1, 'n_estima...              995   \n",
       "209  {'learning_rate': 3, 'max_depth': 1, 'n_estima...              995   \n",
       "208  {'learning_rate': 3, 'max_depth': 1, 'n_estima...              995   \n",
       "\n",
       "     split0_test_score  split0_train_score  split1_test_score  \\\n",
       "1             0.884615            0.884393           0.884615   \n",
       "3             0.892308            0.892100           0.876923   \n",
       "2             0.892308            0.886320           0.876923   \n",
       "4             0.884615            0.892100           0.876923   \n",
       "101           0.884615            0.884393           0.884615   \n",
       "11            0.900000            0.892100           0.876923   \n",
       "5             0.884615            0.899807           0.884615   \n",
       "7             0.876923            0.903661           0.900000   \n",
       "8             0.884615            0.901734           0.900000   \n",
       "6             0.876923            0.899807           0.884615   \n",
       "111           0.892308            0.888247           0.876923   \n",
       "0             0.884615            0.884393           0.838462   \n",
       "100           0.884615            0.884393           0.838462   \n",
       "103           0.884615            0.884393           0.838462   \n",
       "9             0.884615            0.903661           0.892308   \n",
       "12            0.892308            0.897881           0.884615   \n",
       "13            0.900000            0.905588           0.869231   \n",
       "15            0.884615            0.924855           0.876923   \n",
       "14            0.876923            0.909441           0.884615   \n",
       "16            0.853846            0.924855           0.869231   \n",
       "21            0.876923            0.917148           0.884615   \n",
       "22            0.876923            0.922929           0.884615   \n",
       "17            0.853846            0.936416           0.884615   \n",
       "23            0.876923            0.934489           0.884615   \n",
       "34            0.861538            0.971098           0.869231   \n",
       "30            0.861538            0.895954           0.853846   \n",
       "31            0.869231            0.928709           0.869231   \n",
       "27            0.823077            0.963391           0.892308   \n",
       "24            0.830769            0.946050           0.892308   \n",
       "19            0.853846            0.938343           0.869231   \n",
       "..                 ...                 ...                ...   \n",
       "829           0.346154            0.342967           0.130769   \n",
       "746           0.169231            0.183044           0.284615   \n",
       "945           0.161538            0.188825           0.253846   \n",
       "615           0.207692            0.210019           0.161538   \n",
       "618           0.207692            0.210019           0.161538   \n",
       "617           0.207692            0.210019           0.161538   \n",
       "616           0.207692            0.210019           0.161538   \n",
       "619           0.207692            0.210019           0.161538   \n",
       "614           0.207692            0.210019           0.161538   \n",
       "613           0.207692            0.210019           0.161538   \n",
       "417           0.207692            0.210019           0.123077   \n",
       "416           0.207692            0.210019           0.123077   \n",
       "414           0.207692            0.210019           0.123077   \n",
       "415           0.207692            0.210019           0.123077   \n",
       "418           0.207692            0.210019           0.123077   \n",
       "419           0.207692            0.210019           0.123077   \n",
       "413           0.207692            0.210019           0.123077   \n",
       "407           0.115385            0.115607           0.161538   \n",
       "403           0.115385            0.115607           0.161538   \n",
       "404           0.115385            0.115607           0.161538   \n",
       "405           0.115385            0.115607           0.161538   \n",
       "409           0.115385            0.115607           0.161538   \n",
       "408           0.115385            0.115607           0.161538   \n",
       "406           0.115385            0.115607           0.161538   \n",
       "206           0.115385            0.115607           0.161538   \n",
       "203           0.115385            0.115607           0.161538   \n",
       "205           0.115385            0.115607           0.161538   \n",
       "207           0.115385            0.115607           0.161538   \n",
       "209           0.115385            0.115607           0.161538   \n",
       "208           0.115385            0.115607           0.161538   \n",
       "\n",
       "     split1_train_score  split2_test_score  split2_train_score  \\\n",
       "1              0.884393           0.892308            0.892100   \n",
       "3              0.894027           0.876923            0.895954   \n",
       "2              0.895954           0.884615            0.884393   \n",
       "4              0.894027           0.884615            0.892100   \n",
       "101            0.884393           0.884615            0.884393   \n",
       "11             0.899807           0.876923            0.901734   \n",
       "5              0.897881           0.884615            0.897881   \n",
       "7              0.899807           0.853846            0.901734   \n",
       "8              0.899807           0.853846            0.905588   \n",
       "6              0.897881           0.853846            0.903661   \n",
       "111            0.903661           0.853846            0.886320   \n",
       "0              0.872832           0.884615            0.884393   \n",
       "100            0.872832           0.884615            0.884393   \n",
       "103            0.872832           0.884615            0.884393   \n",
       "9              0.903661           0.846154            0.905588   \n",
       "12             0.897881           0.861538            0.911368   \n",
       "13             0.907514           0.853846            0.913295   \n",
       "15             0.911368           0.838462            0.919075   \n",
       "14             0.907514           0.830769            0.919075   \n",
       "16             0.917148           0.838462            0.921002   \n",
       "21             0.913295           0.861538            0.913295   \n",
       "22             0.915222           0.853846            0.913295   \n",
       "17             0.915222           0.838462            0.922929   \n",
       "23             0.922929           0.853846            0.938343   \n",
       "34             0.967245           0.846154            0.959538   \n",
       "30             0.890173           0.830769            0.911368   \n",
       "31             0.940270           0.830769            0.919075   \n",
       "27             0.946050           0.846154            0.961464   \n",
       "24             0.936416           0.830769            0.942197   \n",
       "19             0.930636           0.830769            0.926782   \n",
       "..                  ...                ...                 ...   \n",
       "829            0.102119           0.200000            0.163776   \n",
       "746            0.312139           0.300000            0.350674   \n",
       "945            0.344894           0.338462            0.375723   \n",
       "615            0.127168           0.207692            0.156069   \n",
       "618            0.127168           0.207692            0.156069   \n",
       "617            0.127168           0.207692            0.156069   \n",
       "616            0.127168           0.207692            0.156069   \n",
       "619            0.127168           0.207692            0.156069   \n",
       "614            0.127168           0.207692            0.156069   \n",
       "613            0.127168           0.207692            0.156069   \n",
       "417            0.104046           0.407692            0.344894   \n",
       "416            0.104046           0.407692            0.344894   \n",
       "414            0.104046           0.407692            0.344894   \n",
       "415            0.104046           0.407692            0.344894   \n",
       "418            0.104046           0.407692            0.344894   \n",
       "419            0.104046           0.407692            0.344894   \n",
       "413            0.104046           0.238462            0.167630   \n",
       "407            0.127168           0.615385            0.655106   \n",
       "403            0.127168           0.615385            0.655106   \n",
       "404            0.127168           0.615385            0.655106   \n",
       "405            0.127168           0.615385            0.655106   \n",
       "409            0.127168           0.615385            0.655106   \n",
       "408            0.127168           0.615385            0.655106   \n",
       "406            0.127168           0.615385            0.655106   \n",
       "206            0.127168           0.115385            0.115607   \n",
       "203            0.127168           0.115385            0.115607   \n",
       "205            0.127168           0.115385            0.115607   \n",
       "207            0.127168           0.115385            0.115607   \n",
       "209            0.127168           0.115385            0.115607   \n",
       "208            0.127168           0.115385            0.115607   \n",
       "\n",
       "     split3_test_score  split3_train_score  split4_test_score  \\\n",
       "1             0.884615            0.884393           0.899225   \n",
       "3             0.900000            0.888247           0.891473   \n",
       "2             0.884615            0.884393           0.891473   \n",
       "4             0.900000            0.894027           0.883721   \n",
       "101           0.884615            0.884393           0.883721   \n",
       "11            0.861538            0.894027           0.899225   \n",
       "5             0.869231            0.901734           0.875969   \n",
       "7             0.892308            0.907514           0.875969   \n",
       "8             0.876923            0.909441           0.875969   \n",
       "6             0.884615            0.903661           0.883721   \n",
       "111           0.876923            0.882466           0.883721   \n",
       "0             0.884615            0.884393           0.883721   \n",
       "100           0.884615            0.884393           0.883721   \n",
       "103           0.884615            0.884393           0.883721   \n",
       "9             0.876923            0.913295           0.868217   \n",
       "12            0.846154            0.897881           0.883721   \n",
       "13            0.853846            0.903661           0.875969   \n",
       "15            0.869231            0.913295           0.883721   \n",
       "14            0.861538            0.907514           0.883721   \n",
       "16            0.876923            0.921002           0.868217   \n",
       "21            0.853846            0.899807           0.829457   \n",
       "22            0.846154            0.907514           0.821705   \n",
       "17            0.846154            0.917148           0.852713   \n",
       "23            0.830769            0.924855           0.829457   \n",
       "34            0.838462            0.961464           0.860465   \n",
       "30            0.838462            0.892100           0.883721   \n",
       "31            0.846154            0.921002           0.844961   \n",
       "27            0.846154            0.940270           0.852713   \n",
       "24            0.838462            0.930636           0.860465   \n",
       "19            0.853846            0.926782           0.844961   \n",
       "..                 ...                 ...                ...   \n",
       "829           0.230769            0.208092           0.403101   \n",
       "746           0.276923            0.260116           0.279070   \n",
       "945           0.369231            0.342967           0.162791   \n",
       "615           0.238462            0.169557           0.457364   \n",
       "618           0.238462            0.169557           0.457364   \n",
       "617           0.238462            0.169557           0.457364   \n",
       "616           0.238462            0.169557           0.457364   \n",
       "619           0.238462            0.169557           0.457364   \n",
       "614           0.238462            0.169557           0.457364   \n",
       "613           0.238462            0.169557           0.457364   \n",
       "417           0.238462            0.169557           0.170543   \n",
       "416           0.238462            0.169557           0.170543   \n",
       "414           0.238462            0.169557           0.170543   \n",
       "415           0.238462            0.169557           0.170543   \n",
       "418           0.238462            0.169557           0.170543   \n",
       "419           0.238462            0.169557           0.170543   \n",
       "413           0.392308            0.335260           0.170543   \n",
       "407           0.115385            0.115607           0.116279   \n",
       "403           0.115385            0.115607           0.116279   \n",
       "404           0.115385            0.115607           0.116279   \n",
       "405           0.115385            0.115607           0.116279   \n",
       "409           0.115385            0.115607           0.116279   \n",
       "408           0.115385            0.115607           0.116279   \n",
       "406           0.115385            0.115607           0.116279   \n",
       "206           0.115385            0.115607           0.116279   \n",
       "203           0.115385            0.115607           0.116279   \n",
       "205           0.115385            0.115607           0.116279   \n",
       "207           0.115385            0.115607           0.116279   \n",
       "209           0.115385            0.115607           0.116279   \n",
       "208           0.115385            0.115607           0.116279   \n",
       "\n",
       "     split4_train_score  std_fit_time  std_score_time  std_test_score  \\\n",
       "1              0.890385      0.000128    2.726350e-05        0.005875   \n",
       "3              0.896154      0.001313    1.192671e-04        0.009159   \n",
       "2              0.896154      0.000115    9.174538e-05        0.005585   \n",
       "4              0.896154      0.000490    2.083224e-04        0.007587   \n",
       "101            0.884615      0.000023    4.712251e-06        0.000357   \n",
       "11             0.898077      0.001266    1.082034e-04        0.014740   \n",
       "5              0.901923      0.000552    1.130305e-04        0.006263   \n",
       "7              0.903846      0.000048    7.240031e-05        0.015893   \n",
       "8              0.903846      0.000082    1.108541e-05        0.014952   \n",
       "6              0.903846      0.000012    7.563545e-06        0.011812   \n",
       "111            0.884615      0.000130    6.106495e-07        0.012776   \n",
       "0              0.884615      0.000256    2.436866e-05        0.018386   \n",
       "100            0.884615      0.000014    4.128430e-06        0.018386   \n",
       "103            0.884615      0.000096    1.059459e-05        0.018386   \n",
       "9              0.901923      0.000156    1.628244e-05        0.015914   \n",
       "12             0.905769      0.000374    3.545745e-05        0.017170   \n",
       "13             0.901923      0.000100    3.197711e-05        0.017079   \n",
       "15             0.913462      0.000573    7.712051e-05        0.016995   \n",
       "14             0.907692      0.000704    5.429008e-05        0.020153   \n",
       "16             0.915385      0.000150    2.422239e-05        0.013660   \n",
       "21             0.900000      0.000417    1.428125e-04        0.019247   \n",
       "22             0.928846      0.000369    1.153071e-04        0.022480   \n",
       "17             0.926923      0.000775    1.699214e-05        0.015730   \n",
       "23             0.936538      0.000295    3.678866e-05        0.022792   \n",
       "34             0.965385      0.000372    1.956637e-05        0.011203   \n",
       "30             0.917308      0.000479    2.061051e-05        0.018528   \n",
       "31             0.930769      0.000315    2.656629e-05        0.015031   \n",
       "27             0.961538      0.001518    2.768967e-05        0.022512   \n",
       "24             0.942308      0.000743    4.063380e-05        0.023555   \n",
       "19             0.928846      0.000231    2.344536e-05        0.012603   \n",
       "..                  ...           ...             ...             ...   \n",
       "829            0.400000      0.002139    2.482138e-05        0.098949   \n",
       "746            0.280769      0.001416    7.329035e-06        0.047097   \n",
       "945            0.151923      0.001392    3.638239e-05        0.086277   \n",
       "615            0.459615      0.000268    5.866454e-06        0.104117   \n",
       "618            0.459615      0.000514    4.084761e-05        0.104117   \n",
       "617            0.459615      0.000384    5.840039e-06        0.104117   \n",
       "616            0.459615      0.000374    5.914849e-05        0.104117   \n",
       "619            0.459615      0.000344    1.810136e-05        0.104117   \n",
       "614            0.459615      0.000288    2.362149e-05        0.104117   \n",
       "613            0.459615      0.000291    1.223498e-04        0.104117   \n",
       "417            0.163462      0.000425    9.755487e-06        0.097114   \n",
       "416            0.163462      0.000878    1.652500e-05        0.097114   \n",
       "414            0.163462      0.000210    1.846782e-06        0.097114   \n",
       "415            0.163462      0.000306    2.197852e-05        0.097114   \n",
       "418            0.163462      0.000735    1.774667e-05        0.097114   \n",
       "419            0.163462      0.000288    3.751815e-05        0.097114   \n",
       "413            0.163462      0.000142    9.658277e-06        0.091495   \n",
       "407            0.115385      0.001149    5.023284e-05        0.196206   \n",
       "403            0.115385      0.000037    1.435272e-06        0.196206   \n",
       "404            0.115385      0.000085    2.610630e-05        0.196206   \n",
       "405            0.115385      0.000506    4.432072e-05        0.196206   \n",
       "409            0.115385      0.000556    1.447084e-04        0.196206   \n",
       "408            0.115385      0.000778    1.534926e-04        0.196206   \n",
       "406            0.115385      0.000036    1.081065e-06        0.196206   \n",
       "206            0.115385      0.000197    2.905437e-05        0.018386   \n",
       "203            0.115385      0.000201    7.573408e-05        0.018386   \n",
       "205            0.115385      0.000388    7.798701e-05        0.018386   \n",
       "207            0.115385      0.000243    9.925496e-05        0.018386   \n",
       "209            0.115385      0.000612    1.101124e-04        0.018386   \n",
       "208            0.115385      0.000464    1.330406e-04        0.018386   \n",
       "\n",
       "     std_train_score  \n",
       "1           0.003399  \n",
       "3           0.002922  \n",
       "2           0.005444  \n",
       "4           0.001507  \n",
       "101         0.000089  \n",
       "11          0.003583  \n",
       "5           0.001767  \n",
       "7           0.002563  \n",
       "8           0.003311  \n",
       "6           0.002467  \n",
       "111         0.007544  \n",
       "0           0.004647  \n",
       "100         0.004647  \n",
       "103         0.004647  \n",
       "9           0.004006  \n",
       "12          0.005527  \n",
       "13          0.003924  \n",
       "15          0.004945  \n",
       "14          0.004473  \n",
       "16          0.003315  \n",
       "21          0.007326  \n",
       "22          0.007494  \n",
       "17          0.007583  \n",
       "23          0.006305  \n",
       "34          0.004116  \n",
       "30          0.010905  \n",
       "31          0.007581  \n",
       "27          0.009497  \n",
       "24          0.005406  \n",
       "19          0.004282  \n",
       "..               ...  \n",
       "829         0.111341  \n",
       "746         0.056194  \n",
       "945         0.091708  \n",
       "615         0.120552  \n",
       "618         0.120552  \n",
       "617         0.120552  \n",
       "616         0.120552  \n",
       "619         0.120552  \n",
       "614         0.120552  \n",
       "613         0.120552  \n",
       "417         0.080686  \n",
       "416         0.080686  \n",
       "414         0.080686  \n",
       "415         0.080686  \n",
       "418         0.080686  \n",
       "419         0.080686  \n",
       "413         0.077342  \n",
       "407         0.214713  \n",
       "403         0.214713  \n",
       "404         0.214713  \n",
       "405         0.214713  \n",
       "409         0.214713  \n",
       "408         0.214713  \n",
       "406         0.214713  \n",
       "206         0.004647  \n",
       "203         0.004647  \n",
       "205         0.004647  \n",
       "207         0.004647  \n",
       "209         0.004647  \n",
       "208         0.004647  \n",
       "\n",
       "[1000 rows x 23 columns]"
      ]
     },
     "execution_count": 954,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GBC_grid = pd.DataFrame(GBC_CV.cv_results_)\n",
    "GBC_grid.sort_values('mean_test_score', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Найдем лучшую модель и веса признаков:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 955,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 1, 'max_depth': 1, 'n_estimators': 2}"
      ]
     },
     "execution_count": 955,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GBC_CV.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 956,
   "metadata": {},
   "outputs": [],
   "source": [
    "GBC_best_model = GradientBoostingClassifier(learning_rate = 1, max_depth = 1, n_estimators = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 957,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>columns</th>\n",
       "      <th>weights</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>absences</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>sex_M</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>age</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>famsup_no</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Fjob_services</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          columns   weights\n",
       "12       absences  0.666667\n",
       "16          sex_M  0.333333\n",
       "0             age  0.000000\n",
       "42      famsup_no  0.000000\n",
       "31  Fjob_services  0.000000\n",
       "\n",
       "[5 rows x 2 columns]"
      ]
     },
     "execution_count": 957,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame({'columns': X.columns, \n",
    "              'weights': GBC_best_model.fit(X, y).feature_importances_}).sort_values('weights', \n",
    "                                                                                 ascending = False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Таблицы важности признаков отличаются, потому что наилучшими моделями random forest и gradient boosting оказались модели с разными параметрами."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Значения метрики для 5 лучших моделей:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 958,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.88906009245\n",
      "0.887519260401\n",
      "0.885978428351\n",
      "0.885978428351\n",
      "0.884437596302\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(GBC_grid.sort_values('mean_test_score', ascending = False).iloc[i].mean_test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 959,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "say -v Daniel The programm execution has been finished"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 960,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 961,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "AdaBoost = AdaBoostClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 962,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "AdaBoost = AdaBoostClassifier()\n",
    "params = {'n_estimators': np.arange(1,11),\n",
    "          'learning_rate': np.arange(1,11)} \n",
    "splits = KFold(n_splits=5, shuffle=True, random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 963,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "AdaBoost_CV = GridSearchCV(estimator=AdaBoost, param_grid=params, \n",
    "                             scoring='accuracy', cv=splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 964,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=KFold(n_splits=5, random_state=64, shuffle=True),\n",
       "       error_score='raise',\n",
       "       estimator=AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
       "          learning_rate=1.0, n_estimators=50, random_state=None),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid={'n_estimators': array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10]), 'learning_rate': array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10])},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring='accuracy', verbose=0)"
      ]
     },
     "execution_count": 964,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AdaBoost_CV.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 965,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>param_learning_rate</th>\n",
       "      <th>param_n_estimators</th>\n",
       "      <th>params</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split3_train_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>split4_train_score</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.007529</td>\n",
       "      <td>0.000766</td>\n",
       "      <td>0.895223</td>\n",
       "      <td>0.887903</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>{'learning_rate': 1, 'n_estimators': 5}</td>\n",
       "      <td>1</td>\n",
       "      <td>0.915385</td>\n",
       "      <td>0.874759</td>\n",
       "      <td>0.915385</td>\n",
       "      <td>0.888247</td>\n",
       "      <td>0.876923</td>\n",
       "      <td>0.897881</td>\n",
       "      <td>0.876923</td>\n",
       "      <td>0.886320</td>\n",
       "      <td>0.891473</td>\n",
       "      <td>0.892308</td>\n",
       "      <td>0.000436</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>0.017315</td>\n",
       "      <td>0.007673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.009128</td>\n",
       "      <td>0.000794</td>\n",
       "      <td>0.890601</td>\n",
       "      <td>0.891371</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>{'learning_rate': 1, 'n_estimators': 6}</td>\n",
       "      <td>2</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.878613</td>\n",
       "      <td>0.907692</td>\n",
       "      <td>0.892100</td>\n",
       "      <td>0.861538</td>\n",
       "      <td>0.895954</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.897881</td>\n",
       "      <td>0.883721</td>\n",
       "      <td>0.892308</td>\n",
       "      <td>0.000436</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.016508</td>\n",
       "      <td>0.006746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.005935</td>\n",
       "      <td>0.000624</td>\n",
       "      <td>0.890601</td>\n",
       "      <td>0.888676</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>{'learning_rate': 1, 'n_estimators': 4}</td>\n",
       "      <td>2</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.884393</td>\n",
       "      <td>0.915385</td>\n",
       "      <td>0.880539</td>\n",
       "      <td>0.861538</td>\n",
       "      <td>0.899807</td>\n",
       "      <td>0.892308</td>\n",
       "      <td>0.892100</td>\n",
       "      <td>0.883721</td>\n",
       "      <td>0.886538</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.017886</td>\n",
       "      <td>0.006704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.004816</td>\n",
       "      <td>0.000564</td>\n",
       "      <td>0.889060</td>\n",
       "      <td>0.887519</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>{'learning_rate': 1, 'n_estimators': 3}</td>\n",
       "      <td>4</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.884393</td>\n",
       "      <td>0.915385</td>\n",
       "      <td>0.880539</td>\n",
       "      <td>0.861538</td>\n",
       "      <td>0.899807</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.884393</td>\n",
       "      <td>0.883721</td>\n",
       "      <td>0.888462</td>\n",
       "      <td>0.000277</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.018004</td>\n",
       "      <td>0.006635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.011161</td>\n",
       "      <td>0.001033</td>\n",
       "      <td>0.887519</td>\n",
       "      <td>0.890216</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>{'learning_rate': 1, 'n_estimators': 7}</td>\n",
       "      <td>5</td>\n",
       "      <td>0.907692</td>\n",
       "      <td>0.884393</td>\n",
       "      <td>0.907692</td>\n",
       "      <td>0.884393</td>\n",
       "      <td>0.853846</td>\n",
       "      <td>0.894027</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.899807</td>\n",
       "      <td>0.883721</td>\n",
       "      <td>0.888462</td>\n",
       "      <td>0.000555</td>\n",
       "      <td>0.000355</td>\n",
       "      <td>0.019867</td>\n",
       "      <td>0.005957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.003412</td>\n",
       "      <td>0.000503</td>\n",
       "      <td>0.887519</td>\n",
       "      <td>0.887522</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>{'learning_rate': 1, 'n_estimators': 2}</td>\n",
       "      <td>5</td>\n",
       "      <td>0.869231</td>\n",
       "      <td>0.888247</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.884393</td>\n",
       "      <td>0.861538</td>\n",
       "      <td>0.899807</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.884393</td>\n",
       "      <td>0.899225</td>\n",
       "      <td>0.880769</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>0.022009</td>\n",
       "      <td>0.006582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.012359</td>\n",
       "      <td>0.001355</td>\n",
       "      <td>0.885978</td>\n",
       "      <td>0.890216</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>{'learning_rate': 1, 'n_estimators': 8}</td>\n",
       "      <td>7</td>\n",
       "      <td>0.892308</td>\n",
       "      <td>0.880539</td>\n",
       "      <td>0.915385</td>\n",
       "      <td>0.892100</td>\n",
       "      <td>0.853846</td>\n",
       "      <td>0.892100</td>\n",
       "      <td>0.892308</td>\n",
       "      <td>0.895954</td>\n",
       "      <td>0.875969</td>\n",
       "      <td>0.890385</td>\n",
       "      <td>0.000355</td>\n",
       "      <td>0.000378</td>\n",
       "      <td>0.020396</td>\n",
       "      <td>0.005171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>0.002307</td>\n",
       "      <td>0.000436</td>\n",
       "      <td>0.884438</td>\n",
       "      <td>0.884439</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>{'learning_rate': 6, 'n_estimators': 1}</td>\n",
       "      <td>8</td>\n",
       "      <td>0.869231</td>\n",
       "      <td>0.888247</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.880539</td>\n",
       "      <td>0.869231</td>\n",
       "      <td>0.888247</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.884393</td>\n",
       "      <td>0.899225</td>\n",
       "      <td>0.880769</td>\n",
       "      <td>0.000316</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>0.013588</td>\n",
       "      <td>0.003396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>0.008823</td>\n",
       "      <td>0.001001</td>\n",
       "      <td>0.884438</td>\n",
       "      <td>0.884439</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>{'learning_rate': 9, 'n_estimators': 7}</td>\n",
       "      <td>8</td>\n",
       "      <td>0.869231</td>\n",
       "      <td>0.888247</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.880539</td>\n",
       "      <td>0.869231</td>\n",
       "      <td>0.888247</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.884393</td>\n",
       "      <td>0.899225</td>\n",
       "      <td>0.880769</td>\n",
       "      <td>0.000548</td>\n",
       "      <td>0.000225</td>\n",
       "      <td>0.013588</td>\n",
       "      <td>0.003396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>0.016932</td>\n",
       "      <td>0.001476</td>\n",
       "      <td>0.884438</td>\n",
       "      <td>0.884439</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>{'learning_rate': 9, 'n_estimators': 9}</td>\n",
       "      <td>8</td>\n",
       "      <td>0.869231</td>\n",
       "      <td>0.888247</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.880539</td>\n",
       "      <td>0.869231</td>\n",
       "      <td>0.888247</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.884393</td>\n",
       "      <td>0.899225</td>\n",
       "      <td>0.880769</td>\n",
       "      <td>0.000577</td>\n",
       "      <td>0.000291</td>\n",
       "      <td>0.013588</td>\n",
       "      <td>0.003396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>0.002898</td>\n",
       "      <td>0.000645</td>\n",
       "      <td>0.884438</td>\n",
       "      <td>0.884439</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>{'learning_rate': 10, 'n_estimators': 1}</td>\n",
       "      <td>8</td>\n",
       "      <td>0.869231</td>\n",
       "      <td>0.888247</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.880539</td>\n",
       "      <td>0.869231</td>\n",
       "      <td>0.888247</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.884393</td>\n",
       "      <td>0.899225</td>\n",
       "      <td>0.880769</td>\n",
       "      <td>0.000172</td>\n",
       "      <td>0.000184</td>\n",
       "      <td>0.013588</td>\n",
       "      <td>0.003396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>0.005095</td>\n",
       "      <td>0.000643</td>\n",
       "      <td>0.884438</td>\n",
       "      <td>0.884439</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>{'learning_rate': 10, 'n_estimators': 3}</td>\n",
       "      <td>8</td>\n",
       "      <td>0.869231</td>\n",
       "      <td>0.888247</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.880539</td>\n",
       "      <td>0.869231</td>\n",
       "      <td>0.888247</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.884393</td>\n",
       "      <td>0.899225</td>\n",
       "      <td>0.880769</td>\n",
       "      <td>0.000741</td>\n",
       "      <td>0.000063</td>\n",
       "      <td>0.013588</td>\n",
       "      <td>0.003396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>0.007334</td>\n",
       "      <td>0.000924</td>\n",
       "      <td>0.884438</td>\n",
       "      <td>0.884439</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>{'learning_rate': 10, 'n_estimators': 5}</td>\n",
       "      <td>8</td>\n",
       "      <td>0.869231</td>\n",
       "      <td>0.888247</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.880539</td>\n",
       "      <td>0.869231</td>\n",
       "      <td>0.888247</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.884393</td>\n",
       "      <td>0.899225</td>\n",
       "      <td>0.880769</td>\n",
       "      <td>0.000706</td>\n",
       "      <td>0.000244</td>\n",
       "      <td>0.013588</td>\n",
       "      <td>0.003396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.008459</td>\n",
       "      <td>0.000879</td>\n",
       "      <td>0.884438</td>\n",
       "      <td>0.884439</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>{'learning_rate': 10, 'n_estimators': 7}</td>\n",
       "      <td>8</td>\n",
       "      <td>0.869231</td>\n",
       "      <td>0.888247</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.880539</td>\n",
       "      <td>0.869231</td>\n",
       "      <td>0.888247</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.884393</td>\n",
       "      <td>0.899225</td>\n",
       "      <td>0.880769</td>\n",
       "      <td>0.000288</td>\n",
       "      <td>0.000108</td>\n",
       "      <td>0.013588</td>\n",
       "      <td>0.003396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0.010388</td>\n",
       "      <td>0.001007</td>\n",
       "      <td>0.884438</td>\n",
       "      <td>0.884439</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>{'learning_rate': 10, 'n_estimators': 9}</td>\n",
       "      <td>8</td>\n",
       "      <td>0.869231</td>\n",
       "      <td>0.888247</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.880539</td>\n",
       "      <td>0.869231</td>\n",
       "      <td>0.888247</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.884393</td>\n",
       "      <td>0.899225</td>\n",
       "      <td>0.880769</td>\n",
       "      <td>0.000162</td>\n",
       "      <td>0.000079</td>\n",
       "      <td>0.013588</td>\n",
       "      <td>0.003396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.002943</td>\n",
       "      <td>0.000537</td>\n",
       "      <td>0.884438</td>\n",
       "      <td>0.884439</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>{'learning_rate': 5, 'n_estimators': 1}</td>\n",
       "      <td>8</td>\n",
       "      <td>0.869231</td>\n",
       "      <td>0.888247</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.880539</td>\n",
       "      <td>0.869231</td>\n",
       "      <td>0.888247</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.884393</td>\n",
       "      <td>0.899225</td>\n",
       "      <td>0.880769</td>\n",
       "      <td>0.000568</td>\n",
       "      <td>0.000185</td>\n",
       "      <td>0.013588</td>\n",
       "      <td>0.003396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.002230</td>\n",
       "      <td>0.000401</td>\n",
       "      <td>0.884438</td>\n",
       "      <td>0.884439</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>{'learning_rate': 4, 'n_estimators': 1}</td>\n",
       "      <td>8</td>\n",
       "      <td>0.869231</td>\n",
       "      <td>0.888247</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.880539</td>\n",
       "      <td>0.869231</td>\n",
       "      <td>0.888247</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.884393</td>\n",
       "      <td>0.899225</td>\n",
       "      <td>0.880769</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.013588</td>\n",
       "      <td>0.003396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.011357</td>\n",
       "      <td>0.001060</td>\n",
       "      <td>0.884438</td>\n",
       "      <td>0.884824</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>{'learning_rate': 3, 'n_estimators': 9}</td>\n",
       "      <td>8</td>\n",
       "      <td>0.869231</td>\n",
       "      <td>0.888247</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.880539</td>\n",
       "      <td>0.869231</td>\n",
       "      <td>0.888247</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.884393</td>\n",
       "      <td>0.899225</td>\n",
       "      <td>0.882692</td>\n",
       "      <td>0.001014</td>\n",
       "      <td>0.000119</td>\n",
       "      <td>0.013588</td>\n",
       "      <td>0.003050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.009338</td>\n",
       "      <td>0.001042</td>\n",
       "      <td>0.884438</td>\n",
       "      <td>0.884824</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>{'learning_rate': 3, 'n_estimators': 7}</td>\n",
       "      <td>8</td>\n",
       "      <td>0.869231</td>\n",
       "      <td>0.888247</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.880539</td>\n",
       "      <td>0.869231</td>\n",
       "      <td>0.888247</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.884393</td>\n",
       "      <td>0.899225</td>\n",
       "      <td>0.882692</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.000377</td>\n",
       "      <td>0.013588</td>\n",
       "      <td>0.003050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>0.004304</td>\n",
       "      <td>0.000541</td>\n",
       "      <td>0.884438</td>\n",
       "      <td>0.884439</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>{'learning_rate': 9, 'n_estimators': 3}</td>\n",
       "      <td>8</td>\n",
       "      <td>0.869231</td>\n",
       "      <td>0.888247</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.880539</td>\n",
       "      <td>0.869231</td>\n",
       "      <td>0.888247</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.884393</td>\n",
       "      <td>0.899225</td>\n",
       "      <td>0.880769</td>\n",
       "      <td>0.000171</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.013588</td>\n",
       "      <td>0.003396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.010177</td>\n",
       "      <td>0.000922</td>\n",
       "      <td>0.884438</td>\n",
       "      <td>0.884439</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>{'learning_rate': 4, 'n_estimators': 7}</td>\n",
       "      <td>8</td>\n",
       "      <td>0.869231</td>\n",
       "      <td>0.888247</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.880539</td>\n",
       "      <td>0.869231</td>\n",
       "      <td>0.888247</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.884393</td>\n",
       "      <td>0.899225</td>\n",
       "      <td>0.880769</td>\n",
       "      <td>0.001129</td>\n",
       "      <td>0.000090</td>\n",
       "      <td>0.013588</td>\n",
       "      <td>0.003396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.002691</td>\n",
       "      <td>0.000429</td>\n",
       "      <td>0.884438</td>\n",
       "      <td>0.884439</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>{'learning_rate': 3, 'n_estimators': 1}</td>\n",
       "      <td>8</td>\n",
       "      <td>0.869231</td>\n",
       "      <td>0.888247</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.880539</td>\n",
       "      <td>0.869231</td>\n",
       "      <td>0.888247</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.884393</td>\n",
       "      <td>0.899225</td>\n",
       "      <td>0.880769</td>\n",
       "      <td>0.000528</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>0.013588</td>\n",
       "      <td>0.003396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.002635</td>\n",
       "      <td>0.000653</td>\n",
       "      <td>0.884438</td>\n",
       "      <td>0.884439</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>{'learning_rate': 2, 'n_estimators': 1}</td>\n",
       "      <td>8</td>\n",
       "      <td>0.869231</td>\n",
       "      <td>0.888247</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.880539</td>\n",
       "      <td>0.869231</td>\n",
       "      <td>0.888247</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.884393</td>\n",
       "      <td>0.899225</td>\n",
       "      <td>0.880769</td>\n",
       "      <td>0.000223</td>\n",
       "      <td>0.000202</td>\n",
       "      <td>0.013588</td>\n",
       "      <td>0.003396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.009299</td>\n",
       "      <td>0.000869</td>\n",
       "      <td>0.884438</td>\n",
       "      <td>0.884824</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>{'learning_rate': 3, 'n_estimators': 5}</td>\n",
       "      <td>8</td>\n",
       "      <td>0.869231</td>\n",
       "      <td>0.888247</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.880539</td>\n",
       "      <td>0.869231</td>\n",
       "      <td>0.888247</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.884393</td>\n",
       "      <td>0.899225</td>\n",
       "      <td>0.882692</td>\n",
       "      <td>0.002641</td>\n",
       "      <td>0.000123</td>\n",
       "      <td>0.013588</td>\n",
       "      <td>0.003050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.013071</td>\n",
       "      <td>0.001145</td>\n",
       "      <td>0.884438</td>\n",
       "      <td>0.884439</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>{'learning_rate': 4, 'n_estimators': 5}</td>\n",
       "      <td>8</td>\n",
       "      <td>0.869231</td>\n",
       "      <td>0.888247</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.880539</td>\n",
       "      <td>0.869231</td>\n",
       "      <td>0.888247</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.884393</td>\n",
       "      <td>0.899225</td>\n",
       "      <td>0.880769</td>\n",
       "      <td>0.004349</td>\n",
       "      <td>0.000139</td>\n",
       "      <td>0.013588</td>\n",
       "      <td>0.003396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.008169</td>\n",
       "      <td>0.000975</td>\n",
       "      <td>0.884438</td>\n",
       "      <td>0.884439</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>{'learning_rate': 4, 'n_estimators': 3}</td>\n",
       "      <td>8</td>\n",
       "      <td>0.869231</td>\n",
       "      <td>0.888247</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.880539</td>\n",
       "      <td>0.869231</td>\n",
       "      <td>0.888247</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.884393</td>\n",
       "      <td>0.899225</td>\n",
       "      <td>0.880769</td>\n",
       "      <td>0.001529</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.013588</td>\n",
       "      <td>0.003396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>0.006764</td>\n",
       "      <td>0.000698</td>\n",
       "      <td>0.884438</td>\n",
       "      <td>0.884439</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>{'learning_rate': 9, 'n_estimators': 5}</td>\n",
       "      <td>8</td>\n",
       "      <td>0.869231</td>\n",
       "      <td>0.888247</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.880539</td>\n",
       "      <td>0.869231</td>\n",
       "      <td>0.888247</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.884393</td>\n",
       "      <td>0.899225</td>\n",
       "      <td>0.880769</td>\n",
       "      <td>0.001376</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.013588</td>\n",
       "      <td>0.003396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.011058</td>\n",
       "      <td>0.001018</td>\n",
       "      <td>0.884438</td>\n",
       "      <td>0.884439</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>{'learning_rate': 4, 'n_estimators': 9}</td>\n",
       "      <td>8</td>\n",
       "      <td>0.869231</td>\n",
       "      <td>0.888247</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.880539</td>\n",
       "      <td>0.869231</td>\n",
       "      <td>0.888247</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.884393</td>\n",
       "      <td>0.899225</td>\n",
       "      <td>0.880769</td>\n",
       "      <td>0.000572</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.013588</td>\n",
       "      <td>0.003396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>0.002662</td>\n",
       "      <td>0.000590</td>\n",
       "      <td>0.884438</td>\n",
       "      <td>0.884439</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>{'learning_rate': 9, 'n_estimators': 1}</td>\n",
       "      <td>8</td>\n",
       "      <td>0.869231</td>\n",
       "      <td>0.888247</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.880539</td>\n",
       "      <td>0.869231</td>\n",
       "      <td>0.888247</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.884393</td>\n",
       "      <td>0.899225</td>\n",
       "      <td>0.880769</td>\n",
       "      <td>0.000344</td>\n",
       "      <td>0.000150</td>\n",
       "      <td>0.013588</td>\n",
       "      <td>0.003396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>0.011039</td>\n",
       "      <td>0.001049</td>\n",
       "      <td>0.884438</td>\n",
       "      <td>0.884439</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>{'learning_rate': 8, 'n_estimators': 9}</td>\n",
       "      <td>8</td>\n",
       "      <td>0.869231</td>\n",
       "      <td>0.888247</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.880539</td>\n",
       "      <td>0.869231</td>\n",
       "      <td>0.888247</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.884393</td>\n",
       "      <td>0.899225</td>\n",
       "      <td>0.880769</td>\n",
       "      <td>0.000321</td>\n",
       "      <td>0.000079</td>\n",
       "      <td>0.013588</td>\n",
       "      <td>0.003396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.008913</td>\n",
       "      <td>0.000838</td>\n",
       "      <td>0.147920</td>\n",
       "      <td>0.154839</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>{'learning_rate': 3, 'n_estimators': 6}</td>\n",
       "      <td>62</td>\n",
       "      <td>0.176923</td>\n",
       "      <td>0.184971</td>\n",
       "      <td>0.107692</td>\n",
       "      <td>0.129094</td>\n",
       "      <td>0.130769</td>\n",
       "      <td>0.123314</td>\n",
       "      <td>0.130769</td>\n",
       "      <td>0.144509</td>\n",
       "      <td>0.193798</td>\n",
       "      <td>0.192308</td>\n",
       "      <td>0.000707</td>\n",
       "      <td>0.000068</td>\n",
       "      <td>0.032076</td>\n",
       "      <td>0.028549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>0.011324</td>\n",
       "      <td>0.000987</td>\n",
       "      <td>0.147920</td>\n",
       "      <td>0.152531</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>{'learning_rate': 6, 'n_estimators': 8}</td>\n",
       "      <td>62</td>\n",
       "      <td>0.176923</td>\n",
       "      <td>0.184971</td>\n",
       "      <td>0.107692</td>\n",
       "      <td>0.129094</td>\n",
       "      <td>0.130769</td>\n",
       "      <td>0.123314</td>\n",
       "      <td>0.130769</td>\n",
       "      <td>0.144509</td>\n",
       "      <td>0.193798</td>\n",
       "      <td>0.180769</td>\n",
       "      <td>0.002163</td>\n",
       "      <td>0.000076</td>\n",
       "      <td>0.032076</td>\n",
       "      <td>0.025757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>0.011713</td>\n",
       "      <td>0.001290</td>\n",
       "      <td>0.147920</td>\n",
       "      <td>0.152531</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>{'learning_rate': 6, 'n_estimators': 10}</td>\n",
       "      <td>62</td>\n",
       "      <td>0.176923</td>\n",
       "      <td>0.184971</td>\n",
       "      <td>0.107692</td>\n",
       "      <td>0.129094</td>\n",
       "      <td>0.130769</td>\n",
       "      <td>0.123314</td>\n",
       "      <td>0.130769</td>\n",
       "      <td>0.144509</td>\n",
       "      <td>0.193798</td>\n",
       "      <td>0.180769</td>\n",
       "      <td>0.000605</td>\n",
       "      <td>0.000526</td>\n",
       "      <td>0.032076</td>\n",
       "      <td>0.025757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>0.004498</td>\n",
       "      <td>0.000676</td>\n",
       "      <td>0.147920</td>\n",
       "      <td>0.152531</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>{'learning_rate': 7, 'n_estimators': 2}</td>\n",
       "      <td>62</td>\n",
       "      <td>0.176923</td>\n",
       "      <td>0.184971</td>\n",
       "      <td>0.107692</td>\n",
       "      <td>0.129094</td>\n",
       "      <td>0.130769</td>\n",
       "      <td>0.123314</td>\n",
       "      <td>0.130769</td>\n",
       "      <td>0.144509</td>\n",
       "      <td>0.193798</td>\n",
       "      <td>0.180769</td>\n",
       "      <td>0.000682</td>\n",
       "      <td>0.000163</td>\n",
       "      <td>0.032076</td>\n",
       "      <td>0.025757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.008545</td>\n",
       "      <td>0.000924</td>\n",
       "      <td>0.147920</td>\n",
       "      <td>0.152531</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>{'learning_rate': 4, 'n_estimators': 4}</td>\n",
       "      <td>62</td>\n",
       "      <td>0.176923</td>\n",
       "      <td>0.184971</td>\n",
       "      <td>0.107692</td>\n",
       "      <td>0.129094</td>\n",
       "      <td>0.130769</td>\n",
       "      <td>0.123314</td>\n",
       "      <td>0.130769</td>\n",
       "      <td>0.144509</td>\n",
       "      <td>0.193798</td>\n",
       "      <td>0.180769</td>\n",
       "      <td>0.001474</td>\n",
       "      <td>0.000278</td>\n",
       "      <td>0.032076</td>\n",
       "      <td>0.025757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.011173</td>\n",
       "      <td>0.001109</td>\n",
       "      <td>0.147920</td>\n",
       "      <td>0.152531</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>{'learning_rate': 5, 'n_estimators': 8}</td>\n",
       "      <td>62</td>\n",
       "      <td>0.176923</td>\n",
       "      <td>0.184971</td>\n",
       "      <td>0.107692</td>\n",
       "      <td>0.129094</td>\n",
       "      <td>0.130769</td>\n",
       "      <td>0.123314</td>\n",
       "      <td>0.130769</td>\n",
       "      <td>0.144509</td>\n",
       "      <td>0.193798</td>\n",
       "      <td>0.180769</td>\n",
       "      <td>0.000549</td>\n",
       "      <td>0.000358</td>\n",
       "      <td>0.032076</td>\n",
       "      <td>0.025757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.008852</td>\n",
       "      <td>0.001105</td>\n",
       "      <td>0.147920</td>\n",
       "      <td>0.152531</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>{'learning_rate': 5, 'n_estimators': 6}</td>\n",
       "      <td>62</td>\n",
       "      <td>0.176923</td>\n",
       "      <td>0.184971</td>\n",
       "      <td>0.107692</td>\n",
       "      <td>0.129094</td>\n",
       "      <td>0.130769</td>\n",
       "      <td>0.123314</td>\n",
       "      <td>0.130769</td>\n",
       "      <td>0.144509</td>\n",
       "      <td>0.193798</td>\n",
       "      <td>0.180769</td>\n",
       "      <td>0.000392</td>\n",
       "      <td>0.000285</td>\n",
       "      <td>0.032076</td>\n",
       "      <td>0.025757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>0.004746</td>\n",
       "      <td>0.000753</td>\n",
       "      <td>0.147920</td>\n",
       "      <td>0.152531</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>{'learning_rate': 8, 'n_estimators': 2}</td>\n",
       "      <td>62</td>\n",
       "      <td>0.176923</td>\n",
       "      <td>0.184971</td>\n",
       "      <td>0.107692</td>\n",
       "      <td>0.129094</td>\n",
       "      <td>0.130769</td>\n",
       "      <td>0.123314</td>\n",
       "      <td>0.130769</td>\n",
       "      <td>0.144509</td>\n",
       "      <td>0.193798</td>\n",
       "      <td>0.180769</td>\n",
       "      <td>0.000622</td>\n",
       "      <td>0.000175</td>\n",
       "      <td>0.032076</td>\n",
       "      <td>0.025757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>0.010901</td>\n",
       "      <td>0.001019</td>\n",
       "      <td>0.147920</td>\n",
       "      <td>0.152531</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>{'learning_rate': 8, 'n_estimators': 8}</td>\n",
       "      <td>62</td>\n",
       "      <td>0.176923</td>\n",
       "      <td>0.184971</td>\n",
       "      <td>0.107692</td>\n",
       "      <td>0.129094</td>\n",
       "      <td>0.130769</td>\n",
       "      <td>0.123314</td>\n",
       "      <td>0.130769</td>\n",
       "      <td>0.144509</td>\n",
       "      <td>0.193798</td>\n",
       "      <td>0.180769</td>\n",
       "      <td>0.000834</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.032076</td>\n",
       "      <td>0.025757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.009475</td>\n",
       "      <td>0.001044</td>\n",
       "      <td>0.147920</td>\n",
       "      <td>0.152531</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>{'learning_rate': 4, 'n_estimators': 6}</td>\n",
       "      <td>62</td>\n",
       "      <td>0.176923</td>\n",
       "      <td>0.184971</td>\n",
       "      <td>0.107692</td>\n",
       "      <td>0.129094</td>\n",
       "      <td>0.130769</td>\n",
       "      <td>0.123314</td>\n",
       "      <td>0.130769</td>\n",
       "      <td>0.144509</td>\n",
       "      <td>0.193798</td>\n",
       "      <td>0.180769</td>\n",
       "      <td>0.001419</td>\n",
       "      <td>0.000323</td>\n",
       "      <td>0.032076</td>\n",
       "      <td>0.025757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.009919</td>\n",
       "      <td>0.000915</td>\n",
       "      <td>0.147920</td>\n",
       "      <td>0.152531</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>{'learning_rate': 4, 'n_estimators': 8}</td>\n",
       "      <td>62</td>\n",
       "      <td>0.176923</td>\n",
       "      <td>0.184971</td>\n",
       "      <td>0.107692</td>\n",
       "      <td>0.129094</td>\n",
       "      <td>0.130769</td>\n",
       "      <td>0.123314</td>\n",
       "      <td>0.130769</td>\n",
       "      <td>0.144509</td>\n",
       "      <td>0.193798</td>\n",
       "      <td>0.180769</td>\n",
       "      <td>0.000309</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.032076</td>\n",
       "      <td>0.025757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>0.009779</td>\n",
       "      <td>0.000976</td>\n",
       "      <td>0.147920</td>\n",
       "      <td>0.152531</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>{'learning_rate': 8, 'n_estimators': 6}</td>\n",
       "      <td>62</td>\n",
       "      <td>0.176923</td>\n",
       "      <td>0.184971</td>\n",
       "      <td>0.107692</td>\n",
       "      <td>0.129094</td>\n",
       "      <td>0.130769</td>\n",
       "      <td>0.123314</td>\n",
       "      <td>0.130769</td>\n",
       "      <td>0.144509</td>\n",
       "      <td>0.193798</td>\n",
       "      <td>0.180769</td>\n",
       "      <td>0.000636</td>\n",
       "      <td>0.000163</td>\n",
       "      <td>0.032076</td>\n",
       "      <td>0.025757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>0.007379</td>\n",
       "      <td>0.000781</td>\n",
       "      <td>0.147920</td>\n",
       "      <td>0.152531</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>{'learning_rate': 8, 'n_estimators': 4}</td>\n",
       "      <td>62</td>\n",
       "      <td>0.176923</td>\n",
       "      <td>0.184971</td>\n",
       "      <td>0.107692</td>\n",
       "      <td>0.129094</td>\n",
       "      <td>0.130769</td>\n",
       "      <td>0.123314</td>\n",
       "      <td>0.130769</td>\n",
       "      <td>0.144509</td>\n",
       "      <td>0.193798</td>\n",
       "      <td>0.180769</td>\n",
       "      <td>0.000436</td>\n",
       "      <td>0.000218</td>\n",
       "      <td>0.032076</td>\n",
       "      <td>0.025757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>0.014655</td>\n",
       "      <td>0.001074</td>\n",
       "      <td>0.147920</td>\n",
       "      <td>0.152531</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>{'learning_rate': 8, 'n_estimators': 10}</td>\n",
       "      <td>62</td>\n",
       "      <td>0.176923</td>\n",
       "      <td>0.184971</td>\n",
       "      <td>0.107692</td>\n",
       "      <td>0.129094</td>\n",
       "      <td>0.130769</td>\n",
       "      <td>0.123314</td>\n",
       "      <td>0.130769</td>\n",
       "      <td>0.144509</td>\n",
       "      <td>0.193798</td>\n",
       "      <td>0.180769</td>\n",
       "      <td>0.000957</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.032076</td>\n",
       "      <td>0.025757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.019484</td>\n",
       "      <td>0.001245</td>\n",
       "      <td>0.147920</td>\n",
       "      <td>0.152531</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>{'learning_rate': 4, 'n_estimators': 10}</td>\n",
       "      <td>62</td>\n",
       "      <td>0.176923</td>\n",
       "      <td>0.184971</td>\n",
       "      <td>0.107692</td>\n",
       "      <td>0.129094</td>\n",
       "      <td>0.130769</td>\n",
       "      <td>0.123314</td>\n",
       "      <td>0.130769</td>\n",
       "      <td>0.144509</td>\n",
       "      <td>0.193798</td>\n",
       "      <td>0.180769</td>\n",
       "      <td>0.005307</td>\n",
       "      <td>0.000224</td>\n",
       "      <td>0.032076</td>\n",
       "      <td>0.025757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>0.018030</td>\n",
       "      <td>0.001249</td>\n",
       "      <td>0.147920</td>\n",
       "      <td>0.152531</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>{'learning_rate': 7, 'n_estimators': 10}</td>\n",
       "      <td>62</td>\n",
       "      <td>0.176923</td>\n",
       "      <td>0.184971</td>\n",
       "      <td>0.107692</td>\n",
       "      <td>0.129094</td>\n",
       "      <td>0.130769</td>\n",
       "      <td>0.123314</td>\n",
       "      <td>0.130769</td>\n",
       "      <td>0.144509</td>\n",
       "      <td>0.193798</td>\n",
       "      <td>0.180769</td>\n",
       "      <td>0.004413</td>\n",
       "      <td>0.000204</td>\n",
       "      <td>0.032076</td>\n",
       "      <td>0.025757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.006989</td>\n",
       "      <td>0.000874</td>\n",
       "      <td>0.147920</td>\n",
       "      <td>0.154839</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>{'learning_rate': 3, 'n_estimators': 4}</td>\n",
       "      <td>62</td>\n",
       "      <td>0.176923</td>\n",
       "      <td>0.184971</td>\n",
       "      <td>0.107692</td>\n",
       "      <td>0.129094</td>\n",
       "      <td>0.130769</td>\n",
       "      <td>0.123314</td>\n",
       "      <td>0.130769</td>\n",
       "      <td>0.144509</td>\n",
       "      <td>0.193798</td>\n",
       "      <td>0.192308</td>\n",
       "      <td>0.000299</td>\n",
       "      <td>0.000314</td>\n",
       "      <td>0.032076</td>\n",
       "      <td>0.028549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>0.005809</td>\n",
       "      <td>0.000643</td>\n",
       "      <td>0.147920</td>\n",
       "      <td>0.152531</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>{'learning_rate': 6, 'n_estimators': 4}</td>\n",
       "      <td>62</td>\n",
       "      <td>0.176923</td>\n",
       "      <td>0.184971</td>\n",
       "      <td>0.107692</td>\n",
       "      <td>0.129094</td>\n",
       "      <td>0.130769</td>\n",
       "      <td>0.123314</td>\n",
       "      <td>0.130769</td>\n",
       "      <td>0.144509</td>\n",
       "      <td>0.193798</td>\n",
       "      <td>0.180769</td>\n",
       "      <td>0.000348</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.032076</td>\n",
       "      <td>0.025757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.004554</td>\n",
       "      <td>0.000791</td>\n",
       "      <td>0.147920</td>\n",
       "      <td>0.152531</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>{'learning_rate': 5, 'n_estimators': 2}</td>\n",
       "      <td>62</td>\n",
       "      <td>0.176923</td>\n",
       "      <td>0.184971</td>\n",
       "      <td>0.107692</td>\n",
       "      <td>0.129094</td>\n",
       "      <td>0.130769</td>\n",
       "      <td>0.123314</td>\n",
       "      <td>0.130769</td>\n",
       "      <td>0.144509</td>\n",
       "      <td>0.193798</td>\n",
       "      <td>0.180769</td>\n",
       "      <td>0.000349</td>\n",
       "      <td>0.000236</td>\n",
       "      <td>0.032076</td>\n",
       "      <td>0.025757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.006243</td>\n",
       "      <td>0.000785</td>\n",
       "      <td>0.147920</td>\n",
       "      <td>0.152531</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>{'learning_rate': 5, 'n_estimators': 4}</td>\n",
       "      <td>62</td>\n",
       "      <td>0.176923</td>\n",
       "      <td>0.184971</td>\n",
       "      <td>0.107692</td>\n",
       "      <td>0.129094</td>\n",
       "      <td>0.130769</td>\n",
       "      <td>0.123314</td>\n",
       "      <td>0.130769</td>\n",
       "      <td>0.144509</td>\n",
       "      <td>0.193798</td>\n",
       "      <td>0.180769</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000259</td>\n",
       "      <td>0.032076</td>\n",
       "      <td>0.025757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>0.003557</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.135593</td>\n",
       "      <td>0.132108</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>{'learning_rate': 9, 'n_estimators': 2}</td>\n",
       "      <td>91</td>\n",
       "      <td>0.130769</td>\n",
       "      <td>0.111753</td>\n",
       "      <td>0.107692</td>\n",
       "      <td>0.129094</td>\n",
       "      <td>0.130769</td>\n",
       "      <td>0.123314</td>\n",
       "      <td>0.115385</td>\n",
       "      <td>0.115607</td>\n",
       "      <td>0.193798</td>\n",
       "      <td>0.180769</td>\n",
       "      <td>0.000321</td>\n",
       "      <td>0.000053</td>\n",
       "      <td>0.030339</td>\n",
       "      <td>0.025064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>0.015620</td>\n",
       "      <td>0.001727</td>\n",
       "      <td>0.135593</td>\n",
       "      <td>0.132108</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>{'learning_rate': 9, 'n_estimators': 10}</td>\n",
       "      <td>91</td>\n",
       "      <td>0.130769</td>\n",
       "      <td>0.111753</td>\n",
       "      <td>0.107692</td>\n",
       "      <td>0.129094</td>\n",
       "      <td>0.130769</td>\n",
       "      <td>0.123314</td>\n",
       "      <td>0.115385</td>\n",
       "      <td>0.115607</td>\n",
       "      <td>0.193798</td>\n",
       "      <td>0.180769</td>\n",
       "      <td>0.001463</td>\n",
       "      <td>0.000245</td>\n",
       "      <td>0.030339</td>\n",
       "      <td>0.025064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>0.014038</td>\n",
       "      <td>0.001082</td>\n",
       "      <td>0.135593</td>\n",
       "      <td>0.132108</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>{'learning_rate': 9, 'n_estimators': 8}</td>\n",
       "      <td>91</td>\n",
       "      <td>0.130769</td>\n",
       "      <td>0.111753</td>\n",
       "      <td>0.107692</td>\n",
       "      <td>0.129094</td>\n",
       "      <td>0.130769</td>\n",
       "      <td>0.123314</td>\n",
       "      <td>0.115385</td>\n",
       "      <td>0.115607</td>\n",
       "      <td>0.193798</td>\n",
       "      <td>0.180769</td>\n",
       "      <td>0.003679</td>\n",
       "      <td>0.000148</td>\n",
       "      <td>0.030339</td>\n",
       "      <td>0.025064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>0.007825</td>\n",
       "      <td>0.000795</td>\n",
       "      <td>0.135593</td>\n",
       "      <td>0.132108</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>{'learning_rate': 9, 'n_estimators': 6}</td>\n",
       "      <td>91</td>\n",
       "      <td>0.130769</td>\n",
       "      <td>0.111753</td>\n",
       "      <td>0.107692</td>\n",
       "      <td>0.129094</td>\n",
       "      <td>0.130769</td>\n",
       "      <td>0.123314</td>\n",
       "      <td>0.115385</td>\n",
       "      <td>0.115607</td>\n",
       "      <td>0.193798</td>\n",
       "      <td>0.180769</td>\n",
       "      <td>0.000253</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>0.030339</td>\n",
       "      <td>0.025064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>0.006049</td>\n",
       "      <td>0.000669</td>\n",
       "      <td>0.135593</td>\n",
       "      <td>0.132108</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>{'learning_rate': 9, 'n_estimators': 4}</td>\n",
       "      <td>91</td>\n",
       "      <td>0.130769</td>\n",
       "      <td>0.111753</td>\n",
       "      <td>0.107692</td>\n",
       "      <td>0.129094</td>\n",
       "      <td>0.130769</td>\n",
       "      <td>0.123314</td>\n",
       "      <td>0.115385</td>\n",
       "      <td>0.115607</td>\n",
       "      <td>0.193798</td>\n",
       "      <td>0.180769</td>\n",
       "      <td>0.001390</td>\n",
       "      <td>0.000114</td>\n",
       "      <td>0.030339</td>\n",
       "      <td>0.025064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>0.003792</td>\n",
       "      <td>0.000571</td>\n",
       "      <td>0.117103</td>\n",
       "      <td>0.119800</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>{'learning_rate': 10, 'n_estimators': 2}</td>\n",
       "      <td>96</td>\n",
       "      <td>0.130769</td>\n",
       "      <td>0.111753</td>\n",
       "      <td>0.107692</td>\n",
       "      <td>0.129094</td>\n",
       "      <td>0.130769</td>\n",
       "      <td>0.123314</td>\n",
       "      <td>0.115385</td>\n",
       "      <td>0.115607</td>\n",
       "      <td>0.100775</td>\n",
       "      <td>0.119231</td>\n",
       "      <td>0.000549</td>\n",
       "      <td>0.000087</td>\n",
       "      <td>0.012089</td>\n",
       "      <td>0.006023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>0.006698</td>\n",
       "      <td>0.000687</td>\n",
       "      <td>0.117103</td>\n",
       "      <td>0.119800</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>{'learning_rate': 10, 'n_estimators': 4}</td>\n",
       "      <td>96</td>\n",
       "      <td>0.130769</td>\n",
       "      <td>0.111753</td>\n",
       "      <td>0.107692</td>\n",
       "      <td>0.129094</td>\n",
       "      <td>0.130769</td>\n",
       "      <td>0.123314</td>\n",
       "      <td>0.115385</td>\n",
       "      <td>0.115607</td>\n",
       "      <td>0.100775</td>\n",
       "      <td>0.119231</td>\n",
       "      <td>0.000290</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.012089</td>\n",
       "      <td>0.006023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.007784</td>\n",
       "      <td>0.000799</td>\n",
       "      <td>0.117103</td>\n",
       "      <td>0.119800</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>{'learning_rate': 10, 'n_estimators': 6}</td>\n",
       "      <td>96</td>\n",
       "      <td>0.130769</td>\n",
       "      <td>0.111753</td>\n",
       "      <td>0.107692</td>\n",
       "      <td>0.129094</td>\n",
       "      <td>0.130769</td>\n",
       "      <td>0.123314</td>\n",
       "      <td>0.115385</td>\n",
       "      <td>0.115607</td>\n",
       "      <td>0.100775</td>\n",
       "      <td>0.119231</td>\n",
       "      <td>0.000676</td>\n",
       "      <td>0.000088</td>\n",
       "      <td>0.012089</td>\n",
       "      <td>0.006023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0.009689</td>\n",
       "      <td>0.000933</td>\n",
       "      <td>0.117103</td>\n",
       "      <td>0.119800</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>{'learning_rate': 10, 'n_estimators': 8}</td>\n",
       "      <td>96</td>\n",
       "      <td>0.130769</td>\n",
       "      <td>0.111753</td>\n",
       "      <td>0.107692</td>\n",
       "      <td>0.129094</td>\n",
       "      <td>0.130769</td>\n",
       "      <td>0.123314</td>\n",
       "      <td>0.115385</td>\n",
       "      <td>0.115607</td>\n",
       "      <td>0.100775</td>\n",
       "      <td>0.119231</td>\n",
       "      <td>0.000738</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.012089</td>\n",
       "      <td>0.006023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>0.011416</td>\n",
       "      <td>0.001072</td>\n",
       "      <td>0.117103</td>\n",
       "      <td>0.119800</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>{'learning_rate': 10, 'n_estimators': 10}</td>\n",
       "      <td>96</td>\n",
       "      <td>0.130769</td>\n",
       "      <td>0.111753</td>\n",
       "      <td>0.107692</td>\n",
       "      <td>0.129094</td>\n",
       "      <td>0.130769</td>\n",
       "      <td>0.123314</td>\n",
       "      <td>0.115385</td>\n",
       "      <td>0.115607</td>\n",
       "      <td>0.100775</td>\n",
       "      <td>0.119231</td>\n",
       "      <td>0.000288</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.012089</td>\n",
       "      <td>0.006023</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  mean_score_time  mean_test_score  mean_train_score  \\\n",
       "4        0.007529         0.000766         0.895223          0.887903   \n",
       "5        0.009128         0.000794         0.890601          0.891371   \n",
       "3        0.005935         0.000624         0.890601          0.888676   \n",
       "2        0.004816         0.000564         0.889060          0.887519   \n",
       "6        0.011161         0.001033         0.887519          0.890216   \n",
       "1        0.003412         0.000503         0.887519          0.887522   \n",
       "7        0.012359         0.001355         0.885978          0.890216   \n",
       "50       0.002307         0.000436         0.884438          0.884439   \n",
       "86       0.008823         0.001001         0.884438          0.884439   \n",
       "88       0.016932         0.001476         0.884438          0.884439   \n",
       "90       0.002898         0.000645         0.884438          0.884439   \n",
       "92       0.005095         0.000643         0.884438          0.884439   \n",
       "94       0.007334         0.000924         0.884438          0.884439   \n",
       "96       0.008459         0.000879         0.884438          0.884439   \n",
       "98       0.010388         0.001007         0.884438          0.884439   \n",
       "40       0.002943         0.000537         0.884438          0.884439   \n",
       "30       0.002230         0.000401         0.884438          0.884439   \n",
       "28       0.011357         0.001060         0.884438          0.884824   \n",
       "26       0.009338         0.001042         0.884438          0.884824   \n",
       "82       0.004304         0.000541         0.884438          0.884439   \n",
       "36       0.010177         0.000922         0.884438          0.884439   \n",
       "20       0.002691         0.000429         0.884438          0.884439   \n",
       "10       0.002635         0.000653         0.884438          0.884439   \n",
       "24       0.009299         0.000869         0.884438          0.884824   \n",
       "34       0.013071         0.001145         0.884438          0.884439   \n",
       "32       0.008169         0.000975         0.884438          0.884439   \n",
       "84       0.006764         0.000698         0.884438          0.884439   \n",
       "38       0.011058         0.001018         0.884438          0.884439   \n",
       "80       0.002662         0.000590         0.884438          0.884439   \n",
       "78       0.011039         0.001049         0.884438          0.884439   \n",
       "..            ...              ...              ...               ...   \n",
       "25       0.008913         0.000838         0.147920          0.154839   \n",
       "57       0.011324         0.000987         0.147920          0.152531   \n",
       "59       0.011713         0.001290         0.147920          0.152531   \n",
       "61       0.004498         0.000676         0.147920          0.152531   \n",
       "33       0.008545         0.000924         0.147920          0.152531   \n",
       "47       0.011173         0.001109         0.147920          0.152531   \n",
       "45       0.008852         0.001105         0.147920          0.152531   \n",
       "71       0.004746         0.000753         0.147920          0.152531   \n",
       "77       0.010901         0.001019         0.147920          0.152531   \n",
       "35       0.009475         0.001044         0.147920          0.152531   \n",
       "37       0.009919         0.000915         0.147920          0.152531   \n",
       "75       0.009779         0.000976         0.147920          0.152531   \n",
       "73       0.007379         0.000781         0.147920          0.152531   \n",
       "79       0.014655         0.001074         0.147920          0.152531   \n",
       "39       0.019484         0.001245         0.147920          0.152531   \n",
       "69       0.018030         0.001249         0.147920          0.152531   \n",
       "23       0.006989         0.000874         0.147920          0.154839   \n",
       "53       0.005809         0.000643         0.147920          0.152531   \n",
       "41       0.004554         0.000791         0.147920          0.152531   \n",
       "43       0.006243         0.000785         0.147920          0.152531   \n",
       "81       0.003557         0.000500         0.135593          0.132108   \n",
       "89       0.015620         0.001727         0.135593          0.132108   \n",
       "87       0.014038         0.001082         0.135593          0.132108   \n",
       "85       0.007825         0.000795         0.135593          0.132108   \n",
       "83       0.006049         0.000669         0.135593          0.132108   \n",
       "91       0.003792         0.000571         0.117103          0.119800   \n",
       "93       0.006698         0.000687         0.117103          0.119800   \n",
       "95       0.007784         0.000799         0.117103          0.119800   \n",
       "97       0.009689         0.000933         0.117103          0.119800   \n",
       "99       0.011416         0.001072         0.117103          0.119800   \n",
       "\n",
       "   param_learning_rate param_n_estimators  \\\n",
       "4                    1                  5   \n",
       "5                    1                  6   \n",
       "3                    1                  4   \n",
       "2                    1                  3   \n",
       "6                    1                  7   \n",
       "1                    1                  2   \n",
       "7                    1                  8   \n",
       "50                   6                  1   \n",
       "86                   9                  7   \n",
       "88                   9                  9   \n",
       "90                  10                  1   \n",
       "92                  10                  3   \n",
       "94                  10                  5   \n",
       "96                  10                  7   \n",
       "98                  10                  9   \n",
       "40                   5                  1   \n",
       "30                   4                  1   \n",
       "28                   3                  9   \n",
       "26                   3                  7   \n",
       "82                   9                  3   \n",
       "36                   4                  7   \n",
       "20                   3                  1   \n",
       "10                   2                  1   \n",
       "24                   3                  5   \n",
       "34                   4                  5   \n",
       "32                   4                  3   \n",
       "84                   9                  5   \n",
       "38                   4                  9   \n",
       "80                   9                  1   \n",
       "78                   8                  9   \n",
       "..                 ...                ...   \n",
       "25                   3                  6   \n",
       "57                   6                  8   \n",
       "59                   6                 10   \n",
       "61                   7                  2   \n",
       "33                   4                  4   \n",
       "47                   5                  8   \n",
       "45                   5                  6   \n",
       "71                   8                  2   \n",
       "77                   8                  8   \n",
       "35                   4                  6   \n",
       "37                   4                  8   \n",
       "75                   8                  6   \n",
       "73                   8                  4   \n",
       "79                   8                 10   \n",
       "39                   4                 10   \n",
       "69                   7                 10   \n",
       "23                   3                  4   \n",
       "53                   6                  4   \n",
       "41                   5                  2   \n",
       "43                   5                  4   \n",
       "81                   9                  2   \n",
       "89                   9                 10   \n",
       "87                   9                  8   \n",
       "85                   9                  6   \n",
       "83                   9                  4   \n",
       "91                  10                  2   \n",
       "93                  10                  4   \n",
       "95                  10                  6   \n",
       "97                  10                  8   \n",
       "99                  10                 10   \n",
       "\n",
       "                                       params  rank_test_score  \\\n",
       "4     {'learning_rate': 1, 'n_estimators': 5}                1   \n",
       "5     {'learning_rate': 1, 'n_estimators': 6}                2   \n",
       "3     {'learning_rate': 1, 'n_estimators': 4}                2   \n",
       "2     {'learning_rate': 1, 'n_estimators': 3}                4   \n",
       "6     {'learning_rate': 1, 'n_estimators': 7}                5   \n",
       "1     {'learning_rate': 1, 'n_estimators': 2}                5   \n",
       "7     {'learning_rate': 1, 'n_estimators': 8}                7   \n",
       "50    {'learning_rate': 6, 'n_estimators': 1}                8   \n",
       "86    {'learning_rate': 9, 'n_estimators': 7}                8   \n",
       "88    {'learning_rate': 9, 'n_estimators': 9}                8   \n",
       "90   {'learning_rate': 10, 'n_estimators': 1}                8   \n",
       "92   {'learning_rate': 10, 'n_estimators': 3}                8   \n",
       "94   {'learning_rate': 10, 'n_estimators': 5}                8   \n",
       "96   {'learning_rate': 10, 'n_estimators': 7}                8   \n",
       "98   {'learning_rate': 10, 'n_estimators': 9}                8   \n",
       "40    {'learning_rate': 5, 'n_estimators': 1}                8   \n",
       "30    {'learning_rate': 4, 'n_estimators': 1}                8   \n",
       "28    {'learning_rate': 3, 'n_estimators': 9}                8   \n",
       "26    {'learning_rate': 3, 'n_estimators': 7}                8   \n",
       "82    {'learning_rate': 9, 'n_estimators': 3}                8   \n",
       "36    {'learning_rate': 4, 'n_estimators': 7}                8   \n",
       "20    {'learning_rate': 3, 'n_estimators': 1}                8   \n",
       "10    {'learning_rate': 2, 'n_estimators': 1}                8   \n",
       "24    {'learning_rate': 3, 'n_estimators': 5}                8   \n",
       "34    {'learning_rate': 4, 'n_estimators': 5}                8   \n",
       "32    {'learning_rate': 4, 'n_estimators': 3}                8   \n",
       "84    {'learning_rate': 9, 'n_estimators': 5}                8   \n",
       "38    {'learning_rate': 4, 'n_estimators': 9}                8   \n",
       "80    {'learning_rate': 9, 'n_estimators': 1}                8   \n",
       "78    {'learning_rate': 8, 'n_estimators': 9}                8   \n",
       "..                                        ...              ...   \n",
       "25    {'learning_rate': 3, 'n_estimators': 6}               62   \n",
       "57    {'learning_rate': 6, 'n_estimators': 8}               62   \n",
       "59   {'learning_rate': 6, 'n_estimators': 10}               62   \n",
       "61    {'learning_rate': 7, 'n_estimators': 2}               62   \n",
       "33    {'learning_rate': 4, 'n_estimators': 4}               62   \n",
       "47    {'learning_rate': 5, 'n_estimators': 8}               62   \n",
       "45    {'learning_rate': 5, 'n_estimators': 6}               62   \n",
       "71    {'learning_rate': 8, 'n_estimators': 2}               62   \n",
       "77    {'learning_rate': 8, 'n_estimators': 8}               62   \n",
       "35    {'learning_rate': 4, 'n_estimators': 6}               62   \n",
       "37    {'learning_rate': 4, 'n_estimators': 8}               62   \n",
       "75    {'learning_rate': 8, 'n_estimators': 6}               62   \n",
       "73    {'learning_rate': 8, 'n_estimators': 4}               62   \n",
       "79   {'learning_rate': 8, 'n_estimators': 10}               62   \n",
       "39   {'learning_rate': 4, 'n_estimators': 10}               62   \n",
       "69   {'learning_rate': 7, 'n_estimators': 10}               62   \n",
       "23    {'learning_rate': 3, 'n_estimators': 4}               62   \n",
       "53    {'learning_rate': 6, 'n_estimators': 4}               62   \n",
       "41    {'learning_rate': 5, 'n_estimators': 2}               62   \n",
       "43    {'learning_rate': 5, 'n_estimators': 4}               62   \n",
       "81    {'learning_rate': 9, 'n_estimators': 2}               91   \n",
       "89   {'learning_rate': 9, 'n_estimators': 10}               91   \n",
       "87    {'learning_rate': 9, 'n_estimators': 8}               91   \n",
       "85    {'learning_rate': 9, 'n_estimators': 6}               91   \n",
       "83    {'learning_rate': 9, 'n_estimators': 4}               91   \n",
       "91   {'learning_rate': 10, 'n_estimators': 2}               96   \n",
       "93   {'learning_rate': 10, 'n_estimators': 4}               96   \n",
       "95   {'learning_rate': 10, 'n_estimators': 6}               96   \n",
       "97   {'learning_rate': 10, 'n_estimators': 8}               96   \n",
       "99  {'learning_rate': 10, 'n_estimators': 10}               96   \n",
       "\n",
       "    split0_test_score  split0_train_score  split1_test_score  \\\n",
       "4            0.915385            0.874759           0.915385   \n",
       "5            0.900000            0.878613           0.907692   \n",
       "3            0.900000            0.884393           0.915385   \n",
       "2            0.900000            0.884393           0.915385   \n",
       "6            0.907692            0.884393           0.907692   \n",
       "1            0.869231            0.888247           0.923077   \n",
       "7            0.892308            0.880539           0.915385   \n",
       "50           0.869231            0.888247           0.900000   \n",
       "86           0.869231            0.888247           0.900000   \n",
       "88           0.869231            0.888247           0.900000   \n",
       "90           0.869231            0.888247           0.900000   \n",
       "92           0.869231            0.888247           0.900000   \n",
       "94           0.869231            0.888247           0.900000   \n",
       "96           0.869231            0.888247           0.900000   \n",
       "98           0.869231            0.888247           0.900000   \n",
       "40           0.869231            0.888247           0.900000   \n",
       "30           0.869231            0.888247           0.900000   \n",
       "28           0.869231            0.888247           0.900000   \n",
       "26           0.869231            0.888247           0.900000   \n",
       "82           0.869231            0.888247           0.900000   \n",
       "36           0.869231            0.888247           0.900000   \n",
       "20           0.869231            0.888247           0.900000   \n",
       "10           0.869231            0.888247           0.900000   \n",
       "24           0.869231            0.888247           0.900000   \n",
       "34           0.869231            0.888247           0.900000   \n",
       "32           0.869231            0.888247           0.900000   \n",
       "84           0.869231            0.888247           0.900000   \n",
       "38           0.869231            0.888247           0.900000   \n",
       "80           0.869231            0.888247           0.900000   \n",
       "78           0.869231            0.888247           0.900000   \n",
       "..                ...                 ...                ...   \n",
       "25           0.176923            0.184971           0.107692   \n",
       "57           0.176923            0.184971           0.107692   \n",
       "59           0.176923            0.184971           0.107692   \n",
       "61           0.176923            0.184971           0.107692   \n",
       "33           0.176923            0.184971           0.107692   \n",
       "47           0.176923            0.184971           0.107692   \n",
       "45           0.176923            0.184971           0.107692   \n",
       "71           0.176923            0.184971           0.107692   \n",
       "77           0.176923            0.184971           0.107692   \n",
       "35           0.176923            0.184971           0.107692   \n",
       "37           0.176923            0.184971           0.107692   \n",
       "75           0.176923            0.184971           0.107692   \n",
       "73           0.176923            0.184971           0.107692   \n",
       "79           0.176923            0.184971           0.107692   \n",
       "39           0.176923            0.184971           0.107692   \n",
       "69           0.176923            0.184971           0.107692   \n",
       "23           0.176923            0.184971           0.107692   \n",
       "53           0.176923            0.184971           0.107692   \n",
       "41           0.176923            0.184971           0.107692   \n",
       "43           0.176923            0.184971           0.107692   \n",
       "81           0.130769            0.111753           0.107692   \n",
       "89           0.130769            0.111753           0.107692   \n",
       "87           0.130769            0.111753           0.107692   \n",
       "85           0.130769            0.111753           0.107692   \n",
       "83           0.130769            0.111753           0.107692   \n",
       "91           0.130769            0.111753           0.107692   \n",
       "93           0.130769            0.111753           0.107692   \n",
       "95           0.130769            0.111753           0.107692   \n",
       "97           0.130769            0.111753           0.107692   \n",
       "99           0.130769            0.111753           0.107692   \n",
       "\n",
       "    split1_train_score  split2_test_score  split2_train_score  \\\n",
       "4             0.888247           0.876923            0.897881   \n",
       "5             0.892100           0.861538            0.895954   \n",
       "3             0.880539           0.861538            0.899807   \n",
       "2             0.880539           0.861538            0.899807   \n",
       "6             0.884393           0.853846            0.894027   \n",
       "1             0.884393           0.861538            0.899807   \n",
       "7             0.892100           0.853846            0.892100   \n",
       "50            0.880539           0.869231            0.888247   \n",
       "86            0.880539           0.869231            0.888247   \n",
       "88            0.880539           0.869231            0.888247   \n",
       "90            0.880539           0.869231            0.888247   \n",
       "92            0.880539           0.869231            0.888247   \n",
       "94            0.880539           0.869231            0.888247   \n",
       "96            0.880539           0.869231            0.888247   \n",
       "98            0.880539           0.869231            0.888247   \n",
       "40            0.880539           0.869231            0.888247   \n",
       "30            0.880539           0.869231            0.888247   \n",
       "28            0.880539           0.869231            0.888247   \n",
       "26            0.880539           0.869231            0.888247   \n",
       "82            0.880539           0.869231            0.888247   \n",
       "36            0.880539           0.869231            0.888247   \n",
       "20            0.880539           0.869231            0.888247   \n",
       "10            0.880539           0.869231            0.888247   \n",
       "24            0.880539           0.869231            0.888247   \n",
       "34            0.880539           0.869231            0.888247   \n",
       "32            0.880539           0.869231            0.888247   \n",
       "84            0.880539           0.869231            0.888247   \n",
       "38            0.880539           0.869231            0.888247   \n",
       "80            0.880539           0.869231            0.888247   \n",
       "78            0.880539           0.869231            0.888247   \n",
       "..                 ...                ...                 ...   \n",
       "25            0.129094           0.130769            0.123314   \n",
       "57            0.129094           0.130769            0.123314   \n",
       "59            0.129094           0.130769            0.123314   \n",
       "61            0.129094           0.130769            0.123314   \n",
       "33            0.129094           0.130769            0.123314   \n",
       "47            0.129094           0.130769            0.123314   \n",
       "45            0.129094           0.130769            0.123314   \n",
       "71            0.129094           0.130769            0.123314   \n",
       "77            0.129094           0.130769            0.123314   \n",
       "35            0.129094           0.130769            0.123314   \n",
       "37            0.129094           0.130769            0.123314   \n",
       "75            0.129094           0.130769            0.123314   \n",
       "73            0.129094           0.130769            0.123314   \n",
       "79            0.129094           0.130769            0.123314   \n",
       "39            0.129094           0.130769            0.123314   \n",
       "69            0.129094           0.130769            0.123314   \n",
       "23            0.129094           0.130769            0.123314   \n",
       "53            0.129094           0.130769            0.123314   \n",
       "41            0.129094           0.130769            0.123314   \n",
       "43            0.129094           0.130769            0.123314   \n",
       "81            0.129094           0.130769            0.123314   \n",
       "89            0.129094           0.130769            0.123314   \n",
       "87            0.129094           0.130769            0.123314   \n",
       "85            0.129094           0.130769            0.123314   \n",
       "83            0.129094           0.130769            0.123314   \n",
       "91            0.129094           0.130769            0.123314   \n",
       "93            0.129094           0.130769            0.123314   \n",
       "95            0.129094           0.130769            0.123314   \n",
       "97            0.129094           0.130769            0.123314   \n",
       "99            0.129094           0.130769            0.123314   \n",
       "\n",
       "    split3_test_score  split3_train_score  split4_test_score  \\\n",
       "4            0.876923            0.886320           0.891473   \n",
       "5            0.900000            0.897881           0.883721   \n",
       "3            0.892308            0.892100           0.883721   \n",
       "2            0.884615            0.884393           0.883721   \n",
       "6            0.884615            0.899807           0.883721   \n",
       "1            0.884615            0.884393           0.899225   \n",
       "7            0.892308            0.895954           0.875969   \n",
       "50           0.884615            0.884393           0.899225   \n",
       "86           0.884615            0.884393           0.899225   \n",
       "88           0.884615            0.884393           0.899225   \n",
       "90           0.884615            0.884393           0.899225   \n",
       "92           0.884615            0.884393           0.899225   \n",
       "94           0.884615            0.884393           0.899225   \n",
       "96           0.884615            0.884393           0.899225   \n",
       "98           0.884615            0.884393           0.899225   \n",
       "40           0.884615            0.884393           0.899225   \n",
       "30           0.884615            0.884393           0.899225   \n",
       "28           0.884615            0.884393           0.899225   \n",
       "26           0.884615            0.884393           0.899225   \n",
       "82           0.884615            0.884393           0.899225   \n",
       "36           0.884615            0.884393           0.899225   \n",
       "20           0.884615            0.884393           0.899225   \n",
       "10           0.884615            0.884393           0.899225   \n",
       "24           0.884615            0.884393           0.899225   \n",
       "34           0.884615            0.884393           0.899225   \n",
       "32           0.884615            0.884393           0.899225   \n",
       "84           0.884615            0.884393           0.899225   \n",
       "38           0.884615            0.884393           0.899225   \n",
       "80           0.884615            0.884393           0.899225   \n",
       "78           0.884615            0.884393           0.899225   \n",
       "..                ...                 ...                ...   \n",
       "25           0.130769            0.144509           0.193798   \n",
       "57           0.130769            0.144509           0.193798   \n",
       "59           0.130769            0.144509           0.193798   \n",
       "61           0.130769            0.144509           0.193798   \n",
       "33           0.130769            0.144509           0.193798   \n",
       "47           0.130769            0.144509           0.193798   \n",
       "45           0.130769            0.144509           0.193798   \n",
       "71           0.130769            0.144509           0.193798   \n",
       "77           0.130769            0.144509           0.193798   \n",
       "35           0.130769            0.144509           0.193798   \n",
       "37           0.130769            0.144509           0.193798   \n",
       "75           0.130769            0.144509           0.193798   \n",
       "73           0.130769            0.144509           0.193798   \n",
       "79           0.130769            0.144509           0.193798   \n",
       "39           0.130769            0.144509           0.193798   \n",
       "69           0.130769            0.144509           0.193798   \n",
       "23           0.130769            0.144509           0.193798   \n",
       "53           0.130769            0.144509           0.193798   \n",
       "41           0.130769            0.144509           0.193798   \n",
       "43           0.130769            0.144509           0.193798   \n",
       "81           0.115385            0.115607           0.193798   \n",
       "89           0.115385            0.115607           0.193798   \n",
       "87           0.115385            0.115607           0.193798   \n",
       "85           0.115385            0.115607           0.193798   \n",
       "83           0.115385            0.115607           0.193798   \n",
       "91           0.115385            0.115607           0.100775   \n",
       "93           0.115385            0.115607           0.100775   \n",
       "95           0.115385            0.115607           0.100775   \n",
       "97           0.115385            0.115607           0.100775   \n",
       "99           0.115385            0.115607           0.100775   \n",
       "\n",
       "    split4_train_score  std_fit_time  std_score_time  std_test_score  \\\n",
       "4             0.892308      0.000436        0.000064        0.017315   \n",
       "5             0.892308      0.000436        0.000022        0.016508   \n",
       "3             0.886538      0.000045        0.000014        0.017886   \n",
       "2             0.888462      0.000277        0.000028        0.018004   \n",
       "6             0.888462      0.000555        0.000355        0.019867   \n",
       "1             0.880769      0.000069        0.000059        0.022009   \n",
       "7             0.890385      0.000355        0.000378        0.020396   \n",
       "50            0.880769      0.000316        0.000067        0.013588   \n",
       "86            0.880769      0.000548        0.000225        0.013588   \n",
       "88            0.880769      0.000577        0.000291        0.013588   \n",
       "90            0.880769      0.000172        0.000184        0.013588   \n",
       "92            0.880769      0.000741        0.000063        0.013588   \n",
       "94            0.880769      0.000706        0.000244        0.013588   \n",
       "96            0.880769      0.000288        0.000108        0.013588   \n",
       "98            0.880769      0.000162        0.000079        0.013588   \n",
       "40            0.880769      0.000568        0.000185        0.013588   \n",
       "30            0.880769      0.000056        0.000015        0.013588   \n",
       "28            0.882692      0.001014        0.000119        0.013588   \n",
       "26            0.882692      0.000400        0.000377        0.013588   \n",
       "82            0.880769      0.000171        0.000015        0.013588   \n",
       "36            0.880769      0.001129        0.000090        0.013588   \n",
       "20            0.880769      0.000528        0.000033        0.013588   \n",
       "10            0.880769      0.000223        0.000202        0.013588   \n",
       "24            0.882692      0.002641        0.000123        0.013588   \n",
       "34            0.880769      0.004349        0.000139        0.013588   \n",
       "32            0.880769      0.001529        0.000096        0.013588   \n",
       "84            0.880769      0.001376        0.000060        0.013588   \n",
       "38            0.880769      0.000572        0.000075        0.013588   \n",
       "80            0.880769      0.000344        0.000150        0.013588   \n",
       "78            0.880769      0.000321        0.000079        0.013588   \n",
       "..                 ...           ...             ...             ...   \n",
       "25            0.192308      0.000707        0.000068        0.032076   \n",
       "57            0.180769      0.002163        0.000076        0.032076   \n",
       "59            0.180769      0.000605        0.000526        0.032076   \n",
       "61            0.180769      0.000682        0.000163        0.032076   \n",
       "33            0.180769      0.001474        0.000278        0.032076   \n",
       "47            0.180769      0.000549        0.000358        0.032076   \n",
       "45            0.180769      0.000392        0.000285        0.032076   \n",
       "71            0.180769      0.000622        0.000175        0.032076   \n",
       "77            0.180769      0.000834        0.000100        0.032076   \n",
       "35            0.180769      0.001419        0.000323        0.032076   \n",
       "37            0.180769      0.000309        0.000039        0.032076   \n",
       "75            0.180769      0.000636        0.000163        0.032076   \n",
       "73            0.180769      0.000436        0.000218        0.032076   \n",
       "79            0.180769      0.000957        0.000052        0.032076   \n",
       "39            0.180769      0.005307        0.000224        0.032076   \n",
       "69            0.180769      0.004413        0.000204        0.032076   \n",
       "23            0.192308      0.000299        0.000314        0.032076   \n",
       "53            0.180769      0.000348        0.000013        0.032076   \n",
       "41            0.180769      0.000349        0.000236        0.032076   \n",
       "43            0.180769      0.000100        0.000259        0.032076   \n",
       "81            0.180769      0.000321        0.000053        0.030339   \n",
       "89            0.180769      0.001463        0.000245        0.030339   \n",
       "87            0.180769      0.003679        0.000148        0.030339   \n",
       "85            0.180769      0.000253        0.000036        0.030339   \n",
       "83            0.180769      0.001390        0.000114        0.030339   \n",
       "91            0.119231      0.000549        0.000087        0.012089   \n",
       "93            0.119231      0.000290        0.000028        0.012089   \n",
       "95            0.119231      0.000676        0.000088        0.012089   \n",
       "97            0.119231      0.000738        0.000046        0.012089   \n",
       "99            0.119231      0.000288        0.000040        0.012089   \n",
       "\n",
       "    std_train_score  \n",
       "4          0.007673  \n",
       "5          0.006746  \n",
       "3          0.006704  \n",
       "2          0.006635  \n",
       "6          0.005957  \n",
       "1          0.006582  \n",
       "7          0.005171  \n",
       "50         0.003396  \n",
       "86         0.003396  \n",
       "88         0.003396  \n",
       "90         0.003396  \n",
       "92         0.003396  \n",
       "94         0.003396  \n",
       "96         0.003396  \n",
       "98         0.003396  \n",
       "40         0.003396  \n",
       "30         0.003396  \n",
       "28         0.003050  \n",
       "26         0.003050  \n",
       "82         0.003396  \n",
       "36         0.003396  \n",
       "20         0.003396  \n",
       "10         0.003396  \n",
       "24         0.003050  \n",
       "34         0.003396  \n",
       "32         0.003396  \n",
       "84         0.003396  \n",
       "38         0.003396  \n",
       "80         0.003396  \n",
       "78         0.003396  \n",
       "..              ...  \n",
       "25         0.028549  \n",
       "57         0.025757  \n",
       "59         0.025757  \n",
       "61         0.025757  \n",
       "33         0.025757  \n",
       "47         0.025757  \n",
       "45         0.025757  \n",
       "71         0.025757  \n",
       "77         0.025757  \n",
       "35         0.025757  \n",
       "37         0.025757  \n",
       "75         0.025757  \n",
       "73         0.025757  \n",
       "79         0.025757  \n",
       "39         0.025757  \n",
       "69         0.025757  \n",
       "23         0.028549  \n",
       "53         0.025757  \n",
       "41         0.025757  \n",
       "43         0.025757  \n",
       "81         0.025064  \n",
       "89         0.025064  \n",
       "87         0.025064  \n",
       "85         0.025064  \n",
       "83         0.025064  \n",
       "91         0.006023  \n",
       "93         0.006023  \n",
       "95         0.006023  \n",
       "97         0.006023  \n",
       "99         0.006023  \n",
       "\n",
       "[100 rows x 22 columns]"
      ]
     },
     "execution_count": 965,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AdaBoost_grid=pd.DataFrame(AdaBoost_CV.cv_results_).sort_values('mean_test_score', ascending=False)\n",
    "AdaBoost_grid.sort_values('mean_test_score', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Значения метрики для 5 лучших моделей:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 966,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.895223420647\n",
      "0.890600924499\n",
      "0.890600924499\n",
      "0.88906009245\n",
      "0.887519260401\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(AdaBoost_grid.sort_values('mean_test_score', ascending = False).iloc[i].mean_test_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Судя по значениям метрики, лучшей моделью оказался AdaBoostClasifier(learning_rate = 1, n_estimators = 5)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "nav_menu": {},
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "105px",
    "width": "253px"
   },
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": true,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
